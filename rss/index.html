<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Flamingbytes]]></title><description><![CDATA[Stay hungry, Stay foolish.]]></description><link>https://www.flamingbytes.com/</link><image><url>https://www.flamingbytes.com/favicon.png</url><title>Flamingbytes</title><link>https://www.flamingbytes.com/</link></image><generator>Ghost 5.26</generator><lastBuildDate>Sun, 15 Jan 2023 04:18:59 GMT</lastBuildDate><atom:link href="https://www.flamingbytes.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Using R to analyze US COVID pandemic waves and peaks]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="goal">Goal</h2>
<p>According to weekly data between 2020 to 2022, we want to get to know the waves and peaks of COVID pandemic in these years.</p>
<h2 id="download-the-data">Download the data</h2>
<p>We will continue to use NCHS(National Center for Health Statistics) as our data source.<br>
Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=</a></p>]]></description><link>https://www.flamingbytes.com/blog/using-r-to-analyze-us-covid-waves-and-peaks/</link><guid isPermaLink="false">63c2f4d2639c6f0d6ee203e3</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 11 Jan 2023 18:33:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1593007791459-4b05e1158229?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGNvdmlkfGVufDB8fHx8MTY3MzcyMTIyMw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="goal">Goal</h2>
<img src="https://images.unsplash.com/photo-1593007791459-4b05e1158229?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGNvdmlkfGVufDB8fHx8MTY3MzcyMTIyMw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using R to analyze US COVID pandemic waves and peaks"><p>According to weekly data between 2020 to 2022, we want to get to know the waves and peaks of COVID pandemic in these years.</p>
<h2 id="download-the-data">Download the data</h2>
<p>We will continue to use NCHS(National Center for Health Statistics) as our data source.<br>
Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified</a>, and search <code>Provisional COVID-19 Death Counts by Week</code>, we will find the data we are intrest.</p>
<p><a href="https://data.cdc.gov/NCHS/Provisional-COVID-19-Death-Counts-by-Week-Ending-D/r8kw-7aab">https://data.cdc.gov/NCHS/Provisional-COVID-19-Death-Counts-by-Week-Ending-D/r8kw-7aab</a>, in this page, we can export data into csv file.</p>
<p>With that, we may get the data source csv, <code>Provisional_COVID-19_Death_Counts_by_Week_Ending_Date_and_State.csv</code>.</p>
<h2 id="load-the-data">Load the data</h2>
<pre><code>library(&quot;dplyr&quot;)
library(&quot;janitor&quot;)
library(&quot;tidyr&quot;)
library(&quot;readr&quot;)
df &lt;- readr::read_csv(file.path(getwd(), &quot;Provisional_COVID-19_Death_Counts_by_Week_Ending_Date_and_State.csv&quot;), col_names = TRUE)
df &lt;- clean_names(df)
tmp_start_date &lt;- strptime(df$start_date, &quot;%m/%d/%Y&quot;)
df$start_date &lt;- format(tmp_start_date, &quot;%Y-%m-%d&quot;)

 &gt; glimpse(df)
 Rows: 10,800
 Columns: 17
 $ data_as_of                             &lt;chr&gt; &quot;01/09/2023&quot;, &quot;01/09/2023&quot;, &quot;01&#x2026;
 $ start_date                             &lt;chr&gt; &quot;2019-12-29&quot;, &quot;2020-01-05&quot;, &quot;20&#x2026;
 $ end_date                               &lt;chr&gt; &quot;01/04/2020&quot;, &quot;01/11/2020&quot;, &quot;01&#x2026;
 $ group                                  &lt;chr&gt; &quot;By Week&quot;, &quot;By Week&quot;, &quot;By Week&quot;&#x2026;
 $ year                                   &lt;chr&gt; &quot;2019/2020&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2&#x2026;
 $ month                                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA,&#x2026;
 $ mmwr_week                              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, &#x2026;
 $ week_ending_date                       &lt;chr&gt; &quot;01/04/2020&quot;, &quot;01/11/2020&quot;, &quot;01&#x2026;
 $ state                                  &lt;chr&gt; &quot;United States&quot;, &quot;United States&#x2026;
 $ covid_19_deaths                        &lt;dbl&gt; 0, 1, 2, 3, 0, 4, 6, 6, 9, 38, &#x2026;
 $ total_deaths                           &lt;dbl&gt; 60176, 60734, 59362, 59162, 588&#x2026;
 $ percent_of_expected_deaths             &lt;dbl&gt; 98, 97, 98, 99, 99, 100, 100, 1&#x2026;
 $ pneumonia_deaths                       &lt;dbl&gt; 4111, 4153, 4066, 3915, 3818, 3&#x2026;
 $ pneumonia_and_covid_19_deaths          &lt;dbl&gt; 0, 1, 2, 0, 0, 1, 1, 3, 5, 19, &#x2026;
 $ influenza_deaths                       &lt;dbl&gt; 434, 475, 468, 500, 481, 520, 5&#x2026;
 $ pneumonia_influenza_or_covid_19_deaths &lt;dbl&gt; 4545, 4628, 4534, 4418, 4299, 4&#x2026;
 $ footnote                               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA,&#x2026;

 !&gt; df
 # A tibble: 10,800 &#xD7; 17
    data_as_of start_date end_d&#x2026;&#xB9; group year  month mmwr_&#x2026;&#xB2; week_&#x2026;&#xB3; state covid&#x2026;&#x2074;
    &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;
  1 01/09/2023 2019-12-29 01/04/&#x2026; By W&#x2026; 2019&#x2026;    NA       1 01/04/&#x2026; Unit&#x2026;       0
  2 01/09/2023 2020-01-05 01/11/&#x2026; By W&#x2026; 2020     NA       2 01/11/&#x2026; Unit&#x2026;       1
  3 01/09/2023 2020-01-12 01/18/&#x2026; By W&#x2026; 2020     NA       3 01/18/&#x2026; Unit&#x2026;       2
  4 01/09/2023 2020-01-19 01/25/&#x2026; By W&#x2026; 2020     NA       4 01/25/&#x2026; Unit&#x2026;       3
  5 01/09/2023 2020-01-26 02/01/&#x2026; By W&#x2026; 2020     NA       5 02/01/&#x2026; Unit&#x2026;       0
  6 01/09/2023 2020-02-02 02/08/&#x2026; By W&#x2026; 2020     NA       6 02/08/&#x2026; Unit&#x2026;       4
  7 01/09/2023 2020-02-09 02/15/&#x2026; By W&#x2026; 2020     NA       7 02/15/&#x2026; Unit&#x2026;       6
  8 01/09/2023 2020-02-16 02/22/&#x2026; By W&#x2026; 2020     NA       8 02/22/&#x2026; Unit&#x2026;       6
  9 01/09/2023 2020-02-23 02/29/&#x2026; By W&#x2026; 2020     NA       9 02/29/&#x2026; Unit&#x2026;       9
 10 01/09/2023 2020-03-01 03/07/&#x2026; By W&#x2026; 2020     NA      10 03/07/&#x2026; Unit&#x2026;      38
 # &#x2026; with 10,790 more rows, 7 more variables: total_deaths &lt;dbl&gt;,
 #   percent_of_expected_deaths &lt;dbl&gt;, pneumonia_deaths &lt;dbl&gt;,
 #   pneumonia_and_covid_19_deaths &lt;dbl&gt;, influenza_deaths &lt;dbl&gt;,
 #   pneumonia_influenza_or_covid_19_deaths &lt;dbl&gt;, footnote &lt;chr&gt;, and
 #   abbreviated variable names &#xB9; end_date, &#xB2; mmwr_week, &#xB3; week_ending_date,
 #   &#x2074; covid_19_deaths
 # &#x2139; Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names
</code></pre>
<h2 id="identify-the-data-we-want-to-focus">Identify the data we want to focus</h2>
<p>As we can see, there are 4 diffrent groups, and it has the <code>whole United states</code> and each state&apos;s data</p>
<pre><code>&gt; unique(df$group)
[1] &quot;By Week&quot;  &quot;By Month&quot; &quot;By Year&quot;  &quot;By Total&quot;
&gt;
</code></pre>
<p>We only want to get the weekly data, so we may want to <code>filter</code> with &quot;group=By Week&quot;, and &quot;state=United states&quot;<br>
in the mean time, we may only want to <code>select</code> only 2 columns.</p>
<ul>
<li>start_date</li>
<li>covid_19_deaths</li>
</ul>
<pre><code>&gt; df1 &lt;- df %&gt;%
    filter(state == &quot;United States&quot; &amp; group == &quot;By Week&quot;) %&gt;%
    select(start_date, covid_19_deaths)
&gt; print(df1,n=20)
+ &gt; # A tibble: 158 &#xD7; 2
   start_date covid_19_deaths
   &lt;chr&gt;                &lt;dbl&gt;
 1 2019-12-29               0
 2 2020-01-05               1
 3 2020-01-12               2
 4 2020-01-19               3
 5 2020-01-26               0
 6 2020-02-02               4
 7 2020-02-09               6
 8 2020-02-16               6
 9 2020-02-23               9
10 2020-03-01              38
11 2020-03-08              60
12 2020-03-15             588
13 2020-03-22            3226
14 2020-03-29           10141
15 2020-04-05           16347
16 2020-04-12           17221
17 2020-04-19           15557
18 2020-04-26           13223
19 2020-05-03           11243
20 2020-05-10            9239
...

</code></pre>
<h2 id="draw-the-graph-to-see-the-wave">Draw the graph to see the wave</h2>
<pre><code>library(&quot;ggplot2&quot;)
library(&quot;sjPlot&quot;)
p = ggplot(df1, aes( x=start_date, y=covid_19_deaths, group=1)) +
    geom_line(color=&quot;blue&quot;) +
    theme(axis.text.x=element_text(angle=45,hjust=1,size=5))
save_plot(&quot;covid_plot_weekly_wave.svg&quot;, fig = p, width=60, height=20)
</code></pre>
<p><img src="https://www.flamingbytes.com/content/images/posts/covid_plot_weekly_wave.svg" alt="Using R to analyze US COVID pandemic waves and peaks" loading="lazy"></p>
<h2 id="find-the-peak-by-r-mark-it-in-the-graph">Find the peak by R mark it in the graph</h2>
<p>From above graph, we can easily to figure out the waves and peaks, but we also can let R help us to do it, it&apos;s pretty useful if we have to deal with many data and many graphs.</p>
<p>To achive it, firstly we can call <code>findpeaks</code> from <code>pracma</code> library to find the peaks</p>
<pre><code>library(&quot;pracma&quot;)
+ peaks = findpeaks(df1$covid_19_deaths, npeaks=5,  sortstr=TRUE)
&gt; &gt; peaks
      [,1] [,2] [,3] [,4]
[1,] 26027   54   40   66
[2,] 21364  108   98  121
[3,] 17221   16    8   26
[4,] 15536   88   79   98
[5,]  8308   31   26   38
&gt;
</code></pre>
<p>The 2nd column means the the row index of the peak. in this case, we can tell, the 54th row has the top peak covid death number <code>26027</code>.</p>
<p>It&apos;s not very obvious which week(start_date) is hitting the peak, so we can do something like this.</p>
<pre><code>is_peak &lt;- vector( &quot;logical&quot; , length(df1$covid_19_deaths ))
df1$is_peak = is_peak

for (x in peaks[,2]) {
  df1$is_peak[x] = TRUE
}
</code></pre>
<p>As you can see, we added a new column <code>is_peak</code>, so we can use it to filter out those none peak data, sort the peak data points.</p>
<pre><code>!&gt; df2 = df1 %&gt;% filter(is_peak == TRUE)
 + df2[order(-df2$covid_19_deaths),]
 &gt; # A tibble: 5 &#xD7; 3
   start_date covid_19_deaths is_peak
   &lt;chr&gt;                &lt;dbl&gt; &lt;lgl&gt;
 1 2021-01-03           26027 TRUE
 2 2022-01-16           21364 TRUE
 3 2020-04-12           17221 TRUE
 4 2021-08-29           15536 TRUE
 5 2020-07-26            8308 TRUE
 &gt; &gt;
</code></pre>
<h2 id="hightlight-the-peak-points">Hightlight the peak points</h2>
<pre><code>p = ggplot(df1, aes(x=start_date, y=covid_19_deaths, group=1)) +
    geom_line(color=&quot;blue&quot;) +
    geom_point(data = . %&gt;% filter(is_peak == TRUE), stat=&quot;identity&quot;, size = 4, color = &quot;red&quot;) +
    scale_y_continuous(breaks=seq(0,30000,4000)) +
    theme(axis.text.x=element_text(angle=45,hjust=1,size=5))

save_plot(&quot;covid_plot_weekly_peak.svg&quot;, fig = p, width=60, height=20)
</code></pre>
<p><img src="https://www.flamingbytes.com/content/images/posts/covid_plot_weekly_peak.svg" alt="Using R to analyze US COVID pandemic waves and peaks" loading="lazy"></p>
<h2 id="other-finding">Other finding</h2>
<pre><code>!&gt; &gt; sum(df1$covid_19_deaths)
 [1] 1089714   ===&gt; the total covid_19_deaths death number from 2020 to 2022

!&gt; summary(df1$covid_19_deaths)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
       0    2223    4428    6897    9862   26027
 &gt;
!&gt; df3 &lt;- df %&gt;%
 +     filter(state == &quot;United States&quot; &amp; group == &quot;By Week&quot;) %&gt;%
 +     select(start_date, total_deaths)
 + sum(df3$total_deaths)
 + &gt; [1] 10077273  ===&gt; the total death number from 2020 to 2022 
 + summary(df3$total_deaths)
 &gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    7100   58522   60451   63780   68610   87415
 &gt;
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using R to analyze the quarterly US COVID data]]></title><description><![CDATA[An R example to analyze big amount of data]]></description><link>https://www.flamingbytes.com/blog/analysis-us-quartely-covid-data-from-cdc/</link><guid isPermaLink="false">63bced3f16d5d70e3a76d7f2</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 10 Jan 2023 05:02:33 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1454165804606-c3d57bc86b40?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1fHxkYXRhJTIwbWluaW5nfGVufDB8fHx8MTY3MzMyNTgxNw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="goal">Goal</h2>
<img src="https://images.unsplash.com/photo-1454165804606-c3d57bc86b40?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1fHxkYXRhJTIwbWluaW5nfGVufDB8fHx8MTY3MzMyNTgxNw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using R to analyze the quarterly US COVID data"><p>Based on the overall death data and COVID realated death data since 2019, we want to study the impact trend of COVID to the overall pupulation death of US in these years.</p>
<h2 id="download-the-data">Download the data</h2>
<p>We will use NCHS(National Center for Health Statistics) as our data source.<br>
Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified</a>, and search <code>VSRR Quarterly</code>, we will find the data we are intrested in.</p>
<p><a href="https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x">https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x</a></p>
<p>In this page, we can export data into csv file as <code>NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv</code></p>
<h2 id="take-a-quick-look-at-the-data">Take a quick look at the data</h2>
<p>To load the data:</p>
<pre><code># If &quot;readr&quot; not installed, run install.packages(&quot;readr&quot;) to install it
library(&quot;readr&quot;)
df &lt;- readr::read_csv(file.path(getwd(), &quot;NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv&quot;), col_names = TRUE)

</code></pre>
<p>To check the first few lines:</p>
<pre><code>&gt; head(df)
# A tibble: 6 &#xD7; 69
  Year a&#x2026;&#xB9; Time &#x2026;&#xB2; Cause&#x2026;&#xB3; Rate &#x2026;&#x2074; Unit  Overa&#x2026;&#x2075; Rate &#x2026;&#x2076; Rate &#x2026;&#x2077; Rate &#x2026;&#x2078; Rate &#x2026;&#x2079;
  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
1 2019 Q1  12 mon&#x2026; All ca&#x2026; Age-ad&#x2026; Deat&#x2026;   712.    600.    844.       NA      NA
2 2019 Q1  12 mon&#x2026; Alzhei&#x2026; Age-ad&#x2026; Deat&#x2026;    29.6    33.1    23.8      NA      NA
3 2019 Q1  12 mon&#x2026; COVID-&#x2026; Age-ad&#x2026; Deat&#x2026;    NA      NA      NA        NA      NA
4 2019 Q1  12 mon&#x2026; Cancer  Age-ad&#x2026; Deat&#x2026;   148.    128.    175.       NA      NA
5 2019 Q1  12 mon&#x2026; Chroni&#x2026; Age-ad&#x2026; Deat&#x2026;    11       7.7    14.7      NA      NA
6 2019 Q1  12 mon&#x2026; Chroni&#x2026; Age-ad&#x2026; Deat&#x2026;    38.5    35.7    42.4      NA      NA
# &#x2026; with 59 more variables: `Rate Age 15-24` &lt;dbl&gt;, `Rate Age 25-34` &lt;dbl&gt;,
#   `Rate Age 35-44` &lt;dbl&gt;, `Rate Age 45-54` &lt;dbl&gt;, `Rate Age 55-64` &lt;dbl&gt;,
#   `Rate 65-74` &lt;dbl&gt;, `Rate Age 75-84` &lt;dbl&gt;, `Rate Age 85 plus` &lt;dbl&gt;,
#   `Rate Alaska` &lt;dbl&gt;, `Rate Alabama` &lt;dbl&gt;, `Rate Arkansas` &lt;dbl&gt;,
#   `Rate Arizona` &lt;dbl&gt;, `Rate California` &lt;dbl&gt;, `Rate Colorado` &lt;dbl&gt;,
#   `Rate Connecticut` &lt;dbl&gt;, `Rate District of Columbia` &lt;dbl&gt;,
#   `Rate Delaware` &lt;dbl&gt;, `Rate Florida` &lt;dbl&gt;, `Rate Georgia` &lt;dbl&gt;, &#x2026;
# &#x2139; Use `colnames()` to see all variable names

</code></pre>
<p>To get a summary:</p>
<pre><code>summary(df)
 Year and Quarter   Time Period        Cause of Death      Rate Type        
 Length:1232        Length:1232        Length:1232        Length:1232       
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  

     Unit            Overall Rate     Rate Sex Female   Rate Sex Male    
 Length:1232        Min.   :   1.20   Min.   :   0.60   Min.   :   1.90  
 Class :character   1st Qu.:  11.40   1st Qu.:   6.95   1st Qu.:  13.20  
 Mode  :character   Median :  17.00   Median :  14.80   Median :  23.50  
                    Mean   :  80.51   Mean   :  70.56   Mean   :  91.63  
                    3rd Qu.:  50.60   3rd Qu.:  49.92   3rd Qu.:  65.42  
                    Max.   :1142.30   Max.   :1067.00   Max.   :1219.90  
                    NA&apos;s   :44        NA&apos;s   :44        NA&apos;s   :44       
  ...
</code></pre>
<p>To get glimpse from columns point of view:</p>
<pre><code>&gt; library(&quot;dplyr&quot;)
&gt; glimpse(df)
Rows: 1,232
Columns: 69
$ `Year and Quarter`          &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;&#x2026;
$ `Time Period`               &lt;chr&gt; &quot;12 months ending with quarter&quot;, &quot;12 month&#x2026;
$ `Cause of Death`            &lt;chr&gt; &quot;All causes&quot;, &quot;Alzheimer disease&quot;, &quot;COVID-&#x2026;
$ `Rate Type`                 &lt;chr&gt; &quot;Age-adjusted&quot;, &quot;Age-adjusted&quot;, &quot;Age-adjus&#x2026;
$ Unit                        &lt;chr&gt; &quot;Deaths per 100,000&quot;, &quot;Deaths per 100,000&quot;&#x2026;
$ `Overall Rate`              &lt;dbl&gt; 712.2, 29.6, NA, 148.1, 11.0, 38.5, 21.3, &#x2026;
$ `Rate Sex Female`           &lt;dbl&gt; 600.3, 33.1, NA, 127.9, 7.7, 35.7, 16.8, 1&#x2026;
$ `Rate Sex Male`             &lt;dbl&gt; 843.7, 23.8, NA, 175.4, 14.7, 42.4, 26.9, &#x2026;
$ `Rate Age 1-4`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 5-14`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 15-24`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 25-34`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 35-44`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 45-54`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 55-64`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate 65-74`                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 75-84`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
...
</code></pre>
<h2 id="clear-the-columns-name">Clear the columns name</h2>
<p>As we can see, the column names has space, it may cause some trouble while refering it in R, it&apos;s a common sugestion to convert space to &quot;_&quot; before doing any R o/p.</p>
<p>A good thing is , there is a R package can help us on it.</p>
<pre><code>&gt; library(&quot;janitor&quot;)
&gt; df &lt;- clean_names(df)
!&gt; glimpse(df)
 Rows: 1,232
 Columns: 69
 $ year_and_quarter          &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &#x2026;
 $ time_period               &lt;chr&gt; &quot;12 months ending with quarter&quot;, &quot;12 months &#x2026;
 $ cause_of_death            &lt;chr&gt; &quot;All causes&quot;, &quot;Alzheimer disease&quot;, &quot;COVID-19&#x2026;
 $ rate_type                 &lt;chr&gt; &quot;Age-adjusted&quot;, &quot;Age-adjusted&quot;, &quot;Age-adjuste&#x2026;
 $ unit                      &lt;chr&gt; &quot;Deaths per 100,000&quot;, &quot;Deaths per 100,000&quot;, &#x2026;
 $ overall_rate              &lt;dbl&gt; 712.2, 29.6, NA, 148.1, 11.0, 38.5, 21.3, 20&#x2026;
 $ rate_sex_female           &lt;dbl&gt; 600.3, 33.1, NA, 127.9, 7.7, 35.7, 16.8, 13.&#x2026;
 $ rate_sex_male             &lt;dbl&gt; 843.7, 23.8, NA, 175.4, 14.7, 42.4, 26.9, 27&#x2026;
 $ rate_age_1_4              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &#x2026;
</code></pre>
<p>As you can see, while we run <code>glimpse(df)</code>, the colum name has been changed from &quot;Year and Quarter&quot; to &quot;year_and_quarter&quot;.</p>
<h2 id="filter-and-select">Filter and select</h2>
<p>The raw data has so many columns and rows, but we just want to focus on those cols/rows we really have intrest.</p>
<p>We can use <code>filter</code>(applying on rows) and <code>select</code>(applying on columns) in such condition.</p>
<pre><code>&gt; df1 &lt;- df %&gt;%
      filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;All causes&quot;, &quot;COVID-19&quot;)) %&gt;%
      select(year_and_quarter, cause_of_death, overall_rate)
&gt; print(df1,n=50)
 # A tibble: 28 &#xD7; 3
    year_and_quarter cause_of_death overall_rate
    &lt;chr&gt;            &lt;chr&gt;                 &lt;dbl&gt;
  1 2019 Q1          All causes            910
  2 2019 Q1          COVID-19               NA
  3 2019 Q2          All causes            851.
  4 2019 Q2          COVID-19               NA
  5 2019 Q3          All causes            827.
  6 2019 Q3          COVID-19               NA
  7 2019 Q4          All causes            891.
  8 2019 Q4          COVID-19               NA
  9 2020 Q1          All causes            945.
 10 2020 Q1          COVID-19                8.2
 11 2020 Q2          All causes           1035.
 12 2020 Q2          COVID-19              137.
 13 2020 Q3          All causes            985.
 14 2020 Q3          COVID-19               87.3
 15 2020 Q4          All causes           1142.
 16 2020 Q4          COVID-19              193.
 17 2021 Q1          All causes           1116
 18 2021 Q1          COVID-19              191.
 19 2021 Q2          All causes            915.
 20 2021 Q2          COVID-19               42.4
 21 2021 Q3          All causes           1051.
 22 2021 Q3          COVID-19              138.
 23 2021 Q4          All causes           1092.
 24 2021 Q4          COVID-19              131.
 25 2022 Q1          All causes           1116
 26 2022 Q1          COVID-19              150.
 27 2022 Q2          All causes            899.
 28 2022 Q2          COVID-19               17.6
</code></pre>
<p><code>%&gt;%</code> looks strange, it&apos;s just like <code>| (pipe)</code> in linux shell command.</p>
<pre><code>df1 &lt;- df %&gt;%
    filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;All causes&quot;, &quot;COVID-19&quot;)) %&gt;%
    select(year_and_quarter, cause_of_death, overall_rate)
</code></pre>
<p>It means only keep those rows which meet those condition of <code>filter</code>, and those columns which meet condition of <code>select</code>.</p>
<h2 id="deal-with-na-value">Deal with NA value</h2>
<p>As you can see, in &quot;overall_rate&quot; column, there are few &quot;NA&quot; values, in this context it means 0, so we may want to convert it as 0 for future&apos;s process.</p>
<p>We can do it like as below.</p>
<pre><code> &gt; df1 &lt;- df1 %&gt;%
     mutate_at(c(&quot;overall_rate&quot;), ~coalesce(.,0))
</code></pre>
<p>It means , we want to convert all NA to 0 in &quot;overall_rate&quot; column.<br>
Now, let&apos;s check the df1 again, we can see all NA has been changed to 0.</p>
<pre><code>!+ &gt; df1
 # A tibble: 28 &#xD7; 3
    year_and_quarter cause_of_death overall_rate
    &lt;chr&gt;            &lt;chr&gt;                 &lt;dbl&gt;
  1 2019 Q1          All causes            910
  2 2019 Q1          COVID-19                0
  3 2019 Q2          All causes            851.
  4 2019 Q2          COVID-19                0
  5 2019 Q3          All causes            827.
  6 2019 Q3          COVID-19                0
  7 2019 Q4          All causes            891.
  8 2019 Q4          COVID-19                0
  9 2020 Q1          All causes            945.
 10 2020 Q1          COVID-19                8.2
 # &#x2026; with 18 more rows
</code></pre>
<h2 id="draw-diagram-for-the-whole-us-data">Draw diagram for the whole US data</h2>
<p>To draw a diagram directly:</p>
<pre><code>&gt; ggplot(df1, aes(fill=cause_of_death, x=year_and_quarter, y=overall_rate)) +
    geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) +
    geom_col() +
    geom_smooth(aes(group=cause_of_death)) +
    scale_y_continuous(breaks=seq(0,1500,100))
    theme_bw()
</code></pre>
<p>To save the diagram in a file:</p>
<pre><code>library(sjPlot)
p = ggplot(df1, aes(fill=cause_of_death, x=year_and_quarter, y=overall_rate)) +
    geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) +
    geom_col() +
    geom_smooth(aes(group=cause_of_death)) +
    scale_y_continuous(breaks=seq(0,1500,100)) +
    theme_bw()

save_plot(&quot;covid_plot.svg&quot;, fig = p, width=30, height=20)
</code></pre>
<p>The diagram looks like below.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/image-1.png" class="kg-image" alt="Using R to analyze the quarterly US COVID data" loading="lazy" width="1133" height="755" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/image-1.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/image-1.png 1000w, https://www.flamingbytes.com/content/images/2023/01/image-1.png 1133w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h2 id="draw-diagram-for-california-data">Draw diagram for California data</h2>
<p>Do you want to try it by yourself?</p>
<h2 id="createcalculate-a-new-column-for-covid-ratio">Create/Calculate a new column for covid ratio</h2>
<p>Somehow, we want to get to know the trend for covid ratio.</p>
<ul>
<li>covid_ratio = overall_rate_of_covid / overall_rate_of_all_causes</li>
</ul>
<pre><code>covid_death_rate &lt;- df1 %&gt;%
    filter(cause_of_death == &quot;COVID-19&quot;) %&gt;%
    select(&quot;overall_rate&quot;)

all_causes_rate &lt;- df1 %&gt;%
    filter(cause_of_death == &quot;All causes&quot;) %&gt;%
    select(overall_rate)

covid_ratio &lt;- covid_death_rate / all_causes_rate

df_ratio &lt;- df1 %&gt;%
    filter(cause_of_death == &quot;All causes&quot;) %&gt;%
    select(year_and_quarter)
df_ratio[&quot;covid_ratio&quot;] = covid_ratio


&gt; print(df_ratio)
# A tibble: 14 &#xD7; 2
   year_and_quarter covid_ratio
   &lt;chr&gt;                  &lt;dbl&gt;
 1 2019 Q1              0      
 2 2019 Q2              0      
 3 2019 Q3              0      
 4 2019 Q4              0      
 5 2020 Q1              0.00868
 6 2020 Q2              0.132  
 7 2020 Q3              0.0886 
 8 2020 Q4              0.169  
 9 2021 Q1              0.171  
10 2021 Q2              0.0463 
11 2021 Q3              0.131  
12 2021 Q4              0.120  
13 2022 Q1              0.134  
14 2022 Q2              0.0196 
</code></pre>
<h2 id="draw-diagram-for-covid-ratio">Draw diagram for covid ratio</h2>
<p>Do you want to try it by yourself?</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Getting started with R]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="installation">Installation</h2>
<p>The simplest way is to use Homebrew:</p>
<pre><code>$ brew install r
</code></pre>
<p>Another way is to download installation package from <a href="https://cloud.r-project.org/">https://cloud.r-project.org/</a></p>
<h2 id="hello-world-of-r">&quot;Hello world&quot; of R</h2>
<h3 id="run-it-from-r-console">Run it from R console</h3>
<p>Command <code>R</code> will start a R console, and you can run R code inside R console.</p>]]></description><link>https://www.flamingbytes.com/blog/getting-started-with-r/</link><guid isPermaLink="false">63bcea3316d5d70e3a76d7c5</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 08 Jan 2023 04:42:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1640158615573-cd28feb1bf4e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fGRhdGElMjBtaW58ZW58MHx8fHwxNjczMzI1ODE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="installation">Installation</h2>
<img src="https://images.unsplash.com/photo-1640158615573-cd28feb1bf4e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fGRhdGElMjBtaW58ZW58MHx8fHwxNjczMzI1ODE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Getting started with R"><p>The simplest way is to use Homebrew:</p>
<pre><code>$ brew install r
</code></pre>
<p>Another way is to download installation package from <a href="https://cloud.r-project.org/">https://cloud.r-project.org/</a></p>
<h2 id="hello-world-of-r">&quot;Hello world&quot; of R</h2>
<h3 id="run-it-from-r-console">Run it from R console</h3>
<p>Command <code>R</code> will start a R console, and you can run R code inside R console.</p>
<pre><code>(base) &#x279C;  benchling git:(b_test_pr) &#x2717; R

R version 4.2.2 (2022-10-31) -- &quot;Innocent and Trusting&quot;
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

...
&gt; print(&quot;hello,world&quot;)
[1] &quot;hello,world&quot;
</code></pre>
<h3 id="run-it-from-terminal">Run it from terminal</h3>
<p><code>Rscript</code> is a binary front-end to R, for use in scripting applications, see <a href="https://linux.die.net/man/1/rscript">https://linux.die.net/man/1/rscript</a> for more detail.</p>
<pre><code>(base) &#x279C;  R git:(b_test_pr) &#x2717; cat hello.R
print(&quot;hello,world&quot;)
(base) &#x279C;  R git:(b_test_pr) &#x2717; Rscript hello.R
[1] &quot;hello,world&quot;
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: html--><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8578408127828851" crossorigin="anonymous"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-8578408127828851" data-ad-slot="5596270594"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script><!--kg-card-end: html--><!--kg-card-begin: markdown--><h2 id="install-commonly-used-packages">Install commonly used packages</h2>
<p>R installation package comes along with a lot of useful packages, besides that, there are a lot of useful packages available from <a href="https://cran.r-project.org/web/packages/">CRAN</a>.</p>
<p>Here are top 10 most important packages in R for data science.</p>
<ul>
<li>ggplot2</li>
<li>data.table</li>
<li>dplyr</li>
<li>tidyr</li>
<li>Shiny</li>
<li>plotly</li>
<li>knitr</li>
<li>mlr3</li>
</ul>
<p>To install those packages from CRAN, we can just simiply follow below steps.</p>
<ul>
<li>Start R console</li>
<li>Call &quot;install.packages(XXX)&quot;</li>
</ul>
<p>Here is an example:</p>
<pre><code>&gt; install.packages(&quot;mlr3&quot;)
--- Please select a CRAN mirror for use in this session ---
Secure CRAN mirrors

 1: 0-Cloud [https]
 2: Australia (Canberra) [https]
 3: Australia (Melbourne 1) [https]
 ....
 Selection: 1
also installing the dependencies &#x2018;globals&#x2019;, &#x2018;listenv&#x2019;, &#x2018;PRROC&#x2019;, &#x2018;future&#x2019;, &#x2018;future.apply&#x2019;, &#x2018;lgr&#x2019;, &#x2018;mlbench&#x2019;, &#x2018;mlr3measures&#x2019;, &#x2018;mlr3misc&#x2019;, &#x2018;parallelly&#x2019;, &#x2018;palmerpenguins&#x2019;, &#x2018;paradox&#x2019;

trying URL &apos;https://cloud.r-project.org/bin/macosx/big-sur-arm64/contrib/4.2/globals_0.16.2.tgz&apos;
...
&gt;&gt; library(mlr3)
&gt; ?mlr3
</code></pre>
<p>As above, after installation completes, we can try to run <code>library(&lt;package name&gt;)</code> to verify, and run <code>?&lt;package name&gt;</code> to see its document.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.w3schools.com/r/r_get_started.asp">https://www.w3schools.com/r/r_get_started.asp</a></li>
<li><a href="https://www.datacamp.com/tutorial/top-ten-most-important-packages-in-r-for-data-science">https://www.datacamp.com/tutorial/top-ten-most-important-packages-in-r-for-data-science</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[How to redirect the default "404 page not found" error page in Ghost blog]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>When users access a non-existent page, they may see a page which shows the message &quot;404 page not found&quot; in Ghost blog.</p>
<p>We can redirect the page to another page by creating a <strong>error-404.hbs</strong> file under the theme folder.</p>
<p>In the following example, we redirect the users</p>]]></description><link>https://www.flamingbytes.com/blog/how-to-redirect-the-default-404-page-not-found-error-page-in-ghost-blog/</link><guid isPermaLink="false">63c335e0acbe8dfd252da58a</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Thu, 05 Jan 2023 23:23:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1499750310107-5fef28a66643?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJsb2d8ZW58MHx8fHwxNjczNzM4NjE1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1499750310107-5fef28a66643?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJsb2d8ZW58MHx8fHwxNjczNzM4NjE1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="How to redirect the default &quot;404 page not found&quot; error page in Ghost blog"><p>When users access a non-existent page, they may see a page which shows the message &quot;404 page not found&quot; in Ghost blog.</p>
<p>We can redirect the page to another page by creating a <strong>error-404.hbs</strong> file under the theme folder.</p>
<p>In the following example, we redirect the users to the website home page when they access a non-existent page.</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=&apos;https://www.flamingbytes.com/&apos;&quot; /&gt;
  &lt;/head&gt;
  &lt;body&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[How to install GUI desktop on CentOS 7]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Install GNOME desktop via the groups option of yum:</p>
<pre><code>$ yum update
$ yum -y groups install &quot;GNOME Desktop&quot;
</code></pre>
<p>Inform the startx command which desktop env to run:</p>
<pre><code>$ echo &quot;exec gnome-session&quot; &gt;&gt; ~/.xinitrc
</code></pre>
<p>Manually start the GUI desktop:</p>
<pre><code>$ startx
</code></pre>
<p>Automaticlly start the GUI desktop after reboot:</p>
<pre><code>$ systemctl</code></pre>]]></description><link>https://www.flamingbytes.com/blog/how-to-install-gui-desktop-on-centos-7/</link><guid isPermaLink="false">63c2f5b9639c6f0d6ee203fa</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 04 Jan 2023 18:35:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1508739773434-c26b3d09e071?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGRlc2t0b3B8ZW58MHx8fHwxNjczNzIxNDg0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1508739773434-c26b3d09e071?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGRlc2t0b3B8ZW58MHx8fHwxNjczNzIxNDg0&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="How to install GUI desktop on CentOS 7"><p>Install GNOME desktop via the groups option of yum:</p>
<pre><code>$ yum update
$ yum -y groups install &quot;GNOME Desktop&quot;
</code></pre>
<p>Inform the startx command which desktop env to run:</p>
<pre><code>$ echo &quot;exec gnome-session&quot; &gt;&gt; ~/.xinitrc
</code></pre>
<p>Manually start the GUI desktop:</p>
<pre><code>$ startx
</code></pre>
<p>Automaticlly start the GUI desktop after reboot:</p>
<pre><code>$ systemctl set-default graphical.target
$ reboot
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using signals with kill command in Linux]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="usage-of-signals-with-kill">Usage of signals with kill</h2>
<pre><code>$ kill -l
 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP
 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1
11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM
16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP
21) SIGTTIN 22)</code></pre>]]></description><link>https://www.flamingbytes.com/blog/using-signals-with-kill-in-linux/</link><guid isPermaLink="false">63c37c5215cd8c0d7b6ee906</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 04 Jan 2023 04:15:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1530635481267-00edc014d006?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDY3fHxzaWduYWx8ZW58MHx8fHwxNjczNzU2MTMy&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="usage-of-signals-with-kill">Usage of signals with kill</h2>
<pre><code>$ kill -l
 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP
 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1
11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM
16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP
21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ
26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR
31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3
38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8
43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7
58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2
63) SIGRTMAX-1 64) SIGRTMAX
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="an-example-to-send-sigint-signal-in-shell">An example to send SIGINT signal in shell</h2>
<img src="https://images.unsplash.com/photo-1530635481267-00edc014d006?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDY3fHxzaWduYWx8ZW58MHx8fHwxNjczNzU2MTMy&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using signals with kill command in Linux"><p>In the following example, we start strace for a given process for given amounts of seconds. Once the time is up, we send SIGINT(Ctrl+C) signal to stop the tracing process. And a trace report will be generated to the output file strace.c.out.</p>
<pre><code>$ cat strace_summary.sh
processname=$1
duration=$2
while true
do
    strace -p `pidof $processname` -c &gt; strace.c.out 2&gt;&amp;1 &amp;
    sleep $duration
    break
done

kill -2 `pidof strace`
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="reference">Reference</h2>
<ul>
<li><a href="https://linux.die.net/Bash-Beginners-Guide/sect_12_01.html#:~:text=The%20interrupt%20signal%2C%20sends%20SIGINT,job%20running%20in%20the%20foreground.&amp;text=The%20delayed%20suspend%20character.,background%20or%20kill%20the%20process.">Signals in Linux</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using virtctl to access virtual machine in Kubernetes]]></title><description><![CDATA[<!--kg-card-begin: html--><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8578408127828851" crossorigin="anonymous">
</script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-8578408127828851" data-ad-slot="5596270594">
</ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script><!--kg-card-end: html--><!--kg-card-begin: markdown--><h2 id="install-the-virtctl-client-tool">Install the virtctl client tool</h2>
<p>Basic VirtualMachineInstance operations can be performed with the stock kubectl utility. However, the virtctl binary utility is required to use advanced features such as:</p>
<ul>
<li>Serial and graphical console access</li>
</ul>
<p>It also provides convenience commands for:</p>
<ul>
<li>Starting and stopping VirtualMachineInstances</li>
<li>Live migrating</li></ul>]]></description><link>https://www.flamingbytes.com/blog/using-virtctl-to-access-virtual-machine-in-kubernetes/</link><guid isPermaLink="false">63b4b4a91527a00d97449fdf</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 02 Jan 2023 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372335936-3dc4ff716017?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: html--><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8578408127828851" crossorigin="anonymous">
</script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-8578408127828851" data-ad-slot="5596270594">
</ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script><!--kg-card-end: html--><!--kg-card-begin: markdown--><h2 id="install-the-virtctl-client-tool">Install the virtctl client tool</h2>
<img src="https://images.unsplash.com/photo-1667372335936-3dc4ff716017?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using virtctl to access virtual machine in Kubernetes"><p>Basic VirtualMachineInstance operations can be performed with the stock kubectl utility. However, the virtctl binary utility is required to use advanced features such as:</p>
<ul>
<li>Serial and graphical console access</li>
</ul>
<p>It also provides convenience commands for:</p>
<ul>
<li>Starting and stopping VirtualMachineInstances</li>
<li>Live migrating VirtualMachineInstances</li>
<li>Uploading virtual machine disk images</li>
</ul>
<p>There are two ways to get it:</p>
<ul>
<li>the most recent version of the tool can be retrieved from the official release page</li>
<li>it can be installed as a kubectl plugin using krew</li>
</ul>
<p>Example:</p>
<pre><code class="language-shell">$ export VERSION=v0.48.1
$ wget https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64
$ ln -s virtctl-v0.48.1-linux-amd64 virtctl
$ chmod +x virtctl-v0.48.1-linux-amd64
$ ./virtctl version
Client Version: version.Info{GitVersion:&quot;v0.48.1&quot;, ...}
</code></pre>
<h2 id="access-the-virtual-machine-console">Access the virtual machine console</h2>
<pre><code class="language-shell">$ ./virtctl -h
virtctl controls virtual machine related operations on your kubernetes cluster.

Available Commands:
  addvolume         add a volume to a running VM
  console           Connect to a console of a virtual machine instance.
  expose            Expose a virtual machine instance, virtual machine, or virtual machine instance replica set as a new service.
  fslist            Return full list of filesystems available on the guest machine.
  guestfs           Start a shell into the libguestfs pod
  guestosinfo       Return guest agent info about operating system.
  help              Help about any command
  image-upload      Upload a VM image to a DataVolume/PersistentVolumeClaim.
  migrate           Migrate a virtual machine.
  pause             Pause a virtual machine
  permitted-devices List the permitted devices for vmis.
  port-forward      Forward local ports to a virtualmachine or virtualmachineinstance.
  removevolume      remove a volume from a running VM
  restart           Restart a virtual machine.
  soft-reboot       Soft reboot a virtual machine instance
  ssh               Open a SSH connection to a virtual machine instance.
  start             Start a virtual machine.
  stop              Stop a virtual machine.
  unpause           Unpause a virtual machine
  usbredir          Redirect a usb device to a virtual machine instance.
  userlist          Return full list of logged in users on the guest machine.
  version           Print the client and server version information.
  vnc               Open a vnc connection to a virtual machine instance.

Use &quot;virtctl &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;virtctl options&quot; for a list of global command-line options (applies to all commands).

$ ./virtctl console vm1
[root@vm1 output]# hostname
vm1
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/kubevirt/kubevirt/releases">https://github.com/kubevirt/kubevirt/releases</a></li>
<li><a href="https://kubevirt.io/user-guide/operations/virtctl_client_tool/">https://kubevirt.io/user-guide/operations/virtctl_client_tool/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using node selector to assign virtual machines to a node]]></title><description><![CDATA[Kubernetes node selector]]></description><link>https://www.flamingbytes.com/blog/using-node-selector-to-assign-virtual-mahcines-to-a-node/</link><guid isPermaLink="false">63b4b4011527a00d97449fb7</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Fri, 30 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372335962-5fd503a8ae5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1667372335962-5fd503a8ae5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using node selector to assign virtual machines to a node"><p>In some circumstances, we want to control which node the virtual machine or pod deploys to. The <strong>node selector</strong> can be used to assign virtual machine or pod to a node.</p>
<h2 id="add-label-to-a-node">Add label to a node</h2>
<p>The label can be added to a node from either command line or openshift web console.</p>
<pre><code class="language-shell">$ oc get nodes
NAME                   STATUS                     ROLES    AGE   VERSION
master1   Ready                      master   46h   v1.20.11+63f841a
master2   Ready                      master   46h   v1.20.11+63f841a
master3   Ready                      master   46h   v1.20.11+63f841a
worker1    Ready                      worker   45h   v1.20.11+63f841a
worker2    Ready                      worker   45h   v1.20.11+63f841a
worker3    Ready,SchedulingDisabled   worker   45h   v1.20.11+63f841a

$ oc label nodes worker1 worker-node-name=w1

$ oc describe node worker1 | grep worker-node-name
                    worker-node-name=w1

$ oc get nodes --show-labels
NAME       STATUS                     ROLES    AGE   VERSION            LABELS
worker1    Ready                      worker   45h   v1.20.11+63f841a   &lt;omitted..&gt;worker-node-name=w1
&lt;omitted..&gt;
</code></pre>
<p>This can also be done through openshift web console by clicking the &quot;edit labels&quot; option on a node.<br>
<img src="https://www.flamingbytes.com/content/images/posts/edit-labels.png" alt="Using node selector to assign virtual machines to a node" loading="lazy"></p>
<h2 id="add-a-nodeselector-field-to-the-virtual-machine">Add a nodeSelector field to the virtual machine</h2>
<p>A node selector can be added through openshift web console.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/node-selector-1.png" alt="Using node selector to assign virtual machines to a node" loading="lazy"></p>
<p><img src="https://www.flamingbytes.com/content/images/posts/nodeselector.png" alt="Using node selector to assign virtual machines to a node" loading="lazy"></p>
<pre><code class="language-shell">$ oc describe vm vm1
    Spec:
      Node Selector:
        Worker - Node - Name:            w1
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Deploy application with kubernetes statefulset]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>StatefulSet is the workload API object used to manage stateful applications.</p>
<p>Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are</p>]]></description><link>https://www.flamingbytes.com/blog/deploy-application-with-kubernetes-statefulset/</link><guid isPermaLink="false">63b4b0631527a00d97449efe</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Thu, 29 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372525724-16cede94db0b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1667372525724-16cede94db0b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Deploy application with kubernetes statefulset"><p>StatefulSet is the workload API object used to manage stateful applications.</p>
<p>Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p>
<p>If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.</p>
<p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">Source</a></p>
<p>The example below demonstrates the components of a StatefulSet.</p>
<pre><code class="language-shell">$ cat perfbench-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: perfbench
spec:
  serviceName: perfbench
  replicas: 1
  selector:
    matchLabels:
      app: perfbench
  template:
    metadata:
      labels:
        app: perfbench
    spec:
      containers:
      - name: perfbench
        image: noname/perfbench:latest
        volumeMounts:
        - name: perfbench-data
          mountPath: /perfdata
        - name: perfbench-log
          mountPath: /perflog
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: perfbench-data
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 20Gi
  - metadata:
      name: perfbench-log
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
</code></pre>
<p>The statefulset can be created as below.</p>
<pre><code class="language-shell">$ kubectl apply -f perfbench-statefulset.yaml
</code></pre>
<p>In case of any failure to create the statefulset, you can check the events with the following commands.</p>
<pre><code class="language-shell">$ kubectl describe pod perfbench-0
Name:         perfbench-0
Namespace:    default
Priority:     0
Node:         &lt;hostname&gt;/&lt;ip-address&gt;
Start Time:   Sat, 26 Feb 2022 06:22:27 +0000
Labels:       app=perfbench
              controller-revision-hash=perfbench-7657fb8779
              statefulset.kubernetes.io/pod-name=perfbench-0
Annotations:  cni.projectcalico.org/containerID: c8569d9a3f01f546ca92fb2d0cba98d4d971933e53ab83373ed34c91040d92bc
              cni.projectcalico.org/podIP: 192.168.201.145/32
              cni.projectcalico.org/podIPs: 192.168.201.145/32
Status:       Running
IP:           192.168.201.145
IPs:
  IP:           192.168.201.145
Controlled By:  StatefulSet/perfbench
Containers:
  perfbench:
    Container ID:   docker://b6a2c838837395c1c85913d4735e3167d3cea6b5ed3ade276584461d643eaee5
    Image:          noname/perfbench:latest
    Image ID:       docker-pullable://&lt;noname/perfbench@sha256:5460e3c04ea972afcde3db092b514919867f87974d012f046c538ac816c7aaae
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Sat, 26 Feb 2022 06:22:35 +0000
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /perfdata from perfbench-data (rw)
      /perflog from perfbench-log (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x8qhw (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  perfbench-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  perfbench-data-perfbench-0
    ReadOnly:   false
  perfbench-log:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  perfbench-log-perfbench-0
    ReadOnly:   false
  kube-api-access-x8qhw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  20s (x2 over 21s)  default-scheduler  0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims.
  Normal   Scheduled         18s                default-scheduler  Successfully assigned default/perfbench-0 to &lt;hostname&gt;
  Normal   Pulling           16s                kubelet            Pulling image &quot;&lt;noname/perfbench:latest&quot;
  Normal   Pulled            11s                kubelet            Successfully pulled image &quot;&lt;noname/perfbench:latest&quot; in 4.368378217s
  Normal   Created           10s                kubelet            Created container perfbench
  Normal   Started           10s                kubelet            Started container perfbench
</code></pre>
<p>You can check the pod status and login to the pod container as below.</p>
<pre><code class="language-shell">$ kubectl get pod
NAME         READY   STATUS    RESTARTS   AGE
perfbench-0   1/1     Running   0          3m44s

$ kubectl exec -it perfbench-0 -- bash
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/containers/images/">https://kubernetes.io/docs/concepts/containers/images/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Keep a docker container running and not exiting]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>In the kubernetes environment, we can keep a container(pod) alive and avoid it exits immediately after starting.</p>
<h1 id="method-one">Method one</h1>
<ol>
<li>
<p>Use a long-time-run command in Dockerfile</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
</code></pre>
<p>or</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep infinity&</code></pre></li></ol>]]></description><link>https://www.flamingbytes.com/blog/keep-a-docker-container-running-and-not-exiting/</link><guid isPermaLink="false">63b4b02a1527a00d97449ef1</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 28 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372335937-d03be6fb0a9c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1667372335937-d03be6fb0a9c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Keep a docker container running and not exiting"><p>In the kubernetes environment, we can keep a container(pod) alive and avoid it exits immediately after starting.</p>
<h1 id="method-one">Method one</h1>
<ol>
<li>
<p>Use a long-time-run command in Dockerfile</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
</code></pre>
<p>or</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;]
</code></pre>
</li>
<li>
<p>Build the docker image and push to docker repository</p>
</li>
<li>
<p>Launch the container</p>
<pre><code class="language-shell">$ kubectl run mycontainer -it --image=&lt;docker-image-name&gt;
</code></pre>
</li>
</ol>
<h1 id="method-two">Method two</h1>
<p>When to deploy an application with kubernetes statefulset, we also can add it to the statefulset yaml file instead of adding it to the docker image through Dockerfile.</p>
<pre><code class="language-shell">$ cat myapp.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp
spec:
  serviceName: myapp
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: noname/myapp:latest
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
        imagePullPolicy: Always
        volumeMounts:
        - name: myapp-data
          mountPath: /data
        - name: myapp-log
          mountPath: /log
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: myapp-data
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 20Gi
  - metadata:
      name: myapp-log
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi

$ kubectl apply -f myapp.yaml          
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Hosting the Ghost blog on GitHub]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h1 id="install-ghost-locally">Install Ghost locally</h1>
<p><a href="https://pages.github.com/">GitHub Pages</a> is a great solution for hosting static website. If you use the Ghost to manage your website, you can <a href="https://ghost.org/docs/install/local/">install Ghost locally</a> and convert it to a static website in order to host it on Github.</p>
<h2 id="install-ubuntu-virtual-machine-on-windowsoptional">Install Ubuntu virtual machine on Windows(optional)</h2>
<h2 id="install-ghost-cli">Install Ghost-CLI</h2>]]></description><link>https://www.flamingbytes.com/blog/ghost-github/</link><guid isPermaLink="false">63afb59696dbf412f1f80301</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 27 Dec 2022 04:07:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1543616991-75a2c125ff5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1MXx8YmxvZ3xlbnwwfHx8fDE2NzI0NjIyOTE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h1 id="install-ghost-locally">Install Ghost locally</h1>
<img src="https://images.unsplash.com/photo-1543616991-75a2c125ff5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1MXx8YmxvZ3xlbnwwfHx8fDE2NzI0NjIyOTE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Hosting the Ghost blog on GitHub"><p><a href="https://pages.github.com/">GitHub Pages</a> is a great solution for hosting static website. If you use the Ghost to manage your website, you can <a href="https://ghost.org/docs/install/local/">install Ghost locally</a> and convert it to a static website in order to host it on Github.</p>
<h2 id="install-ubuntu-virtual-machine-on-windowsoptional">Install Ubuntu virtual machine on Windows(optional)</h2>
<h2 id="install-ghost-cli">Install Ghost-CLI</h2>
<p>Ghost-CLI is a commandline tool to help you get Ghost installed and configured for use, quickly and easily.</p>
<pre><code>$ npm install ghost-cli@latest -g
</code></pre>
<p>Ghost runs as background process and remains running until you stop or restart it. The following are some useful commands:</p>
<pre><code>ghost help
ghost ls
ghost log
ghost stop
ghost start
</code></pre>
<h2 id="install-ghost">Install Ghost</h2>
<pre><code>$ mkdir my-ghost-website
$ cd my-ghost-website
$ ghost install local
</code></pre>
<p>Once the Ghost is installed you can access the website on <a href="https://www.flamingbytes.com/">https://www.flamingbytes.com</a> and <a href="https://www.flamingbytes.com/ghost">https://www.flamingbytes.com/ghost</a> for Ghost Admin.</p>
<h1 id="generate-the-static-website">Generate the static website</h1>
<p>In order to generate the static website, we&apos;ll use the Ghost static site generator. For the Linux and macOS, this tool can be used directly.</p>
<pre><code>$ sudo npm install -g ghost-static-site-generator
</code></pre>
<p>Now, we can push the generated static pages to the Github repository named <strong>username.github.io</strong>.</p>
<p>To generate the static pages, we can run the following command. The static pages are generated in a folder called <em>static</em>.</p>
<pre><code>$ gssg --url https://username.github.io
</code></pre>
<h1 id="push-the-static-pages-to-github">Push the static pages to Github</h1>
<p>Before pushing the static pages to Github, a repository called <em>username.github.io</em> should be created on Github website.</p>
<p>The static pages can be pushed to Github repository as below.</p>
<pre><code>$ cd static
$ git init .
$ git remote add origin https://github.com/username/username.github.io.git
$ git branch -M main
$ git add .
$ git commit -m &apos;Init my website&apos;
$ git push -u origin main
</code></pre>
<p>After the push, Github will build and deploy the pages automatically. And the updated website will be available to access in a few minutes.</p>
<h1 id="reference">Reference</h1>
<ul>
<li><a href="https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl">https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl</a></li>
<li><a href="https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text=Install%20and%20run%20Ghost%20locally,static%20website%20to%20GitHub%20Pages">https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text=Install and run Ghost locally,static website to GitHub Pages</a></li>
<li><a href="https://ghost.org/docs/reinstall/">https://ghost.org/docs/reinstall/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Pull an image from a private docker registry in Kubernetes Pod]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="log-in-to-docker-hub">Log in to Docker Hub</h2>
<p>In order to pull a image from Docker Hub, you must authenticate with a registry. Use <strong>docker</strong> tool to log in to the Docker Hub as below. A username and password is needed to log in.</p>
<pre><code class="language-shell">$ docker login
</code></pre>
<p>The login process creates or updates the</p>]]></description><link>https://www.flamingbytes.com/blog/pull-an-image-from-a-private-docker-registry-in-kubernetes-pod/</link><guid isPermaLink="false">63b4afde1527a00d97449ee4</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 27 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372459510-55b5e2087cd0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="log-in-to-docker-hub">Log in to Docker Hub</h2>
<img src="https://images.unsplash.com/photo-1667372459510-55b5e2087cd0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Pull an image from a private docker registry in Kubernetes Pod"><p>In order to pull a image from Docker Hub, you must authenticate with a registry. Use <strong>docker</strong> tool to log in to the Docker Hub as below. A username and password is needed to log in.</p>
<pre><code class="language-shell">$ docker login
</code></pre>
<p>The login process creates or updates the config.json file which holds an authorization token.</p>
<pre><code class="language-shell">$ cat /root/.docker/config.json
{
	&quot;auths&quot;: {
		&quot;https://index.docker.io/v1/&quot;: {
			&quot;auth&quot;: &quot;xxx=&quot;
		}
	}
}
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: html--><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8578408127828851" crossorigin="anonymous"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-8578408127828851" data-ad-slot="5596270594"></ins>
<script>(adsbygoogle = window.adsbygoogle || []).push({});</script><!--kg-card-end: html--><!--kg-card-begin: markdown--><h2 id="create-a-secret-based-on-existing-credentials">Create a Secret based on existing credentials</h2>
<p>A Kubernetes cluster uses the Secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.</p>
<p>If you already ran docker login, you can copy that credential into Kubernetes:</p>
<pre><code class="language-shell">$ kubectl create secret generic regcred --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson
</code></pre>
<p>You can inspect the Secret as below.</p>
<pre><code class="language-shell">$ kubectl get secret regcred --output=yaml

apiVersion: v1
data:
  .dockerconfigjson: &lt;base64-formatted-docker-credentials&gt;
kind: Secret
metadata:
  creationTimestamp: &quot;2022-02-28T22:25:43Z&quot;
  name: regcred
  namespace: default
  resourceVersion: &quot;1503624&quot;
  uid: yyy
type: kubernetes.io/dockerconfigjson
</code></pre>
<p>The value of the .dockerconfigjson field is a base64 representation of your Docker credentials. To understand what is in the .dockerconfigjson field, convert the secret data to a readable format:</p>
<pre><code class="language-shell">$ kubectl get secret regcred --output=&quot;jsonpath={.data.\.dockerconfigjson}&quot; | base64 --decode
{
	&quot;auths&quot;: {
		&quot;https://index.docker.io/v1/&quot;: {
			&quot;auth&quot;: &quot;xxx=&quot;
		}
	}
}
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: html--><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8578408127828851" crossorigin="anonymous"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-8578408127828851" data-ad-slot="5596270594"></ins>
<script>(adsbygoogle = window.adsbygoogle || []).push({});</script><!--kg-card-end: html--><!--kg-card-begin: markdown--><h2 id="create-a-pod-that-uses-the-secret-to-pull-image">Create a Pod that uses the Secret to pull image</h2>
<pre><code class="language-shell">$ vi my-private-reg-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: &lt;your-private-image&gt;
  imagePullSecrets:
  - name: regcred

$ kubectl apply -f my-private-reg-pod.yaml
$ kubectl get pod private-reg  
</code></pre>
<p>Note that the <strong>imagePullSecrets</strong> field specifies that Kubernetes should get the credentials from a Secret named <em>regcred</em> in order to pull a container image from Docker Hub.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Deploy kubernetes statefulset with affinity]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Here is an example to demonstrate how to assign statefulset and pod to the target worker node.</p>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">affinity</a> helps define which node will be assigned for the pod by using the node labels.</p>
<pre><code class="language-shell">$ kubectl get nodes -o wide --show-labels

$ kubectl label nodes &lt;node-name&gt; portworx.io/fio=true

$ cat</code></pre>]]></description><link>https://www.flamingbytes.com/blog/deploy-kubernetes-statefulset-with-affinity/</link><guid isPermaLink="false">63b495a11527a00d97449da1</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 27 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1605745341075-1b7460b99df8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRvY2tlcnxlbnwwfHx8fDE2NzI3NzcyNzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1605745341075-1b7460b99df8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRvY2tlcnxlbnwwfHx8fDE2NzI3NzcyNzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Deploy kubernetes statefulset with affinity"><p>Here is an example to demonstrate how to assign statefulset and pod to the target worker node.</p>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">affinity</a> helps define which node will be assigned for the pod by using the node labels.</p>
<pre><code class="language-shell">$ kubectl get nodes -o wide --show-labels

$ kubectl label nodes &lt;node-name&gt; portworx.io/fio=true

$ cat statefulset-node-select.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: fiobench
spec:
  serviceName: fiobench
  replicas: 1
  selector:
    matchLabels:
      app: fiobench
  template:
    metadata:
      labels:
        app: fiobench
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: portworx.io/fio
                operator: In
                values:
                - &quot;true&quot;
      containers:
      - name: fiobench
        image: pwxbuild/perfbench:fio
        imagePullPolicy: Always
        volumeMounts:
        - name: fiobench-data-1
          mountPath: /fiodata1
        - name: fiobench-data-2
          mountPath: /fiodata2
        securityContext:
          privileged: true
      imagePullSecrets:
      - name: regcred
  volumeClaimTemplates:
  - metadata:
      name: fiobench-data-1
    spec:
      storageClassName: fio-repl-node-select
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1400Gi
  - metadata:
      name: fiobench-data-2
    spec:
      storageClassName: fio-repl-node-select
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1400Gi

$ kubectl apply -f statefulset-node-select.yaml
$ kubectl get statefulset
$ kubectl get pod -o wide
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using kubeconfig to configure access to remote Kubernetes cluster]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-kubectl-binary">Install kubectl binary</h2>
<p>The Kubernetes command-line tool, <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.</p>
<pre><code class="language-shell">[root@localhost ~]# curl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.</code></pre>]]></description><link>https://www.flamingbytes.com/blog/using-kubeconfig-to-configure-access-to-remote-kubernetes-cluster/</link><guid isPermaLink="false">63b48cf81527a00d97449c75</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 26 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372459534-848ec00d4da7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-kubectl-binary">Install kubectl binary</h2>
<img src="https://images.unsplash.com/photo-1667372459534-848ec00d4da7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using kubeconfig to configure access to remote Kubernetes cluster"><p>The Kubernetes command-line tool, <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.</p>
<pre><code class="language-shell">[root@localhost ~]# curl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;
[root@localhost ~]# sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
[root@localhost ~]# ls -la /usr/local/bin/kubectl
[root@localhost ~]# kubectl version --output=yaml
clientVersion:
  buildDate: &quot;2022-09-21T14:33:49Z&quot;
  compiler: gc
  gitCommit: 5835544ca568b757a8ecae5c153f317e5736700e
  gitTreeState: clean
  gitVersion: v1.25.2
  goVersion: go1.19.1
  major: &quot;1&quot;
  minor: &quot;25&quot;
  platform: linux/amd64
kustomizeVersion: v4.5.7

The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
<h2 id="use-kubeconfig-file-to-access-remote-kubernetes-cluster">Use kubeconfig file to access remote Kubernetes cluster</h2>
<p>A <em>kubeconfig</em> is a YAML file with all the Kubernetes cluster details, certificate, and secret token to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using managed Kubernetes cluster. A <em>kubeconfig</em> file is a file to configure access to Kubernetes when to use with <em>kubectl</em> cli tool.</p>
<p>When to deploy Kubernetes cluster, a <em>kubeconfig</em> is automatically generated. It&apos;s saved in ~/.kube/config drectory.</p>
<pre><code class="language-shell">[root@host1 ~]#  ls -la ~/.kube/config
-rw------- 1 root root 5577 May  9 02:25 /root/.kube/config
</code></pre>
<p>You can access the Kubernetes cluster by providing the kubeconfig file remotely.</p>
<pre><code class="language-shell">[root@localhost ~]# scp host1:/root/.kube/config ./kubeconfig-remote
[root@localhost ~]# kubectl --kubeconfig=./kubeconfig-remote get nodes
NAME            STATUS   ROLES    AGE    VERSION
host0   Ready    &lt;none&gt;   135d   v1.19.2
host1   Ready    &lt;none&gt;   135d   v1.19.2
host2   Ready    &lt;none&gt;   135d   v1.19.2
host3   Ready    master   135d   v1.19.2
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Lambda architecture in big data system]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="what-is-lambda-architecture">What is Lambda Architecture</h2>
<p>When working with very large data sets, it can take a long time to run the sort of queries that clients need. These queries can&apos;t be performed in real time, and often require algorithms such as MapReduce that operate in parallel across the entire</p>]]></description><link>https://www.flamingbytes.com/blog/lambda-architecture-in-big-data-system/</link><guid isPermaLink="false">63b497f11527a00d97449dea</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 25 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1609788063095-d71bf3c1f01f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDYzfHxiaWclMjBkYXRhfGVufDB8fHx8MTY3Mjc3OTg0Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="what-is-lambda-architecture">What is Lambda Architecture</h2>
<img src="https://images.unsplash.com/photo-1609788063095-d71bf3c1f01f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDYzfHxiaWclMjBkYXRhfGVufDB8fHx8MTY3Mjc3OTg0Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Lambda architecture in big data system"><p>When working with very large data sets, it can take a long time to run the sort of queries that clients need. These queries can&apos;t be performed in real time, and often require algorithms such as MapReduce that operate in parallel across the entire data set. The results are then stored separately from the raw data and used for querying.</p>
<p>One drawback to this approach is that it introduces latency &#x2014; if processing takes a few hours, a query may return results that are several hours old. Ideally, you would like to get some results in real time (perhaps with some loss of accuracy), and combine these results with the results from the batch analytics.</p>
<p>The lambda architecture, first proposed by Nathan Marz, addresses this problem by creating two paths for data flow. All data coming into the system goes through these two paths:</p>
<ul>
<li>A batch layer (cold path) stores all of the incoming data in its raw form and performs batch processing on the data. The result of this processing is stored as a batch view.</li>
<li>A speed layer (hot path) analyzes data in real time. This layer is designed for low latency, at the expense of accuracy.</li>
</ul>
<p>The batch layer feeds into a serving layer that indexes the batch view for efficient querying. The speed layer updates the serving layer with incremental updates based on the most recent data.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/lambda.png" alt="Lambda architecture in big data system" loading="lazy">{:.shadow}</p>
<p><a href="https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/">Source</a></p>
<p>Data that flows into the hot path is constrained by latency requirements imposed by the speed layer, so that it can be processed as quickly as possible. Often, this requires a tradeoff of some level of accuracy in favor of data that is ready as quickly as possible. For example, consider an IoT scenario where a large number of temperature sensors are sending telemetry data. The speed layer may be used to process a sliding time window of the incoming data.</p>
<p>Data flowing into the cold path, on the other hand, is not subject to the same low latency requirements. This allows for high accuracy computation across large data sets, which can be very time intensive.</p>
<p>Eventually, the hot and cold paths converge at the analytics client application. If the client needs to display timely, yet potentially less accurate data in real time, it will acquire its result from the hot path. Otherwise, it will select results from the cold path to display less timely but more accurate data. In other words, the hot path has data for a relatively small window of time, after which the results can be updated with more accurate data from the cold path.</p>
<p>The raw data stored at the batch layer is immutable. Incoming data is always appended to the existing data, and the previous data is never overwritten. Any changes to the value of a particular datum are stored as a new timestamped event record. This allows for recomputation at any point in time across the history of the data collected. The ability to recompute the batch view from the original raw data is important, because it allows for new views to be created as the system evolves.</p>
<h2 id="batch-layer">Batch Layer</h2>
<p>New data comes continuously, as a feed to the data system. It gets fed to the batch layer and the speed layer simultaneously. It looks at all the data at once and eventually corrects the data in the stream layer.  Here we can find lots of ETL and a traditional data warehouse. This layer is built using a predefined schedule, usually once or twice a day. The batch layer has two very important functions:</p>
<ul>
<li>To manage the master dataset</li>
<li>To pre-compute the batch views.</li>
</ul>
<h2 id="serving-layer">Serving Layer</h2>
<p>The outputs from the batch layer in the form of batch views and those coming from the speed layer in the form of near real-time views get forwarded to the serving.  This layer indexes the batch views so that they can be queried in low-latency on an ad-hoc basis.</p>
<h2 id="speed-layer-stream-layer">Speed Layer (Stream Layer)</h2>
<p>This layer handles the data that are not already delivered in the batch view due to the latency of the batch layer. In addition, it only deals with recent data in order to provide a complete view of the data to the user by creating real-time views.</p>
<h2 id="benefits-of-lambda-architectures">Benefits of lambda architectures</h2>
<p>Here are the main benefits of lambda architectures:</p>
<ul>
<li>No Server Management &#x2013; you do not have to install, maintain, or administer any software.</li>
<li>Flexible Scaling &#x2013; your application can be either automatically scaled or scaled by the adjustment of its capacity</li>
<li>Automated High Availability &#x2013; refers to the fact that serverless applications have already built-in availability and faults tolerance. It represents a guarantee that all requests will get a response about whether they were successful or not.</li>
<li>Business Agility &#x2013; React in real-time to changing business/market scenarios</li>
</ul>
<h2 id="challenges-with-lambda-architectures">Challenges with lambda architectures</h2>
<ul>
<li>Complexity &#x2013; lambda architectures can be highly complex. Administrators must typically maintain two separate code bases for batch and streaming layers, which can make debugging difficult.</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li>&lt;Big Data - Priciples and best practices of scalable real-time data systems&gt;</li>
<li><a href="https://databricks.com/glossary/lambda-architecture">https://databricks.com/glossary/lambda-architecture</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/">https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>