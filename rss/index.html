<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Flamingbytes]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>https://relentlesstorm.github.io/</link><image><url>https://relentlesstorm.github.io/favicon.png</url><title>Flamingbytes</title><link>https://relentlesstorm.github.io/</link></image><generator>Ghost 5.26</generator><lastBuildDate>Tue, 03 Jan 2023 00:20:40 GMT</lastBuildDate><atom:link href="https://relentlesstorm.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Hosting the Ghost blog on GitHub]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h1 id="install-ghost-locally">Install Ghost locally</h1>
<p><a href="https://pages.github.com/">GitHub Pages</a> is a great solution for hosting static website. If you use the Ghost to manage your website, you can <a href="https://ghost.org/docs/install/local/">install Ghost locally</a> and convert it to a static website in order to host it on Github.</p>
<h2 id="install-ubuntu-virtual-machine-on-windowsoptional">Install Ubuntu virtual machine on Windows(optional)</h2>
<h2 id="install-ghost-cli">Install Ghost-CLI</h2>]]></description><link>https://relentlesstorm.github.io/ghost-github/</link><guid isPermaLink="false">63afb59696dbf412f1f80301</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 27 Dec 2022 04:07:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1543616991-75a2c125ff5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1MXx8YmxvZ3xlbnwwfHx8fDE2NzI0NjIyOTE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h1 id="install-ghost-locally">Install Ghost locally</h1>
<img src="https://images.unsplash.com/photo-1543616991-75a2c125ff5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1MXx8YmxvZ3xlbnwwfHx8fDE2NzI0NjIyOTE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Hosting the Ghost blog on GitHub"><p><a href="https://pages.github.com/">GitHub Pages</a> is a great solution for hosting static website. If you use the Ghost to manage your website, you can <a href="https://ghost.org/docs/install/local/">install Ghost locally</a> and convert it to a static website in order to host it on Github.</p>
<h2 id="install-ubuntu-virtual-machine-on-windowsoptional">Install Ubuntu virtual machine on Windows(optional)</h2>
<h2 id="install-ghost-cli">Install Ghost-CLI</h2>
<p>Ghost-CLI is a commandline tool to help you get Ghost installed and configured for use, quickly and easily.</p>
<pre><code>$ npm install ghost-cli@latest -g
</code></pre>
<p>Ghost runs as background process and remains running until you stop or restart it. The following are some useful commands:</p>
<pre><code>ghost help
ghost ls
ghost log
ghost stop
ghost start
</code></pre>
<h2 id="install-ghost">Install Ghost</h2>
<pre><code>$ mkdir my-ghost-website
$ cd my-ghost-website
$ ghost install local
</code></pre>
<p>Once the Ghost is installed you can access the website on <a href="https://relentlesstorm.github.io/">https://relentlesstorm.github.io</a> and <a href="https://relentlesstorm.github.io/ghost">https://relentlesstorm.github.io/ghost</a> for Ghost Admin.</p>
<h1 id="generate-the-static-website">Generate the static website</h1>
<p>In order to generate the static website, we&apos;ll use the Ghost static site generator. For the Linux and macOS, this tool can be used directly.</p>
<pre><code>$ sudo npm install -g ghost-static-site-generator
</code></pre>
<p>Now, we can push the generated static pages to the Github repository named <strong>username.github.io</strong>.</p>
<p>To generate the static pages, we can run the following command. The static pages are generated in a folder called <em>static</em>.</p>
<pre><code>$ gssg --url https://username.github.io
</code></pre>
<h1 id="push-the-static-pages-to-github">Push the static pages to Github</h1>
<p>Before pushing the static pages to Github, a repository called <em>username.github.io</em> should be created on Github website.</p>
<p>The static pages can be pushed to Github repository as below.</p>
<pre><code>$ cd static
$ git init .
$ git remote add origin https://github.com/username/username.github.io.git
$ git branch -M main
$ git add .
$ git commit -m &apos;Init my website&apos;
$ git push -u origin main
</code></pre>
<p>After the push, Github will build and deploy the pages automatically. And the updated website will be available to access in a few minutes.</p>
<h1 id="reference">Reference</h1>
<ul>
<li><a href="https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl">https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl</a></li>
<li><a href="https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text=Install%20and%20run%20Ghost%20locally,static%20website%20to%20GitHub%20Pages">https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text=Install and run Ghost locally,static website to GitHub Pages</a></li>
<li><a href="https://ghost.org/docs/reinstall/">https://ghost.org/docs/reinstall/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Use sysbench for CockroachDB performance benchmarking]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro">Intro</h2>
<p>Cockroach uses TPC-C as the official OLTP workload benchmark since it&apos;s a more realistic measurement by modeling the real world applications.</p>
<p>However, <a href="https://github.com/akopytov/sysbench">sysbench</a> is a straight-forward throughput/latency benchmarking tool. It is a scriptable multi-threaded benchmark tool based on LuaJIT. It is most frequently used for database</p>]]></description><link>https://relentlesstorm.github.io/crdb-sysbench/</link><guid isPermaLink="false">63afb5b296dbf412f1f8030c</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Thu, 08 Dec 2022 04:08:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1597852074816-d933c7d2b988?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro">Intro</h2>
<img src="https://images.unsplash.com/photo-1597852074816-d933c7d2b988?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Use sysbench for CockroachDB performance benchmarking"><p>Cockroach uses TPC-C as the official OLTP workload benchmark since it&apos;s a more realistic measurement by modeling the real world applications.</p>
<p>However, <a href="https://github.com/akopytov/sysbench">sysbench</a> is a straight-forward throughput/latency benchmarking tool. It is a scriptable multi-threaded benchmark tool based on LuaJIT. It is most frequently used for database benchmarks, but can also be used to create arbitrarily complex workloads that do not involve a database server.</p>
<ul>
<li>oltp_*.lua: a collection of OLTP-like database benchmarks</li>
<li>fileio: a filesystem-level benchmark</li>
<li>cpu: a simple CPU benchmark</li>
<li>memory: a memory access benchmark</li>
<li>threads: a thread-based scheduler benchmark</li>
<li>mutex: a POSIX mutex benchmark</li>
</ul>
<p>This Cockroach <a href="https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/">blog</a> explains why we use sysbench for competitive benchmarks.</p>
<p>Sysbench contains a collection of simple SQL workloads. These workloads perform low-level SQL operations. For example, they run concurrent INSERT or UPDATE statements on rows as fast as possible. It gives us a picture on the system performance under different access patterns. Unlike TPC-C, Sysbench does not attempt to model a real application.</p>
<p>Sysbench includes the following workloads:</p>
<ul>
<li>oltp_point_select: single-row point selects</li>
<li>oltp_insert: single-row inserts</li>
<li>oltp_delete: single-row deletes</li>
<li>oltp_update_index: single-row update on column that requires update to secondary index</li>
<li>oltp_update_non_index single-row: update on column that does not require update to secondary index</li>
<li>oltp_read_only: transactions that run collection of small scans</li>
<li>oltp_read_write: transactions that run collection of small scans and writes</li>
<li>oltp_write_only: transactions that run collection of writes</li>
</ul>
<p>Sysbench supports the following two database drivers.</p>
<ul>
<li>mysql - MySQL driver</li>
<li>pgsql - PostgreSQL driver</li>
</ul>
<p>From this Cockroach <a href="https://www.cockroachlabs.com/blog/why-postgres/">blog</a>, CockroachDB is compatible with PostgreSQL.</p>
<p>In this article, we will explore how to run sysbench with CockroachDB using pqsql driver.</p>
<h2 id="install-the-cockroachdb-cluster">Install the CockroachDB cluster</h2>
<p>Refer to this [post]({% post_url 2022-09-01-cockroachdb-tpcc %}) on how to deploy CockroachDB cluster.</p>
<pre><code class="language-shell">[root@crdb_node1 ~]# cockroach version
Build Tag:        v22.1.6
Build Time:       2022/08/23 17:05:04
Distribution:     CCL
Platform:         linux amd64 (x86_64-pc-linux-gnu)
Go Version:       go1.17.11
C Compiler:       gcc 6.5.0
Build Commit ID:  760a8253ae6478d69da0330133e3efec8e950e4e
Build Type:       release
</code></pre>
<h2 id="interact-with-the-cockroachdb">Interact with the CockroachDB</h2>
<h3 id="use-the-cockroachdb-built-in-client">Use the CockroachDB built-in client</h3>
<p>CockroachDB comes with a built-in client for executing SQL statements from an interactive shell or directly from the command line.</p>
<p>To use this client, run the cockroach sql command as following:</p>
<pre><code class="language-shell">[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;create database testdb&apos;

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;show databases&apos;
  database_name | owner | primary_region | regions | survival_goal
----------------+-------+----------------+---------+----------------
  defaultdb     | root  | NULL           | {}      | NULL
  postgres      | root  | NULL           | {}      | NULL
  system        | node  | NULL           | {}      | NULL
  testdb        | root  | NULL           | {}      | NULL
(4 rows)

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;show tables from testdb&apos;
SHOW TABLES 0

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;drop database testdb&apos;
</code></pre>
<h3 id="use-the-postgres-client">Use the Postgres client</h3>
<p>The cockroachDB can also be interacted by using Postgres client.</p>
<p>To install the Postgres client:</p>
<pre><code class="language-shell">[root@node0 ~]# yum -y install postgresql postgresql-libs
Installed:
  postgresql.x86_64 0:9.2.24-8.el7_9
</code></pre>
<p>To create the database and user:</p>
<pre><code class="language-shell">[root@node0 ~]# psql -h node1 -U root -p 26257
psql (9.2.24, server 13.0.0)

root=&gt; create database testdb;
root=&gt; create user tester;
root=&gt; grant all on database testdb to tester;

root=&gt; show databases;
 database_name | owner | primary_region | regions | survival_goal
---------------+-------+----------------+---------+---------------
 defaultdb     | root  |                | {}      |
 postgres      | root  |                | {}      |
 system        | node  |                | {}      |
 testdb        | root  |                | {}      |
(4 rows)

root=&gt; show users;
 username | options | member_of
----------+---------+-----------
 admin    |         | {}
 root     |         | {admin}
 tester   |         | {}
(3 rows)

root=&gt; \c testdb;
psql (9.2.24, server 13.0.0)
WARNING: psql version 9.2, server version 13.0.
         Some psql features might not work.
You are now connected to database &quot;testdb&quot; as user &quot;root&quot;.

testdb=&gt; \dt;
                          List of relations
    Schema    |       Name        | Type  |          Owner
--------------+-------------------+-------+--------------------------
 pg_extension | geography_columns | table | unknown (OID=3233629770)
 pg_extension | geometry_columns  | table | unknown (OID=3233629770)
 pg_extension | spatial_ref_sys   | table | unknown (OID=3233629770)
(3 rows)

testdb=&gt;  SELECT * FROM pg_catalog.pg_tables where schemaname != &apos;pg_catalog&apos; AND schemaname != &apos;information_schema&apos; and schemaname != &apos;crdb_internal&apos;;
  schemaname  |     tablename     | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity
--------------+-------------------+------------+------------+------------+----------+-------------+-------------
 pg_extension | geography_columns | node       |            | f          | f        | f           | f
 pg_extension | geometry_columns  | node       |            | f          | f        | f           | f
 pg_extension | spatial_ref_sys   | node       |            | f          | f        | f           | f
(3 rows)
</code></pre>
<h2 id="install-sysbench">Install sysbench</h2>
<p>To install sysbench:</p>
<pre><code class="language-shell">[root@node0 ~]# curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash^C
[root@node0 ~]# sudo yum -y install sysbench
[root@node0 ~]# sysbench --version
sysbench 1.0.20
</code></pre>
<p>To get familiar with the sysbench parameters:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench --help
Usage:
  sysbench [options]... [testname] [command]

Commands implemented by most tests: prepare run cleanup help

General options:
  --threads=N                     number of threads to use [1]
  --events=N                      limit for total number of events [0]
  --time=N                        limit for total execution time in seconds [10]
  --forced-shutdown=STRING        number of seconds to wait after the --time limit before forcing shutdown, or &apos;off&apos; to disable [off]
  --thread-stack-size=SIZE        size of stack per thread [64K]
  --rate=N                        average transactions rate. 0 for unlimited rate [0]
  --report-interval=N             periodically report intermediate statistics with a specified interval in seconds. 0 disables intermediate reports [0]
  --report-checkpoints=[LIST,...] dump full statistics and reset all counters at specified points in time. The argument is a list of comma-separated values representing the amount of time in seconds elapsed from start of test when report checkpoint(s) must be performed. Report checkpoints are off by default. []
  --debug[=on|off]                print more debugging info [off]
  --validate[=on|off]             perform validation checks where possible [off]
  --help[=on|off]                 print help and exit [off]
  --version[=on|off]              print version and exit [off]
  --config-file=FILENAME          File containing command line options
  --tx-rate=N                     deprecated alias for --rate [0]
  --max-requests=N                deprecated alias for --events [0]
  --max-time=N                    deprecated alias for --time [0]
  --num-threads=N                 deprecated alias for --threads [1]

Pseudo-Random Numbers Generator options:
  --rand-type=STRING random numbers distribution {uniform,gaussian,special,pareto} [special]
  --rand-spec-iter=N number of iterations used for numbers generation [12]
  --rand-spec-pct=N  percentage of values to be treated as &apos;special&apos; (for special distribution) [1]
  --rand-spec-res=N  percentage of &apos;special&apos; values to use (for special distribution) [75]
  --rand-seed=N      seed for random number generator. When 0, the current time is used as a RNG seed. [0]
  --rand-pareto-h=N  parameter h for pareto distribution [0.2]

Log options:
  --verbosity=N verbosity level {5 - debug, 0 - only critical messages} [3]

  --percentile=N       percentile to calculate in latency statistics (1-100). Use the special value of 0 to disable percentile calculations [95]
  --histogram[=on|off] print latency histogram in report [off]

General database options:

  --db-driver=STRING  specifies database driver to use (&apos;help&apos; to get list of available drivers) [mysql]
  --db-ps-mode=STRING prepared statements usage mode {auto, disable} [auto]
  --db-debug[=on|off] print database-specific debug information [off]


Compiled-in database drivers:
  mysql - MySQL driver
  pgsql - PostgreSQL driver

mysql options:
  --mysql-host=[LIST,...]          MySQL server host [localhost]
  --mysql-port=[LIST,...]          MySQL server port [3306]
  --mysql-socket=[LIST,...]        MySQL socket
  --mysql-user=STRING              MySQL user [sbtest]
  --mysql-password=STRING          MySQL password []
  --mysql-db=STRING                MySQL database name [sbtest]
  --mysql-ssl[=on|off]             use SSL connections, if available in the client library [off]
  --mysql-ssl-cipher=STRING        use specific cipher for SSL connections []
  --mysql-compression[=on|off]     use compression, if available in the client library [off]
  --mysql-debug[=on|off]           trace all client library calls [off]
  --mysql-ignore-errors=[LIST,...] list of errors to ignore, or &quot;all&quot; [1213,1020,1205]
  --mysql-dry-run[=on|off]         Dry run, pretend that all MySQL client API calls are successful without executing them [off]

pgsql options:
  --pgsql-host=STRING     PostgreSQL server host [localhost]
  --pgsql-port=N          PostgreSQL server port [5432]
  --pgsql-user=STRING     PostgreSQL user [sbtest]
  --pgsql-password=STRING PostgreSQL password []
  --pgsql-db=STRING       PostgreSQL database name [sbtest]

Compiled-in tests:
  fileio - File I/O test
  cpu - CPU performance test
  memory - Memory functions speed test
  threads - Threads subsystem performance test
  mutex - Mutex performance test

See &apos;sysbench &lt;testname&gt; help&apos; for a list of options for each test.
</code></pre>
<p>Sysbench includes the following lua scripts to simulate OLTP workloads.</p>
<pre><code class="language-shell">[root@node0 ~]# ls -l /usr/share/sysbench/
total 60
-rwxr-xr-x   1 root root  1452 Apr 24  2020 bulk_insert.lua
-rw-r--r--   1 root root 14369 Apr 24  2020 oltp_common.lua
-rwxr-xr-x   1 root root  1290 Apr 24  2020 oltp_delete.lua
-rwxr-xr-x   1 root root  2415 Apr 24  2020 oltp_insert.lua
-rwxr-xr-x   1 root root  1265 Apr 24  2020 oltp_point_select.lua
-rwxr-xr-x   1 root root  1649 Apr 24  2020 oltp_read_only.lua
-rwxr-xr-x   1 root root  1824 Apr 24  2020 oltp_read_write.lua
-rwxr-xr-x   1 root root  1118 Apr 24  2020 oltp_update_index.lua
-rwxr-xr-x   1 root root  1127 Apr 24  2020 oltp_update_non_index.lua
-rwxr-xr-x   1 root root  1440 Apr 24  2020 oltp_write_only.lua
-rwxr-xr-x   1 root root  1919 Apr 24  2020 select_random_points.lua
-rwxr-xr-x   1 root root  2118 Apr 24  2020 select_random_ranges.lua
drwxr-xr-x   4 root root    49 Dec  8 20:39 tests

[root@node0 ~]# ls -l /usr/share/sysbench/tests/include/oltp_legacy/
total 52
-rw-r--r-- 1 root root 1195 Apr 24  2020 bulk_insert.lua
-rw-r--r-- 1 root root 4696 Apr 24  2020 common.lua
-rw-r--r-- 1 root root  366 Apr 24  2020 delete.lua
-rw-r--r-- 1 root root 1171 Apr 24  2020 insert.lua
-rw-r--r-- 1 root root 3004 Apr 24  2020 oltp.lua
-rw-r--r-- 1 root root  368 Apr 24  2020 oltp_simple.lua
-rw-r--r-- 1 root root  527 Apr 24  2020 parallel_prepare.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 select.lua
-rw-r--r-- 1 root root 1448 Apr 24  2020 select_random_points.lua
-rw-r--r-- 1 root root 1556 Apr 24  2020 select_random_ranges.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 update_index.lua
-rw-r--r-- 1 root root  578 Apr 24  2020 update_non_index.lua
</code></pre>
<p>To get more options for each specific lua workload:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_insert.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_insert.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_write_only.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_read_only.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_read_only.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_point_select.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_point_select.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]
</code></pre>
<h2 id="prepare-for-the-benchmark">Prepare for the benchmark</h2>
<p>Before running the benchmark, the database should be created and the data tables should be populated.</p>
<p>To create the database:</p>
<pre><code class="language-shell">[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;create database testdb;&apos;
</code></pre>
<p>To populate the database:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --db-driver=pgsql prepare
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Creating table &apos;sbtest1&apos;...
Inserting 100000 records into &apos;sbtest1&apos;
Creating a secondary index on &apos;sbtest1&apos;...
Creating table &apos;sbtest2&apos;...
Inserting 100000 records into &apos;sbtest2&apos;
Creating a secondary index on &apos;sbtest2&apos;...
Creating table &apos;sbtest3&apos;...
Inserting 100000 records into &apos;sbtest3&apos;
Creating a secondary index on &apos;sbtest3&apos;...
Creating table &apos;sbtest4&apos;...
Inserting 100000 records into &apos;sbtest4&apos;
Creating a secondary index on &apos;sbtest4&apos;...
Creating table &apos;sbtest5&apos;...
Inserting 100000 records into &apos;sbtest5&apos;
Creating a secondary index on &apos;sbtest5&apos;...
Creating table &apos;sbtest6&apos;...
Inserting 100000 records into &apos;sbtest6&apos;
Creating a secondary index on &apos;sbtest6&apos;...
Creating table &apos;sbtest7&apos;...
Inserting 100000 records into &apos;sbtest7&apos;
Creating a secondary index on &apos;sbtest7&apos;...
Creating table &apos;sbtest8&apos;...
Inserting 100000 records into &apos;sbtest8&apos;
Creating a secondary index on &apos;sbtest8&apos;...
Creating table &apos;sbtest9&apos;...
Inserting 100000 records into &apos;sbtest9&apos;
Creating a secondary index on &apos;sbtest9&apos;...
Creating table &apos;sbtest10&apos;...
Inserting 100000 records into &apos;sbtest10&apos;
Creating a secondary index on &apos;sbtest10&apos;...
Creating table &apos;sbtest11&apos;...
Inserting 100000 records into &apos;sbtest11&apos;
Creating a secondary index on &apos;sbtest11&apos;...
Creating table &apos;sbtest12&apos;...
Inserting 100000 records into &apos;sbtest12&apos;
Creating a secondary index on &apos;sbtest12&apos;...
Creating table &apos;sbtest13&apos;...
Inserting 100000 records into &apos;sbtest13&apos;
Creating a secondary index on &apos;sbtest13&apos;...
Creating table &apos;sbtest14&apos;...
Inserting 100000 records into &apos;sbtest14&apos;
Creating a secondary index on &apos;sbtest14&apos;...
Creating table &apos;sbtest15&apos;...
Inserting 100000 records into &apos;sbtest15&apos;
Creating a secondary index on &apos;sbtest15&apos;...
Creating table &apos;sbtest16&apos;...
Inserting 100000 records into &apos;sbtest16&apos;
Creating a secondary index on &apos;sbtest16&apos;...
Creating table &apos;sbtest17&apos;...
Inserting 100000 records into &apos;sbtest17&apos;
Creating a secondary index on &apos;sbtest17&apos;...
Creating table &apos;sbtest18&apos;...
Inserting 100000 records into &apos;sbtest18&apos;
Creating a secondary index on &apos;sbtest18&apos;...
Creating table &apos;sbtest19&apos;...
Inserting 100000 records into &apos;sbtest19&apos;
Creating a secondary index on &apos;sbtest19&apos;...
Creating table &apos;sbtest20&apos;...
Inserting 100000 records into &apos;sbtest20&apos;
Creating a secondary index on &apos;sbtest20&apos;...
Creating table &apos;sbtest21&apos;...
Inserting 100000 records into &apos;sbtest21&apos;
Creating a secondary index on &apos;sbtest21&apos;...
Creating table &apos;sbtest22&apos;...
Inserting 100000 records into &apos;sbtest22&apos;
Creating a secondary index on &apos;sbtest22&apos;...
Creating table &apos;sbtest23&apos;...
Inserting 100000 records into &apos;sbtest23&apos;
Creating a secondary index on &apos;sbtest23&apos;...
Creating table &apos;sbtest24&apos;...
Inserting 100000 records into &apos;sbtest24&apos;

testdb=&gt;  show tables from testdb;
 schema_name | table_name | type  | owner  | estimated_row_count | locality
-------------+------------+-------+--------+---------------------+----------
 public      | sbtest1    | table | tester |              100000 |
 public      | sbtest10   | table | tester |              100000 |
 public      | sbtest11   | table | tester |              100000 |
 public      | sbtest12   | table | tester |              100000 |
 public      | sbtest13   | table | tester |              100000 |
 public      | sbtest14   | table | tester |              100000 |
 public      | sbtest15   | table | tester |              100000 |
 public      | sbtest16   | table | tester |              100000 |
 public      | sbtest17   | table | tester |              100000 |
 public      | sbtest18   | table | tester |              100000 |
 public      | sbtest19   | table | tester |              100000 |
 public      | sbtest2    | table | tester |              100000 |
 public      | sbtest20   | table | tester |              100000 |
 public      | sbtest21   | table | tester |              100000 |
 public      | sbtest22   | table | tester |              100000 |
 public      | sbtest23   | table | tester |              100000 |
 public      | sbtest24   | table | tester |              100000 |
 public      | sbtest3    | table | tester |              100000 |
 public      | sbtest4    | table | tester |              100000 |
 public      | sbtest5    | table | tester |              100000 |
 public      | sbtest6    | table | tester |              100000 |
 public      | sbtest7    | table | tester |              100000 |
 public      | sbtest8    | table | tester |              100000 |
 public      | sbtest9    | table | tester |              100000 |
(24 rows)

testdb=&gt; select sum(range_size)/1000 from crdb_internal.ranges where database_name=&apos;testdb&apos;;
       ?column?
-----------------------
 602418.53400000000000
(1 row)
</code></pre>
<p>To populate the database with 2 or more threads:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=root --pgsql-password= --table_size=1000000 --tables=24 --threads=2 --db-driver=pgsql prepare
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Initializing worker threads...

Creating table &apos;sbtest2&apos;...
Creating table &apos;sbtest1&apos;...
Inserting 1000000 records into &apos;sbtest1&apos;
Inserting 1000000 records into &apos;sbtest2&apos;
Creating a secondary index on &apos;sbtest2&apos;...
Creating table &apos;sbtest4&apos;...
Inserting 1000000 records into &apos;sbtest4&apos;
Creating a secondary index on &apos;sbtest1&apos;...
Creating table &apos;sbtest3&apos;...
Inserting 1000000 records into &apos;sbtest3&apos;
Creating a secondary index on &apos;sbtest4&apos;...
Creating a secondary index on &apos;sbtest3&apos;...
Creating table &apos;sbtest6&apos;...
Inserting 1000000 records into &apos;sbtest6&apos;
Creating table &apos;sbtest5&apos;...
Inserting 1000000 records into &apos;sbtest5&apos;
Creating a secondary index on &apos;sbtest5&apos;...
Creating a secondary index on &apos;sbtest6&apos;...
Creating table &apos;sbtest7&apos;...
Inserting 1000000 records into &apos;sbtest7&apos;
Creating table &apos;sbtest8&apos;...
Inserting 1000000 records into &apos;sbtest8&apos;
Creating a secondary index on &apos;sbtest7&apos;...
Creating a secondary index on &apos;sbtest8&apos;...
Creating table &apos;sbtest9&apos;...
Inserting 1000000 records into &apos;sbtest9&apos;
Creating table &apos;sbtest10&apos;...
Inserting 1000000 records into &apos;sbtest10&apos;
Creating a secondary index on &apos;sbtest10&apos;...
Creating table &apos;sbtest12&apos;...
Inserting 1000000 records into &apos;sbtest12&apos;
Creating a secondary index on &apos;sbtest9&apos;...
Creating table &apos;sbtest11&apos;...
Inserting 1000000 records into &apos;sbtest11&apos;
Creating a secondary index on &apos;sbtest12&apos;...
Creating table &apos;sbtest14&apos;...
Inserting 1000000 records into &apos;sbtest14&apos;
Creating a secondary index on &apos;sbtest11&apos;...
Creating table &apos;sbtest13&apos;...
Inserting 1000000 records into &apos;sbtest13&apos;
Creating a secondary index on &apos;sbtest14&apos;...
Creating table &apos;sbtest16&apos;...
Inserting 1000000 records into &apos;sbtest16&apos;
Creating a secondary index on &apos;sbtest13&apos;...
Creating table &apos;sbtest15&apos;...
Inserting 1000000 records into &apos;sbtest15&apos;
Creating a secondary index on &apos;sbtest16&apos;...
Creating table &apos;sbtest18&apos;...
Inserting 1000000 records into &apos;sbtest18&apos;
Creating a secondary index on &apos;sbtest15&apos;...
Creating table &apos;sbtest17&apos;...
Inserting 1000000 records into &apos;sbtest17&apos;
Creating a secondary index on &apos;sbtest18&apos;...
Creating table &apos;sbtest20&apos;...
Inserting 1000000 records into &apos;sbtest20&apos;
Creating a secondary index on &apos;sbtest17&apos;...
Creating table &apos;sbtest19&apos;...
Inserting 1000000 records into &apos;sbtest19&apos;
Creating a secondary index on &apos;sbtest20&apos;...
Creating table &apos;sbtest22&apos;...
Inserting 1000000 records into &apos;sbtest22&apos;
Creating a secondary index on &apos;sbtest19&apos;...
Creating table &apos;sbtest21&apos;...
Inserting 1000000 records into &apos;sbtest21&apos;
Creating a secondary index on &apos;sbtest22&apos;...
Creating table &apos;sbtest24&apos;...
Inserting 1000000 records into &apos;sbtest24&apos;
Creating a secondary index on &apos;sbtest21&apos;...
Creating table &apos;sbtest23&apos;...
Inserting 1000000 records into &apos;sbtest23&apos;
Creating a secondary index on &apos;sbtest24&apos;...
Creating a secondary index on &apos;sbtest23&apos;...
</code></pre>
<h2 id="run-sysbench">Run sysbench</h2>
<p>Now we can run sysbench on the populated database. We can increase the threads to stress the database with more workloads in order to measure the system performance limit.</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --time=60 --report-interval=10 --db-driver=pgsql run
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Report intermediate results every 10 second(s)
Initializing random number generator from current time

Initializing worker threads...

Threads started!

[ 10s ] thds: 1 tps: 80.86 qps: 485.66 (r/w/o: 0.00/84.66/401.00) lat (ms,95%): 20.37 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 1 tps: 82.60 qps: 495.41 (r/w/o: 0.00/92.00/403.41) lat (ms,95%): 20.74 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 1 tps: 81.00 qps: 485.79 (r/w/o: 0.00/94.10/391.69) lat (ms,95%): 20.74 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 1 tps: 81.00 qps: 486.21 (r/w/o: 0.00/102.90/383.31) lat (ms,95%): 20.37 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 1 tps: 79.20 qps: 474.90 (r/w/o: 0.00/103.10/371.80) lat (ms,95%): 21.50 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 1 tps: 78.20 qps: 469.39 (r/w/o: 0.00/107.30/362.09) lat (ms,95%): 21.11 err/s: 0.00 reconn/s: 0.00
SQL statistics:
    queries performed:
        read:                            0
        write:                           5842
        other:                           23138
        total:                           28980
    transactions:                        4830   (80.48 per sec.)
    queries:                             28980  (482.90 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          60.0081s
    total number of events:              4830

Latency (ms):
         min:                                    6.87
         avg:                                   12.42
         max:                                   32.90
         95th percentile:                       21.11
         sum:                                59978.07

Threads fairness:
    events (avg/stddev):           4830.0000/0.00
    execution time (avg/stddev):   59.9781/0.00
</code></pre>
<h2 id="cleanup-the-database">Cleanup the database</h2>
<p>Once the sysbench is done, we can cleanup the test data as below.</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --db-driver=pgsql cleanup
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Dropping table &apos;sbtest1&apos;...
Dropping table &apos;sbtest2&apos;...
Dropping table &apos;sbtest3&apos;...
Dropping table &apos;sbtest4&apos;...
Dropping table &apos;sbtest5&apos;...
Dropping table &apos;sbtest6&apos;...
Dropping table &apos;sbtest7&apos;...
Dropping table &apos;sbtest8&apos;...
Dropping table &apos;sbtest9&apos;...
Dropping table &apos;sbtest10&apos;...
Dropping table &apos;sbtest11&apos;...
Dropping table &apos;sbtest12&apos;...
Dropping table &apos;sbtest13&apos;...
Dropping table &apos;sbtest14&apos;...
Dropping table &apos;sbtest15&apos;...
Dropping table &apos;sbtest16&apos;...
Dropping table &apos;sbtest17&apos;...
Dropping table &apos;sbtest18&apos;...
Dropping table &apos;sbtest19&apos;...
Dropping table &apos;sbtest20&apos;...
Dropping table &apos;sbtest21&apos;...
Dropping table &apos;sbtest22&apos;...
Dropping table &apos;sbtest23&apos;...
Dropping table &apos;sbtest24&apos;...

testdb=&gt; show tables from testdb;
 schema_name | table_name | type | owner | estimated_row_count | locality
-------------+------------+------+-------+---------------------+----------
(0 rows)

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &quot;drop database testdb&quot;
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/akopytov/sysbench">https://github.com/akopytov/sysbench</a></li>
<li><a href="https://www.cockroachlabs.com/blog/why-postgres/">https://www.cockroachlabs.com/blog/why-postgres/</a></li>
<li><a href="https://www.cockroachlabs.com/compare/cockroachdb-vs-postgresql/">https://www.cockroachlabs.com/compare/cockroachdb-vs-postgresql/</a></li>
<li><a href="https://www.cockroachlabs.com/blog/postgresql-vs-cockroachdb/">https://www.cockroachlabs.com/blog/postgresql-vs-cockroachdb/</a></li>
<li><a href="https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/">https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/</a></li>
<li><a href="https://blog.purestorage.com/purely-informational/how-to-benchmark-mysql-performance/">https://blog.purestorage.com/purely-informational/how-to-benchmark-mysql-performance/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using cgroups to limit block device bandwidth]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>The block I/O controller specifies upper IO rate limits on devices.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/blkio&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)

$ lscgroup  | grep blkio
blkio:/
</code></pre>
<p>In</p>]]></description><link>https://relentlesstorm.github.io/cgroups-limit-io/</link><guid isPermaLink="false">63afc44996dbf412f1f80353</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 05 Dec 2022 05:11:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1646226343350-1ee5021e342a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGhhcmRkaXNrfGVufDB8fHx8MTY3MjQ2MzUxOQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1646226343350-1ee5021e342a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGhhcmRkaXNrfGVufDB8fHx8MTY3MjQ2MzUxOQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using cgroups to limit block device bandwidth"><p>The block I/O controller specifies upper IO rate limits on devices.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/blkio&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)

$ lscgroup  | grep blkio
blkio:/
</code></pre>
<p>In this post, we will learn how to use the following control files to limit block device bandwidth for the user tasks.</p>
<ul>
<li>blkio.throttle.read_bps_device - Specifies upper limit on READ rate from the device. IO rate is specified in bytes per second. Rules are per device.</li>
<li>blkio.throttle.read_iops_device - Specifies upper limit on READ rate from the device. IO rate is specified in IO per second. Rules are per device.</li>
<li>blkio.throttle.write_bps_device - Specifies upper limit on WRITE rate to the device. IO rate is specified in bytes per second. Rules are per device.</li>
<li>blkio.throttle.write_iops_device - Specifies upper limit on WRITE rate to the device. IO rate is specified in io per second. Rules are per device.</li>
</ul>
<h2 id="create-blkio-control-group">Create blkio control group</h2>
<p>Install libcgroup package to manage cgroups:</p>
<pre><code class="language-shell">$ yum install libcgroup libcgroup-tools
</code></pre>
<p>Create blkio control group:</p>
<pre><code class="language-shell">$ cgcreate -g blkio:/blkiolimited

$ lscgroup | grep blkio
blkio:/
blkio:/blkiolimited

$ ls /sys/fs/cgroup/blkio/blkiolimited/
blkio.bfq.io_service_bytes            blkio.bfq.weight                 blkio.throttle.io_service_bytes_recursive  blkio.throttle.read_iops_device   cgroup.procs
blkio.bfq.io_service_bytes_recursive  blkio.bfq.weight_device          blkio.throttle.io_serviced                 blkio.throttle.write_bps_device   notify_on_release
blkio.bfq.io_serviced                 blkio.reset_stats                blkio.throttle.io_serviced_recursive       blkio.throttle.write_iops_device  tasks
blkio.bfq.io_serviced_recursive       blkio.throttle.io_service_bytes  blkio.throttle.read_bps_device             cgroup.clone_children
</code></pre>
<h2 id="limit-the-block-device-bandwidth">Limit the block device bandwidth</h2>
<h3 id="limit-block-device-bandwidth-for-root-group">Limit block device bandwidth for root group</h3>
<p>To specify a bandwidth rate on particular device for root group, we can use the policy format as &#x201C;major:minor bytes_per_second&#x201D;.</p>
<p>The following example puts a bandwidth limit of 1MB/s on writes for root group on device having major/minor number 259:12.</p>
<pre><code class="language-shell">$ ls -la /dev/ | grep &quot;nvme3n1&quot;
brw-rw----   1 root disk    259,  12 Dec 23 21:44 nvme3n1

$ echo &quot;259:12  1048576&quot; &gt; /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device
</code></pre>
<h3 id="limit-block-device-bandwidth-for-user-defined-group">Limit block device bandwidth for user defined group</h3>
<p>The following examples specifies the IOPS limit on the device 259:12 for the user defined cgroups &quot;blkio:/blkiolimited&quot;.</p>
<p>Use control files to specify the limit directly:</p>
<pre><code class="language-shell">$ cat /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
$ echo &quot;259:12  8192&quot; &gt; /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
$ cat /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
259:12 8192
</code></pre>
<p>Use libcgroup tools to specify the limit:</p>
<pre><code class="language-shell">$ cgset -r blkio.throttle.write_iops_device=&quot;259:12 8192&quot; blkiolimited
$ cgget -r blkio.throttle.write_iops_device blkiolimited
blkiolimited:
blkio.throttle.write_iops_device: 259:12 8192
</code></pre>
<h2 id="verify-the-disk-bandwidth-usage">Verify the disk bandwidth usage</h2>
<h3 id="use-fio-to-write-50g-data-on-root-groupunlimited-bandwidth">Use fio to write 50G data on root group(unlimited bandwidth)</h3>
<pre><code class="language-shell">$ fio --blocksize=4k --ioengine=libaio --readwrite=randwrite --filesize=50G --group_reporting --direct=1 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<pre><code class="language-shell">$ iostat -ktdx 5 | grep &quot;nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme3n1           0.00     0.05    0.00    8.02     0.01   508.54   126.83     0.06    7.80    0.08    7.80   0.04   0.03
nvme3n1           0.00     0.00    0.00 133828.60     0.00 535314.40     8.00     1.18    0.01    0.00    0.01   0.00  55.64
nvme3n1           0.00     0.20    0.00 246289.20     0.00 985157.60     8.00     2.21    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 246518.80     0.00 986076.80     8.00     2.23    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 244115.60     0.00 976462.40     8.00     2.24    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 240184.00     0.00 960736.80     8.00     2.23    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 250391.60     0.00 1001567.20     8.00     2.41    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 262449.60     0.00 1049799.20     8.00     2.53    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 252171.20     0.00 1008685.60     8.00     2.30    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 236467.60     0.00 945872.00     8.00     2.16    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 255060.80     0.00 1020244.00     8.00    17.04    0.07    0.00    0.07   0.00 100.00
nvme3n1           0.00     0.20    0.00 235199.60     0.00 940798.40     8.00    44.76    0.19    0.00    0.19   0.00 100.00
nvme3n1           0.00     0.20    0.00 18768.00     0.00 75072.80     8.00     5.81    0.31    0.00    0.31   0.00   8.40
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
^C
</code></pre>
<h3 id="use-fio-to-write-50g-data-on-user-defined-grouplimited-bandwidth">Use fio to write 50G data on user defined group(limited bandwidth)</h3>
<pre><code class="language-shell">$ cgget -r blkio.throttle.write_iops_device blkiolimited
blkiolimited:
blkio.throttle.write_iops_device: 259:12 8192

$ cgexec -g blkio:blkiolimited fio --blocksize=4k --ioengine=libaio --readwrite=randwrite --filesize=50G --group_reporting --direct=1 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<pre><code class="language-shell">$ iostat -ktdx 5 | grep &quot;nvme3n1&quot;
nvme3n1           0.00     0.05    0.00   24.54     0.01   574.51    46.82     0.06    2.57    0.08    2.57   0.01   0.04
nvme3n1           0.00     0.00    0.00 3112.20     0.00 12448.80     8.00     0.04    0.01    0.00    0.01   0.01   4.60
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.11    0.01    0.00    0.01   0.01  11.92
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.02
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.22
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.10
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.02  12.36
^C
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/blkio-controller.html">https://docs.kernel.org/admin-guide/cgroup-v1/blkio-controller.html</a></li>
<li><a href="https://andrestc.com/post/cgroups-io/">https://andrestc.com/post/cgroups-io/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using cgroups to limit Memory usage]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>The memory controller isolates the memory behaviour of a group of tasks from the rest of the system.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/memory&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,</code></pre>]]></description><link>https://relentlesstorm.github.io/cgroups-limit-memory/</link><guid isPermaLink="false">63afc3ca96dbf412f1f80342</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 04 Dec 2022 05:08:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1589532768434-a92c95dad7cb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE5fHxkYXRhYmFzZXxlbnwwfHx8fDE2NzI0NjI4NzI&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1589532768434-a92c95dad7cb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE5fHxkYXRhYmFzZXxlbnwwfHx8fDE2NzI0NjI4NzI&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using cgroups to limit Memory usage"><p>The memory controller isolates the memory behaviour of a group of tasks from the rest of the system.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/memory&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)

$ lssubsys -am | grep memory
memory /sys/fs/cgroup/memory
</code></pre>
<p>In this post, we will learn how to use the following control files to limit and monitor memory usage for the user tasks.</p>
<ul>
<li>memory.usage_in_bytes - show current usage for memory</li>
<li>memory.limit_in_bytes - set/show limit of memory usage</li>
</ul>
<h2 id="create-memory-control-group">Create memory control group</h2>
<p>Install libcgroup package to manage cgroups:</p>
<pre><code class="language-shell">$ yum install libcgroup libcgroup-tools
</code></pre>
<p>Create memory control group:</p>
<pre><code class="language-shell">$ cgcreate -g memory:/memlimited
$ lscgroup | grep memory
memory:/
memory:/memlimited

$ ls /sys/fs/cgroup/memory/memlimited/
cgroup.clone_children           memory.kmem.slabinfo                memory.memsw.failcnt             memory.soft_limit_in_bytes
cgroup.event_control            memory.kmem.tcp.failcnt             memory.memsw.limit_in_bytes      memory.stat
cgroup.procs                    memory.kmem.tcp.limit_in_bytes      memory.memsw.max_usage_in_bytes  memory.swappiness
memory.failcnt                  memory.kmem.tcp.max_usage_in_bytes  memory.memsw.usage_in_bytes      memory.usage_in_bytes
memory.force_empty              memory.kmem.tcp.usage_in_bytes      memory.move_charge_at_immigrate  memory.use_hierarchy
memory.kmem.failcnt             memory.kmem.usage_in_bytes          memory.numa_stat                 notify_on_release
memory.kmem.limit_in_bytes      memory.limit_in_bytes               memory.oom_control               tasks
memory.kmem.max_usage_in_bytes  memory.max_usage_in_bytes           memory.pressure_level
</code></pre>
<h2 id="limit-the-memory-usage">Limit the memory usage</h2>
<h3 id="using-control-files-directly">Using control files directly</h3>
<pre><code class="language-shell">$ echo 32G &gt; /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
$ cat /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
34359738368
</code></pre>
<h3 id="using-libcgroup-tools">Using libcgroup tools</h3>
<p>Limit the memory usage:</p>
<pre><code class="language-shell">$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<h2 id="verify-the-memory-usage">Verify the memory usage</h2>
<h3 id="unlimit-the-memory-usage">Unlimit the memory usage</h3>
<pre><code class="language-shell">$ cgset -r memory.limit_in_bytes=-1 memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 9223372036854771712
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code class="language-shell">$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is unlimited:</p>
<pre><code class="language-shell">$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
14143488
14143488
819499008
13288558592
25772953600
38258790400
50776608768
55638511616
55638429696
55638478848
55638528000
55638577152
55210229760
55210229760
^C
</code></pre>
<h3 id="limit-the-memory-usage-to-32gb">Limit the memory usage to 32GB</h3>
<pre><code class="language-shell">$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code class="language-shell">$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code class="language-shell">$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
9134080
6819614720
18048208896
29089763328
34359726080
34359672832
34359607296
34359717888
34359635968
34359619584
34280120320
34280120320
^C
</code></pre>
<p>From vmstat output, the cache usage is limited to 32GB. There is also swapping out activity due the memory pressure.</p>
<pre><code class="language-shell">$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0      0 1053891776    968 142992    0    0     0    30    0    0  0  0 100  0  0 2022-12-23 22:04:30
 2  0      0 1045755200    968 8266836    0    0  1697     0 3789  727  2  1 98  0  0 2022-12-23 22:04:35
 1  0      0 1034800320    976 19222628    0    0     0     2 1770  575  0  1 99  0  0 2022-12-23 22:04:40
 1  0      0 1024034304    984 29988256    0    0     0 163843 2999  780  0  1 99  0  0 2022-12-23 22:04:45
 1  1      0 1020353664    992 33665560    0    0     0 985500 8317 2414  0  1 98  0  0 2022-12-23 22:04:50
 1  1 317440 1020354112   1000 33666764    0 63474     3 1518376 18524 2604  0  1 98  1  0 2022-12-23 22:04:55
 1  1 340480 1020353472   1008 33666660    0 4571     0 1602014 15377 2381  0  1 98  1  0 2022-12-23 22:05:00
 2  0 340480 1020355904   1016 33667212   39    0    39 1632866 22166 70618  0  1 98  0  0 2022-12-23 22:05:05
 1  0 340480 1020356032   1020 33667228    0    0     0 1861355 27437 108029  0  1 99  0  0 2022-12-23 22:05:10
 2  0 340480 1020356224   1024 33667236    0    0     0 1874438 28732 111364  0  1 98  0  0 2022-12-23 22:05:15
 0  0    512 1020432704   1024 33599516  212    0   550 915429 13644 56805  0  1 99  0  0 2022-12-23 22:05:20
 0  0    512 1020432960   1028 33599516    0    0     3     4  165  149  0  0 100  0  0 2022-12-23 22:05:25
^C
</code></pre>
<h3 id="limit-the-memory-usage-for-the-tasks-in-the-current-bash">Limit the memory usage for the tasks in the current bash</h3>
<pre><code class="language-shell">$ cat /sys/fs/cgroup/memory/memlimited/tasks
$ echo $$ &gt; /sys/fs/cgroup/memory/memlimited/tasks
$ cat /sys/fs/cgroup/memory/memlimited/tasks
27875
28889

$ ps -ef | egrep &quot;27875|29023&quot; | grep -v grep
root     27875 27873  0 21:11 pts/0    00:00:17 -bash
root     29026 27875  0 22:11 pts/0    00:00:00 ps -ef
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code class="language-shell">$ fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code class="language-shell">$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
22835200
22835200
22835200
10150678528
21453983744
32693850112
34359607296
34359607296
34359738368
34359730176
34359730176
34359660544
34280062976
^C
</code></pre>
<pre><code class="language-shell">$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0    512 1053886784   1004 144860    0    0     0    31    0    0  0  0 100  0  0 2022-12-23 22:09:17
 0  0    512 1053886528   1004 144824    0    0     0     0  143  148  0  0 100  0  0 2022-12-23 22:09:22
 1  0    512 1050907776   1004 3109688    0    0  1660     3 2690  566  1  0 98  0  0 2022-12-23 22:09:27
 1  0    512 1039836288   1012 14180528    0    0     0     2 1699  508  0  1 99  0  0 2022-12-23 22:09:32
 1  0    512 1028822720   1020 25194536    0    0     0     2 1625  494  0  1 99  0  0 2022-12-23 22:09:37
 1  1    512 1020348800   1028 33665604    0    0     0 385026 4478 1313  0  1 99  0  0 2022-12-23 22:09:42
 1  1  46080 1020340224   1036 33676796    0 9090     0 1563114 13781 2301  0  1 98  1  0 2022-12-23 22:09:47
 1  1 340480 1020338688   1044 33676544    0 58904     0 1557223 19212 2580  0  1 98  1  0 2022-12-23 22:09:52
 1  1 340480 1020337792   1052 33676376   33    0    33 1481028 14133 5776  0  1 98  1  0 2022-12-23 22:09:57
 1  0 340480 1020342208   1056 33676396    0    0   184 1877767 27447 114955  0  1 98  0  0 2022-12-23 22:10:02
 1  1 340480 1020341056   1056 33677100    0    0     0 1817494 27635 101675  0  1 98  0  0 2022-12-23 22:10:07
 1  0 195144 1020350208   1060 33676988   83    0    83 1872111 27068 113280  0  1 98  0  0 2022-12-23 22:10:12
^C
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/memory.html">https://docs.kernel.org/admin-guide/cgroup-v1/memory.html</a></li>
<li><a href="https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled">https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using cgroups to limit CPU utilization]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-cgroups">Intro to cgroups</h2>
<p>Cgroups(control groups) make it possible to allocate system resources such as CPU time, memory, disk I/O and network bandwidth, or combinations of them,  among a group of tasks(processes) running on a system.</p>
<p>The following commands output the available subsystems(resource controllers) for the cgroups.</p>]]></description><link>https://relentlesstorm.github.io/cgroup-limit-cpu/</link><guid isPermaLink="false">63afb5c896dbf412f1f80317</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sat, 03 Dec 2022 04:09:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1613068687893-5e85b4638b56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-cgroups">Intro to cgroups</h2>
<img src="https://images.unsplash.com/photo-1613068687893-5e85b4638b56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using cgroups to limit CPU utilization"><p>Cgroups(control groups) make it possible to allocate system resources such as CPU time, memory, disk I/O and network bandwidth, or combinations of them,  among a group of tasks(processes) running on a system.</p>
<p>The following commands output the available subsystems(resource controllers) for the cgroups. Each subsystem has a bunch of tunables to control the resource allocation.</p>
<pre><code class="language-shell">$ lssubsys -am
cpuset /sys/fs/cgroup/cpuset
cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct
blkio /sys/fs/cgroup/blkio
memory /sys/fs/cgroup/memory
devices /sys/fs/cgroup/devices
freezer /sys/fs/cgroup/freezer
net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio
perf_event /sys/fs/cgroup/perf_event
hugetlb /sys/fs/cgroup/hugetlb
pids /sys/fs/cgroup/pids
rdma /sys/fs/cgroup/rdma

$ mount | grep cgroup
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
</code></pre>
<h2 id="cpu-subsystem-and-tunables">CPU subsystem and tunables</h2>
<h3 id="ceiling-enforcement-parameters">Ceiling enforcement parameters</h3>
<ul>
<li>
<p>cpu.cfs_period_us</p>
<p>specifies a period of time in microseconds (&#xB5;s, represented here as &quot;us&quot;) for how regularly a cgroup&apos;s access to CPU resources should be reallocated. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. The upper limit of the cpu.cfs_quota_us parameter is 1 second and the lower limit is 1000 microseconds.</p>
</li>
<li>
<p>cpu.cfs_quota_us</p>
<p>specifies the total amount of time in microseconds (&#xB5;s, represented here as &quot;us&quot;) for which all tasks in a cgroup can run during one period (as defined by cpu.cfs_period_us). As soon as tasks in a cgroup use up all the time specified by the quota, they are throttled for the remainder of the time specified by the period and not allowed to run until the next period. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. Note that the quota and period parameters operate on a CPU basis. To allow a process to fully utilize two CPUs, for example, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 100000.</p>
<p>Setting the value in cpu.cfs_quota_us to -1 indicates that the cgroup does not adhere to any CPU time restrictions. This is also the default value for every cgroup (except the root cgroup).</p>
</li>
</ul>
<h3 id="relative-shares-parameter">Relative shares parameter</h3>
<ul>
<li>
<p>cpu.shares</p>
<p>contains an integer value that specifies a relative share of CPU time available to the tasks in a cgroup. For example, tasks in two cgroups that have cpu.shares set to 100 will receive equal CPU time, but tasks in a cgroup that has cpu.shares set to 200 receive twice the CPU time of tasks in a cgroup where cpu.shares is set to 100. The value specified in the cpu.shares file must be 2 or higher.</p>
<p>Note that shares of CPU time are distributed per all CPU cores on multi-core systems. Even if a cgroup is limited to less than 100% of CPU on a multi-core system, it may use 100% of each individual CPU core.</p>
<p>Using relative shares to specify CPU access has two implications on resource management that should be considered:</p>
<ul>
<li>
<p>Because the CFS does not demand equal usage of CPU, it is hard to predict how much CPU time a cgroup will be allowed to utilize. When tasks in one   cgroup are idle and are not using any CPU time, the leftover time is collected in a global pool of unused CPU cycles. Other cgroups are allowed to borrow CPU cycles from this pool.</p>
</li>
<li>
<p>The actual amount of CPU time that is available to a cgroup can vary depending on the number of cgroups that exist on the system. If a cgroup has a relative share of 1000 and two other cgroups have a relative share of 500, the first cgroup receives 50% of all CPU time in cases when processes in all cgroups attempt to use 100% of the CPU. However, if another cgroup is added with a relative share of 1000, the first cgroup is only allowed 33% of the CPU (the rest of the cgroups receive 16.5%, 16.5%, and 33% of CPU).</p>
</li>
</ul>
</li>
</ul>
<h2 id="using-libcgroup-tools">Using libcgroup tools</h2>
<p>Install libcgroup package to manage cgroups:</p>
<pre><code class="language-shell">$ yum install libcgroup libcgroup-tools
</code></pre>
<p>List the cgroups:</p>
<pre><code class="language-shell">$ lscgroup
hugetlb:/
cpu,cpuacct:/
cpuset:/
blkio:/
memory:/
freezer:/
net_cls,net_prio:/
pids:/
rdma:/
perf_event:/
devices:/
devices:/system.slice
devices:/system.slice/irqbalance.service
devices:/system.slice/systemd-udevd.service
devices:/system.slice/polkit.service
devices:/system.slice/chronyd.service
devices:/system.slice/auditd.service
devices:/system.slice/tuned.service
devices:/system.slice/systemd-journald.service
devices:/system.slice/sshd.service
devices:/system.slice/crond.service
devices:/system.slice/NetworkManager.service
devices:/system.slice/rsyslog.service
devices:/system.slice/abrtd.service
devices:/system.slice/lvm2-lvmetad.service
devices:/system.slice/postfix.service
devices:/system.slice/dbus.service
devices:/system.slice/system-getty.slice
devices:/system.slice/systemd-logind.service
devices:/system.slice/abrt-oops.service

$ ls /sys/fs/cgroup
blkio  cpuacct      cpuset   freezer  memory   net_cls,net_prio  perf_event  rdma
cpu    cpu,cpuacct  devices  hugetlb  net_cls  net_prio          pids        systemd
</code></pre>
<p>Create the cgroup:</p>
<pre><code class="language-shell">$ cgcreate -g cpu:/cpulimited

$ lscgroup | grep cpulimited
cpu,cpuacct:/cpulimited

$ ls cpulimited/
cgroup.clone_children  cpuacct.usage_percpu       cpu.cfs_period_us  cpu.stat
cgroup.procs           cpuacct.usage_percpu_sys   cpu.cfs_quota_us   notify_on_release
cpuacct.stat           cpuacct.usage_percpu_user  cpu.rt_period_us   tasks
cpuacct.usage          cpuacct.usage_sys          cpu.rt_runtime_us
cpuacct.usage_all      cpuacct.usage_user         cpu.shares
</code></pre>
<p>Limit CPU utilization by percentage:</p>
<pre><code class="language-shell">$ lscpu | grep ^CPU\(s\):
CPU(s):                96

$ cgset -r cpu.cfs_quota_us=200000 cpulimited
</code></pre>
<p>Check the cgroup settings:</p>
<pre><code class="language-shell">$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000

$ cgget -g cpu:cpulimited
cpulimited:
cpu.cfs_period_us: 100000
cpu.stat: nr_periods 2
	nr_throttled 0
	throttled_time 0
cpu.shares: 1024
cpu.cfs_quota_us: 200000
cpu.rt_runtime_us: 0
cpu.rt_period_us: 1000000
</code></pre>
<p>Delete the cgroup:</p>
<pre><code class="language-shell">$ cgdelete cpu,cpuacct:/cpulimited
</code></pre>
<h2 id="verify-the-cpu-utilization-with-fio-workload">Verify the CPU utilization with fio workload</h2>
<p>Create a fio job file:</p>
<pre><code class="language-shell">$ cat burn_cpu.job
[burn_cpu]
# Don&apos;t transfer any data, just burn CPU cycles
ioengine=cpuio
# Stress the CPU at 100%
cpuload=100
# Make 4 clones of the job
numjobs=4
</code></pre>
<p>Run the fio jobs without CPU limit:</p>
<pre><code class="language-shell">$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: -1

$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code class="language-shell">$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
13775 root      20   0 1079912   4016   2404 R 100.0  0.0   0:11.65 fio
13776 root      20   0 1079916   4004   2392 R 100.0  0.0   0:11.65 fio
13777 root      20   0 1079920   4004   2392 R 100.0  0.0   0:11.65 fio
13778 root      20   0 1079924   4004   2392 R 100.0  0.0   0:11.65 fio
</code></pre>
<p>The CPU utilization is 400% for the 4 fio jobs when there is no CPU limit set. Note that, there is totally 9600% CPU bandwidth available.</p>
<p>Limit the CPU utilization to 200%:</p>
<pre><code class="language-shell">$ cgset -r cpu.cfs_quota_us=200000 cpulimited

$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000
</code></pre>
<p>Run the fio jobs again:</p>
<pre><code class="language-shell">$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code class="language-shell">$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12908 root      20   0 1079916   3948   2336 R  50.3  0.0   0:06.91 fio
12909 root      20   0 1079920   3948   2336 R  50.0  0.0   0:06.88 fio
12910 root      20   0 1079924   3948   2336 R  50.0  0.0   0:06.93 fio
12907 root      20   0 1079912   3948   2336 R  49.3  0.0   0:06.86 fio
</code></pre>
<p>The CPU utilization is 200% for the 4 fio jobs when the CPU utilization is limited to 200%.</p>
<p>Check the processes are running on which CPU cores:</p>
<pre><code class="language-shell">$ mpstat -P ALL 5 | awk &apos;{if ($3==&quot;CPU&quot; || $NF&lt;99)print;}&apos;

12:40:32 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:37 AM  all    2.11    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   97.89
12:40:37 AM    0   20.52    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   79.48
12:40:37 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:37 AM    2   50.10    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.90
12:40:37 AM   24   29.74    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   70.26
12:40:37 AM   87   50.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.80

12:40:37 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:42 AM  all    2.11    0.00    0.01    0.00    0.00    0.00    0.00    0.00    0.00   97.88
12:40:42 AM    0   11.49    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00   88.31
12:40:42 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:42 AM    2   50.30    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.70
12:40:42 AM   24   38.97    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   61.03
12:40:42 AM   87   49.90    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   50.10
</code></pre>
<p>The 4 fio jobs are running on 5 CPU cores with total utilization of 200%. So, it indicates this method limits the total CPU utilization out of all the CPU cores. However, the number of CPU cores is not limited.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html">https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_libcgroup_tools">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_libcgroup_tools</a></li>
<li><a href="https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups">https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[How to delete partition in Linux]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="identify-the-disk-which-contains-the-partitions">Identify the disk which contains the partitions</h2>
<pre><code class="language-shell">$ lsblk
NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sr0                           11:0    1   9.5G  0 rom
nvme0n1                      259:6    0   1.5T  0 disk
&#x251C;&#x2500;nvme0n1p1                  259:7    0   200M  0 part /boot/efi
&#x251C;&#x2500;nvme0n1p2                  259:8    0     1G</code></pre>]]></description><link>https://relentlesstorm.github.io/how-to-delete-partition-in-linux/</link><guid isPermaLink="false">63afc55496dbf412f1f80368</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Fri, 11 Nov 2022 05:17:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1629654297299-c8506221ca97?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGxpbnV4fGVufDB8fHx8MTY3MjQ2Mzc1MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="identify-the-disk-which-contains-the-partitions">Identify the disk which contains the partitions</h2>
<pre><code class="language-shell">$ lsblk
NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sr0                           11:0    1   9.5G  0 rom
nvme0n1                      259:6    0   1.5T  0 disk
&#x251C;&#x2500;nvme0n1p1                  259:7    0   200M  0 part /boot/efi
&#x251C;&#x2500;nvme0n1p2                  259:8    0     1G  0 part /boot
&#x2514;&#x2500;nvme0n1p3                  259:9    0   1.5T  0 part
  &#x251C;&#x2500;centos_init500--c11-root 253:0    0    50G  0 lvm  /
  &#x251C;&#x2500;centos_init500--c11-swap 253:1    0     4G  0 lvm  [SWAP]
  &#x2514;&#x2500;centos_init500--c11-home 253:2    0   1.4T  0 lvm  /home
nvme1n1                      259:10   0   1.5T  0 disk
&#x251C;&#x2500;nvme1n1p1                  259:13   0   100M  0 part
&#x251C;&#x2500;nvme1n1p5                  259:14   0     4G  0 part
&#x251C;&#x2500;nvme1n1p6                  259:15   0     4G  0 part
&#x251C;&#x2500;nvme1n1p7                  259:17   0 119.9G  0 part
&#x2514;&#x2500;nvme1n1p8                  259:18   0   1.3T  0 part
</code></pre>
<pre><code class="language-shell">$ fdisk -l
[..]
Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x7f33a7d2

        Device Boot      Start         End      Blocks   Id  System
/dev/nvme1n1p1               1  3125627534  1562813767   ee  GPT
[..]
</code></pre>
<h2 id="delete-the-partitions">Delete the partitions</h2>
<img src="https://images.unsplash.com/photo-1629654297299-c8506221ca97?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGxpbnV4fGVufDB8fHx8MTY3MjQ2Mzc1MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="How to delete partition in Linux"><p>Select the disk that contains the partitions you intend to delete:</p>
<pre><code class="language-shell">$ fdisk /dev/nvme1n1
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.
</code></pre>
<p>Delete the partition:</p>
<pre><code class="language-shell">Command (m for help): d
Selected partition 1
Partition 1 is deleted
</code></pre>
<p><strong>NOTE:</strong> The partition is automatically selected if there are no other partitions on disk. If the disk contains multiple partitions, select a partition that you want to delete.</p>
<p>Verify the paritions:</p>
<pre><code class="language-shell">Command (m for help): p

Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x7f33a7d2

        Device Boot      Start         End      Blocks   Id  System
</code></pre>
<p>Save the change and quit fdisk:</p>
<pre><code class="language-shell">Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
</code></pre>
<p>Verify the partitions with lsblk:</p>
<pre><code class="language-shell">$ lsblk
NAME                         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                           11:0    1  9.5G  0 rom
nvme0n1                      259:6    0  1.5T  0 disk
&#x251C;&#x2500;nvme0n1p1                  259:7    0  200M  0 part /boot/efi
&#x251C;&#x2500;nvme0n1p2                  259:8    0    1G  0 part /boot
&#x2514;&#x2500;nvme0n1p3                  259:9    0  1.5T  0 part
  &#x251C;&#x2500;centos_init500--c11-root 253:0    0   50G  0 lvm  /
  &#x251C;&#x2500;centos_init500--c11-swap 253:1    0    4G  0 lvm  [SWAP]
  &#x2514;&#x2500;centos_init500--c11-home 253:2    0  1.4T  0 lvm  /home
nvme1n1                      259:10   0  1.5T  0 disk
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Useful Elastic cluster APIs]]></title><description><![CDATA[Useful Elastic cluster APIs]]></description><link>https://relentlesstorm.github.io/useful-es-apis/</link><guid isPermaLink="false">63afca8a96dbf412f1f803d9</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Fri, 28 Oct 2022 05:37:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1623282033815-40b05d96c903?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGFwaXxlbnwwfHx8fDE2NzI0NjUxMTA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="cluster-settings">Cluster settings</h2>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_cluster/health
{
  &quot;cluster_name&quot;: &quot;rally-benchmark&quot;,
  &quot;status&quot;: &quot;green&quot;,
  &quot;timed_out&quot;: false,
  &quot;number_of_nodes&quot;: 3,
  &quot;number_of_data_nodes&quot;: 3,
  &quot;active_primary_shards&quot;: 8,
  &quot;active_shards&quot;: 16,
  &quot;relocating_shards&quot;: 0,
  &quot;initializing_shards&quot;: 0,
  &quot;unassigned_shards&quot;: 0,
  &quot;delayed_unassigned_shards&quot;: 0,
  &quot;number_of_pending_tasks&quot;: 0,
  &quot;number_of_in_flight_fetch&quot;: 0,
  &quot;task_max_waiting_in_queue_millis&quot;: 0,
  &quot;active_shards_percent_as_number&quot;: 100
}
</code></pre>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_cluster/settings| jq -r
{
  &quot;persistent&quot;: {},
  &quot;transient&quot;: {}
}
</code></pre>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_cluster/stats| jq -r
{
  &quot;_nodes&quot;: {
    &quot;total&quot;: 3,
    &quot;successful&quot;: 3,
    &quot;failed&quot;: 0
  },
  &quot;cluster_name&quot;: &quot;rally-benchmark&quot;,
  &quot;cluster_uuid&quot;: &quot;_aPIWBOCS0Gtyo4fTp3Wrg&quot;,
  &quot;timestamp&quot;: 1667177008901,
  &quot;status&quot;: &quot;green&quot;,
  &quot;indices&quot;: {
    &quot;count&quot;: 4,
    &quot;shards&quot;: {
      &quot;total&quot;: 16,
      &quot;primaries&quot;: 8,
      &quot;replication&quot;: 1,
      &quot;index&quot;: {
        &quot;shards&quot;: {
          &quot;min&quot;: 2,
          &quot;max&quot;: 10,
          &quot;avg&quot;: 4
        },
        &quot;primaries&quot;: {
          &quot;min&quot;: 1,
          &quot;max&quot;: 5,
          &quot;avg&quot;: 2
        },
        &quot;replication&quot;: {
          &quot;min&quot;: 1,
          &quot;max&quot;: 1,
          &quot;avg&quot;: 1
        }
      }
    },
    &quot;docs&quot;: {
      &quot;count&quot;: 11396556,
      &quot;deleted&quot;: 0
    },
    &quot;store&quot;: {
      &quot;size_in_bytes&quot;: 6443644350,
      &quot;total_data_set_size_in_bytes&quot;: 6443644350,
      &quot;reserved_in_bytes&quot;: 0
    },
    &quot;fielddata&quot;: {
      &quot;memory_size_in_bytes&quot;: 14072,
      &quot;evictions&quot;: 0
    },
    &quot;query_cache&quot;: {
      &quot;memory_size_in_bytes&quot;: 0,
      &quot;total_count&quot;: 3883795,
      &quot;hit_count&quot;: 0,
      &quot;miss_count&quot;: 3883795,
      &quot;cache_size&quot;: 0,
      &quot;cache_count&quot;: 0,
      &quot;evictions&quot;: 0
    },
    &quot;completion&quot;: {
      &quot;size_in_bytes&quot;: 0
    },
    &quot;segments&quot;: {
      &quot;count&quot;: 219,
      &quot;memory_in_bytes&quot;: 1526988,
      &quot;terms_memory_in_bytes&quot;: 1189120,
      &quot;stored_fields_memory_in_bytes&quot;: 115320,
      &quot;term_vectors_memory_in_bytes&quot;: 0,
      &quot;norms_memory_in_bytes&quot;: 155520,
      &quot;points_memory_in_bytes&quot;: 0,
      &quot;doc_values_memory_in_bytes&quot;: 67028,
      &quot;index_writer_memory_in_bytes&quot;: 0,
      &quot;version_map_memory_in_bytes&quot;: 0,
      &quot;fixed_bit_set_memory_in_bytes&quot;: 0,
      &quot;max_unsafe_auto_id_timestamp&quot;: 1667016435644,
      &quot;file_sizes&quot;: {}
    },
    &quot;mappings&quot;: {
      &quot;field_types&quot;: [
        {
          &quot;name&quot;: &quot;boolean&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;constant_keyword&quot;,
          &quot;count&quot;: 3,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;date&quot;,
          &quot;count&quot;: 6,
          &quot;index_count&quot;: 2,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;geo_point&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;integer&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;ip&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;keyword&quot;,
          &quot;count&quot;: 32,
          &quot;index_count&quot;: 3,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;long&quot;,
          &quot;count&quot;: 3,
          &quot;index_count&quot;: 2,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;object&quot;,
          &quot;count&quot;: 13,
          &quot;index_count&quot;: 2,
          &quot;script_count&quot;: 0
        },
        {
          &quot;name&quot;: &quot;text&quot;,
          &quot;count&quot;: 17,
          &quot;index_count&quot;: 3,
          &quot;script_count&quot;: 0
        }
      ],
      &quot;runtime_field_types&quot;: []
    },
    &quot;analysis&quot;: {
      &quot;char_filter_types&quot;: [],
      &quot;tokenizer_types&quot;: [],
      &quot;filter_types&quot;: [],
      &quot;analyzer_types&quot;: [],
      &quot;built_in_char_filters&quot;: [],
      &quot;built_in_tokenizers&quot;: [],
      &quot;built_in_filters&quot;: [],
      &quot;built_in_analyzers&quot;: []
    },
    &quot;versions&quot;: [
      {
        &quot;version&quot;: &quot;7.17.0&quot;,
        &quot;index_count&quot;: 4,
        &quot;primary_shard_count&quot;: 8,
        &quot;total_primary_bytes&quot;: 3221823908
      }
    ]
  },
  &quot;nodes&quot;: {
    &quot;count&quot;: {
      &quot;total&quot;: 3,
      &quot;coordinating_only&quot;: 0,
      &quot;data&quot;: 3,
      &quot;data_cold&quot;: 3,
      &quot;data_content&quot;: 3,
      &quot;data_frozen&quot;: 3,
      &quot;data_hot&quot;: 3,
      &quot;data_warm&quot;: 3,
      &quot;ingest&quot;: 3,
      &quot;master&quot;: 3,
      &quot;ml&quot;: 0,
      &quot;remote_cluster_client&quot;: 3,
      &quot;transform&quot;: 3,
      &quot;voting_only&quot;: 0
    },
    &quot;versions&quot;: [
      &quot;7.17.0&quot;
    ],
    &quot;os&quot;: {
      &quot;available_processors&quot;: 24,
      &quot;allocated_processors&quot;: 24,
      &quot;names&quot;: [
        {
          &quot;name&quot;: &quot;Linux&quot;,
          &quot;count&quot;: 3
        }
      ],
      &quot;pretty_names&quot;: [
        {
          &quot;pretty_name&quot;: &quot;CentOS Linux 7 (Core)&quot;,
          &quot;count&quot;: 3
        }
      ],
      &quot;architectures&quot;: [
        {
          &quot;arch&quot;: &quot;amd64&quot;,
          &quot;count&quot;: 3
        }
      ],
      &quot;mem&quot;: {
        &quot;total_in_bytes&quot;: 100700000256,
        &quot;free_in_bytes&quot;: 68239511552,
        &quot;used_in_bytes&quot;: 32460488704,
        &quot;free_percent&quot;: 68,
        &quot;used_percent&quot;: 32
      }
    },
    &quot;process&quot;: {
      &quot;cpu&quot;: {
        &quot;percent&quot;: 0
      },
      &quot;open_file_descriptors&quot;: {
        &quot;min&quot;: 460,
        &quot;max&quot;: 493,
        &quot;avg&quot;: 474
      }
    },
    &quot;jvm&quot;: {
      &quot;max_uptime_in_millis&quot;: 169991344,
      &quot;versions&quot;: [
        {
          &quot;version&quot;: &quot;1.8.0_352&quot;,
          &quot;vm_name&quot;: &quot;OpenJDK 64-Bit Server VM&quot;,
          &quot;vm_version&quot;: &quot;25.352-b08&quot;,
          &quot;vm_vendor&quot;: &quot;Red Hat, Inc.&quot;,
          &quot;bundled_jdk&quot;: true,
          &quot;using_bundled_jdk&quot;: false,
          &quot;count&quot;: 3
        }
      ],
      &quot;mem&quot;: {
        &quot;heap_used_in_bytes&quot;: 1671452984,
        &quot;heap_max_in_bytes&quot;: 3113877504
      },
      &quot;threads&quot;: 197
    },
    &quot;fs&quot;: {
      &quot;total_in_bytes&quot;: 126144983040,
      &quot;free_in_bytes&quot;: 115344986112,
      &quot;available_in_bytes&quot;: 115344986112
    },
    &quot;plugins&quot;: [],
    &quot;network_types&quot;: {
      &quot;transport_types&quot;: {
        &quot;netty4&quot;: 3
      },
      &quot;http_types&quot;: {
        &quot;netty4&quot;: 3
      }
    },
    &quot;discovery_types&quot;: {
      &quot;zen&quot;: 3
    },
    &quot;packaging_types&quot;: [
      {
        &quot;flavor&quot;: &quot;default&quot;,
        &quot;type&quot;: &quot;tar&quot;,
        &quot;count&quot;: 3
      }
    ],
    &quot;ingest&quot;: {
      &quot;number_of_pipelines&quot;: 2,
      &quot;processor_stats&quot;: {
        &quot;gsub&quot;: {
          &quot;count&quot;: 0,
          &quot;failed&quot;: 0,
          &quot;current&quot;: 0,
          &quot;time_in_millis&quot;: 0
        },
        &quot;script&quot;: {
          &quot;count&quot;: 0,
          &quot;failed&quot;: 0,
          &quot;current&quot;: 0,
          &quot;time_in_millis&quot;: 0
        }
      }
    }
  }
}
</code></pre>
<h2 id="node-stats">Node stats</h2>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_nodes
$ curl -q -s http://10.10.10.1:39200/_nodes/usage
$ curl -q -s http://10.10.10.2:39200/_nodes/stats/fs?pretty
{
  &quot;_nodes&quot; : {
    &quot;total&quot; : 3,
    &quot;successful&quot; : 3,
    &quot;failed&quot; : 0
  },
  &quot;cluster_name&quot; : &quot;rally-benchmark&quot;,
  &quot;nodes&quot; : {
    &quot;Zyv-MlSUQeOpU-__o2NblA&quot; : {
      &quot;timestamp&quot; : 1667082588000,
      &quot;name&quot; : &quot;rally-node-1&quot;,
      &quot;transport_address&quot; : &quot;10.10.10.2:39300&quot;,
      &quot;host&quot; : &quot;10.10.10.2&quot;,
      &quot;ip&quot; : &quot;10.10.10.2:39300&quot;,
      &quot;roles&quot; : [
        &quot;data&quot;,
        &quot;data_cold&quot;,
        &quot;data_content&quot;,
        &quot;data_frozen&quot;,
        &quot;data_hot&quot;,
        &quot;data_warm&quot;,
        &quot;ingest&quot;,
        &quot;master&quot;,
        &quot;remote_cluster_client&quot;,
        &quot;transform&quot;
      ],
      &quot;attributes&quot; : {
        &quot;xpack.installed&quot; : &quot;true&quot;,
        &quot;transform.node&quot; : &quot;true&quot;
      },
      &quot;fs&quot; : {
        &quot;timestamp&quot; : 1667082588001,
        &quot;total&quot; : {
          &quot;total_in_bytes&quot; : 42048327680,
          &quot;free_in_bytes&quot; : 39852097536,
          &quot;available_in_bytes&quot; : 39852097536
        },
        &quot;data&quot; : [
          {
            &quot;path&quot; : &quot;/home/es/.rally/benchmarks/races/53e2ac0b-c648-4153-97d4-46749f90daed/rally-node-1/install/elasticsearch-7.17.0/data/nodes/0&quot;,
            &quot;mount&quot; : &quot;/home (/dev/mapper/centos-home)&quot;,
            &quot;type&quot; : &quot;xfs&quot;,
            &quot;total_in_bytes&quot; : 42048327680,
            &quot;free_in_bytes&quot; : 39852097536,
            &quot;available_in_bytes&quot; : 39852097536
          }
        ],
        &quot;io_stats&quot; : {
          &quot;devices&quot; : [
            {
              &quot;device_name&quot; : &quot;dm-2&quot;,
              &quot;operations&quot; : 52115,
              &quot;read_operations&quot; : 0,
              &quot;write_operations&quot; : 52115,
              &quot;read_kilobytes&quot; : 0,
              &quot;write_kilobytes&quot; : 7926721,
              &quot;io_time_in_millis&quot; : 8549
            }
          ],
          &quot;total&quot; : {
            &quot;operations&quot; : 52115,
            &quot;read_operations&quot; : 0,
            &quot;write_operations&quot; : 52115,
            &quot;read_kilobytes&quot; : 0,
            &quot;write_kilobytes&quot; : 7926721,
            &quot;io_time_in_millis&quot; : 8549
          }
        }
      }
    },
    [..]
  }
}
</code></pre>
<img src="https://images.unsplash.com/photo-1623282033815-40b05d96c903?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGFwaXxlbnwwfHx8fDE2NzI0NjUxMTA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Useful Elastic cluster APIs"><p>The data path can also be checked in node-config.json.</p>
<pre><code class="language-shell">$ cat races/aa826112-d371-4f09-9b68-f9084e7c9e0b/node-config.json
{
  &quot;build-type&quot;: &quot;tar&quot;,
  &quot;car-runtime-jdks&quot;: &quot;17,16,15,14,13,12,11,8&quot;,
  &quot;car-provides-bundled-jdk&quot;: true,
  &quot;ip&quot;: &quot;10.10.10.1&quot;,
  &quot;node-name&quot;: &quot;rally-node-0&quot;,
  &quot;node-root-path&quot;: &quot;/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0&quot;,
  &quot;binary-path&quot;: &quot;/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0&quot;,
  &quot;data-paths&quot;: [
    &quot;/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/data&quot;
  ]
}
</code></pre>
<h2 id="shards-and-replicas">Shards and Replicas</h2>
<p>List all the available shards</p>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_cat/shards
.geoip_databases                                              0 p STARTED      41  39.1mb 10.10.10.2 rally-node-1
.geoip_databases                                              0 r STARTED      41  39.1mb 10.10.10.3 rally-node-2
geonames                                                      4 p STARTED 2279907 610.1mb 10.10.10.1  rally-node-0
geonames                                                      3 p STARTED 2282121 605.1mb 10.10.10.2 rally-node-1
geonames                                                      2 p STARTED 2280777 603.3mb 10.10.10.3 rally-node-2
geonames                                                      1 p STARTED 2277042 609.3mb 10.10.10.1  rally-node-0
geonames                                                      0 p STARTED 2276656 605.3mb 10.10.10.2 rally-node-1
.ds-.logs-deprecation.elasticsearch-default-2022.10.29-000001 0 r STARTED                 10.10.10.1  rally-node-0
.ds-.logs-deprecation.elasticsearch-default-2022.10.29-000001 0 p STARTED                 10.10.10.3 rally-node-2
.ds-ilm-history-5-2022.10.29-000001                           0 r STARTED                 10.10.10.2 rally-node-1
.ds-ilm-history-5-2022.10.29-000001                           0 p STARTED                 10.10.10.1  rally-node-0                        0 p STARTED                 10.10.10.1  rally-node-0
</code></pre>
<p>List all the available shards for the target index</p>
<p>By default, there are 5 primary shards for the target index.</p>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_cat/shards/geonames
geonames 4 p STARTED 2279907 610.1mb 10.10.10.1 rally-node-0
geonames 3 p STARTED 2282121 605.1mb 10.10.10.2 rally-node-1
geonames 2 p STARTED 2280777 603.3mb 10.10.10.3 rally-node-2
geonames 1 p STARTED 2277042 609.3mb 10.10.10.1 rally-node-0
geonames 0 p STARTED 2276656 605.3mb 10.10.10.2 rally-node-1
</code></pre>
<p>Check the settings of target index</p>
<p>By default, there are 5 primary shards and no replica shards for the target index.</p>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/geonames/_settings | jq -r
{
  &quot;geonames&quot;: {
    &quot;settings&quot;: {
      &quot;index&quot;: {
        &quot;routing&quot;: {
          &quot;allocation&quot;: {
            &quot;include&quot;: {
              &quot;_tier_preference&quot;: &quot;data_content&quot;
            }
          }
        },
        &quot;number_of_shards&quot;: &quot;5&quot;,
        &quot;provided_name&quot;: &quot;geonames&quot;,
        &quot;creation_date&quot;: &quot;1667016351070&quot;,
        &quot;requests&quot;: {
          &quot;cache&quot;: {
            &quot;enable&quot;: &quot;false&quot;
          }
        },
        &quot;store&quot;: {
          &quot;type&quot;: &quot;fs&quot;
        },
        &quot;number_of_replicas&quot;: &quot;0&quot;,
        &quot;uuid&quot;: &quot;ecaI9RlcSvqu0Lwo4f3nOA&quot;,
        &quot;version&quot;: {
          &quot;created&quot;: &quot;7170099&quot;
        }
      }
    }
  }
}
</code></pre>
<p>Change number_of_replicas for the target index</p>
<pre><code class="language-shell">$ curl -X PUT -H &quot;Content-Type: application/json&quot; &quot;http://10.10.10.1:39200/geonames/_settings&quot; -d &apos;{&quot;number_of_replicas&quot;:1}&apos;
{&quot;acknowledged&quot;:true}

$ curl -q -s http://10.10.10.1:39200/geonames/_settings | jq -r
{
  &quot;geonames&quot;: {
    &quot;settings&quot;: {
      &quot;index&quot;: {
        &quot;routing&quot;: {
          &quot;allocation&quot;: {
            &quot;include&quot;: {
              &quot;_tier_preference&quot;: &quot;data_content&quot;
            }
          }
        },
        &quot;number_of_shards&quot;: &quot;5&quot;,
        &quot;provided_name&quot;: &quot;geonames&quot;,
        &quot;creation_date&quot;: &quot;1667016351070&quot;,
        &quot;requests&quot;: {
          &quot;cache&quot;: {
            &quot;enable&quot;: &quot;false&quot;
          }
        },
        &quot;store&quot;: {
          &quot;type&quot;: &quot;fs&quot;
        },
        &quot;number_of_replicas&quot;: &quot;1&quot;,
        &quot;uuid&quot;: &quot;ecaI9RlcSvqu0Lwo4f3nOA&quot;,
        &quot;version&quot;: {
          &quot;created&quot;: &quot;7170099&quot;
        }
      }
    }
  }
}
</code></pre>
<p>Verify the index replicas</p>
<p>After changing number_of_replicas from 0 to 1, one replica shard is created for each primary shard. The primary shard and replica shard are located on different nodes. If a node goes down, the replica shards will automatically become primary shards and the cluster still works properly.</p>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_cat/shards/geonames
geonames 3 p STARTED 2282121 605.1mb 10.10.10.2 rally-node-1
geonames 3 r STARTED 2282121 605.1mb 10.10.10.1 rally-node-0
geonames 4 r STARTED 2279907 610.1mb 10.10.10.2 rally-node-1
geonames 4 p STARTED 2279907 610.1mb 10.10.10.1 rally-node-0
geonames 2 r STARTED 2280777 603.3mb 10.10.10.2 rally-node-1
geonames 2 p STARTED 2280777 603.3mb 10.10.10.3 rally-node-2
geonames 1 p STARTED 2277042 609.3mb 10.10.10.1 rally-node-0
geonames 1 r STARTED 2277042 609.3mb 10.10.10.3 rally-node-2
geonames 0 p STARTED 2276656 605.3mb 10.10.10.2 rally-node-1
geonames 0 r STARTED 2276656 605.3mb 10.10.10.3 rally-node-2
</code></pre>
<h2 id="task-management">Task management</h2>
<pre><code class="language-shell">$ curl -q -s http://10.10.10.1:39200/_tasks
{
  &quot;nodes&quot;: {
    &quot;Zyv-MlSUQeOpU-__o2NblA&quot;: {
      &quot;name&quot;: &quot;rally-node-1&quot;,
      &quot;transport_address&quot;: &quot;10.10.10.2:39300&quot;,
      &quot;host&quot;: &quot;10.10.10.2&quot;,
      &quot;ip&quot;: &quot;10.10.10.2:39300&quot;,
      &quot;roles&quot;: [
        &quot;data&quot;,
        &quot;data_cold&quot;,
        &quot;data_content&quot;,
        &quot;data_frozen&quot;,
        &quot;data_hot&quot;,
        &quot;data_warm&quot;,
        &quot;ingest&quot;,
        &quot;master&quot;,
        &quot;remote_cluster_client&quot;,
        &quot;transform&quot;
      ],
      &quot;attributes&quot;: {
        &quot;xpack.installed&quot;: &quot;true&quot;,
        &quot;transform.node&quot;: &quot;true&quot;
      },
      &quot;tasks&quot;: {
        &quot;Zyv-MlSUQeOpU-__o2NblA:500850&quot;: {
          &quot;node&quot;: &quot;Zyv-MlSUQeOpU-__o2NblA&quot;,
          &quot;id&quot;: 500850,
          &quot;type&quot;: &quot;transport&quot;,
          &quot;action&quot;: &quot;cluster:monitor/tasks/lists[n]&quot;,
          &quot;start_time_in_millis&quot;: 1667176733761,
          &quot;running_time_in_nanos&quot;: 125953,
          &quot;cancellable&quot;: false,
          &quot;parent_task_id&quot;: &quot;viZm_2hlQbib6qqccdXgNA:324323&quot;,
          &quot;headers&quot;: {}
        }
      }
    },
    [..]
  }
}
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html</a></li>
<li><a href="https://stackoverflow.com/questions/15694724/shards-and-replicas-in-elasticsearch">Shards and replics in Elasticsearch</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Benchmarking Elasticsearch cluster with Rally]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-rally-on-each-node">Install Rally on each node</h2>
<p>Prerequisites</p>
<pre><code class="language-shell">$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;
</code></pre>
<p>Install Python 3.8+</p>
<pre><code class="language-shell">$ wget https://www.python.org/ftp/python/3.8.15/Python-3.8.15.tar.xz
$ tar xf Python-3.8.15.tar.xz
$ vim Python-3.8.15/Modules/</code></pre>]]></description><link>https://relentlesstorm.github.io/benchmark-es-cluster-with-rally/</link><guid isPermaLink="false">63afc9d396dbf412f1f803c7</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 26 Oct 2022 05:34:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1496096265110-f83ad7f96608?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDMwfHx0ZWNobm9sb2d5fGVufDB8fHx8MTY3MjQ2NDY5NQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-rally-on-each-node">Install Rally on each node</h2>
<img src="https://images.unsplash.com/photo-1496096265110-f83ad7f96608?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDMwfHx0ZWNobm9sb2d5fGVufDB8fHx8MTY3MjQ2NDY5NQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Benchmarking Elasticsearch cluster with Rally"><p>Prerequisites</p>
<pre><code class="language-shell">$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;
</code></pre>
<p>Install Python 3.8+</p>
<pre><code class="language-shell">$ wget https://www.python.org/ftp/python/3.8.15/Python-3.8.15.tar.xz
$ tar xf Python-3.8.15.tar.xz
$ vim Python-3.8.15/Modules/Setup
SSL=/usr/local/ssl
_ssl _ssl.c \
        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        -L$(SSL)/lib -lssl -lcrypto
$ mv Python-3.8.15 /usr/src
$ cd /usr/src/Python-3.8.15/
$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl
$ pip3.8 install --upgrade pip
</code></pre>
<p>Install Git (Not required for load generator node)</p>
<pre><code class="language-shell">$ yum install libcurl-devel
$ wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.38.1.tar.xz
$ tar xvf git-2.38.1.tar.xz
$ cd git-2.38.1/
$ make configure
$ ./configure --prefix=/usr/local
$ make install
$ git --version
git version 2.38.1
</code></pre>
<p>Install JDK (Not required for load generator node)</p>
<pre><code class="language-shell">$ yum install java
$ java -version
openjdk version &quot;1.8.0_352&quot;
</code></pre>
<p>Install esrally</p>
<pre><code class="language-shell">$ pip3.8 install esrally
$ esrally --version
esrally 2.6.0
</code></pre>
<p>Elasticsearch can not be launched as root. Create a non-root user on each node.</p>
<pre><code class="language-shell">$ groupadd es
$ useradd es -g es
$ passwd es
$ cd /home/es
$ su - es
</code></pre>
<p>Set JAVA_HOME path</p>
<pre><code class="language-shell">$ vim .bash_profile
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
export JAVA_HOME
$ source .bash_profile
$ echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
</code></pre>
<h2 id="benchmarking-a-single-node">Benchmarking a Single Node</h2>
<p>Install Elasticsearch</p>
<pre><code class="language-shell">$ esrally install --distribution-version=7.17.0 --node-name=&quot;rally-node-0&quot; --network-host=&quot;127.0.0.1&quot; --http-port=39200 --master-nodes=&quot;rally-node-0&quot; --seed-hosts=&quot;127.0.0.1:39300&quot;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)                       [100%]
{
  &quot;installation-id&quot;: &quot;10735bfa-f1b8-44c4-8e7f-8932c8daa201&quot;
}

--------------------------------
[INFO] SUCCESS (took 10 seconds)
--------------------------------
</code></pre>
<p>Start the Elasticsearch node</p>
<pre><code class="language-shell">$ export INSTALLATION_ID=10735bfa-f1b8-44c4-8e7f-8932c8daa201
$ export RACE_ID=$(uuidgen)
$ esrally start --installation-id=&quot;${INSTALLATION_ID}&quot; --race-id=&quot;${RACE_ID}&quot;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/


-------------------------------
[INFO] SUCCESS (took 3 seconds)
-------------------------------
</code></pre>
<p>Run a benchmark</p>
<pre><code class="language-shell">$ esrally race --pipeline=benchmark-only --target-host=127.0.0.1:39200 --track=geonames --challenge=append-no-conflicts-index-only --on-error=abort --race-id=${RACE_ID}

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [4e93324a-9326-49e8-be72-9f77cc837657]
[INFO] Downloading track data (252.9 MB total size)                               [100.0%]
[INFO] Decompressing track data from [/home/es/.rally/benchmarks/data/geonames/documents-2.json.bz2] to [/home/es/.rally/benchmarks/data/geonames/documents-2.json] (resulting size: [3.30] GB) ... [OK]
[INFO] Preparing file offset table for [/home/es/.rally/benchmarks/data/geonames/documents-2.json] ... [OK]
[INFO] Racing on track [geonames], challenge [append-no-conflicts-index-only] and car [&apos;external&apos;] with version [7.17.0].

Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running force-merge                                                            [100% done]
Running wait-until-merges-finish                                               [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |         Task |           Value |   Unit |
|---------------------------------------------------------------:|-------------:|----------------:|-------:|
|                     Cumulative indexing time of primary shards |              |    12.5856      |    min |
|             Min cumulative indexing time across primary shards |              |     0.0190333   |    min |
|          Median cumulative indexing time across primary shards |              |     2.49065     |    min |
|             Max cumulative indexing time across primary shards |              |     2.61288     |    min |
|            Cumulative indexing throttle time of primary shards |              |     0.0081      |    min |
|    Min cumulative indexing throttle time across primary shards |              |     0           |    min |
| Median cumulative indexing throttle time across primary shards |              |     0           |    min |
|    Max cumulative indexing throttle time across primary shards |              |     0.0061      |    min |
|                        Cumulative merge time of primary shards |              |     6.75938     |    min |
|                       Cumulative merge count of primary shards |              |    55           |        |
|                Min cumulative merge time across primary shards |              |     0           |    min |
|             Median cumulative merge time across primary shards |              |     1.32735     |    min |
|                Max cumulative merge time across primary shards |              |     1.47518     |    min |
|               Cumulative merge throttle time of primary shards |              |     1.72393     |    min |
|       Min cumulative merge throttle time across primary shards |              |     0           |    min |
|    Median cumulative merge throttle time across primary shards |              |     0.323533    |    min |
|       Max cumulative merge throttle time across primary shards |              |     0.42085     |    min |
|                      Cumulative refresh time of primary shards |              |     2.97073     |    min |
|                     Cumulative refresh count of primary shards |              |   246           |        |
|              Min cumulative refresh time across primary shards |              |     0.00185     |    min |
|           Median cumulative refresh time across primary shards |              |     0.5943      |    min |
|              Max cumulative refresh time across primary shards |              |     0.599217    |    min |
|                        Cumulative flush time of primary shards |              |     0.237767    |    min |
|                       Cumulative flush count of primary shards |              |     8           |        |
|                Min cumulative flush time across primary shards |              |     0.00241667  |    min |
|             Median cumulative flush time across primary shards |              |     0.0473      |    min |
|                Max cumulative flush time across primary shards |              |     0.0492167   |    min |
|                                        Total Young Gen GC time |              |    14.009       |      s |
|                                       Total Young Gen GC count |              |  1159           |        |
|                                          Total Old Gen GC time |              |     3.491       |      s |
|                                         Total Old Gen GC count |              |    66           |        |
|                                                     Store size |              |     3.20482     |     GB |
|                                                  Translog size |              |     3.07336e-07 |     GB |
|                                         Heap used for segments |              |     1.00685     |     MB |
|                                       Heap used for doc values |              |     0.0602913   |     MB |
|                                            Heap used for terms |              |     0.77124     |     MB |
|                                            Heap used for norms |              |     0.104492    |     MB |
|                                           Heap used for points |              |     0           |     MB |
|                                    Heap used for stored fields |              |     0.0708237   |     MB |
|                                                  Segment count |              |   141           |        |
|                                    Total Ingest Pipeline count |              |     0           |        |
|                                     Total Ingest Pipeline time |              |     0           |      s |
|                                   Total Ingest Pipeline failed |              |     0           |        |
|                                                 Min Throughput | index-append | 86654           | docs/s |
|                                                Mean Throughput | index-append | 87091.9         | docs/s |
|                                              Median Throughput | index-append | 87152.4         | docs/s |
|                                                 Max Throughput | index-append | 87276.3         | docs/s |
|                                        50th percentile latency | index-append |   315.247       |     ms |
|                                        90th percentile latency | index-append |   573.935       |     ms |
|                                        99th percentile latency | index-append |  1139.89        |     ms |
|                                       100th percentile latency | index-append |  1153.22        |     ms |
|                                   50th percentile service time | index-append |   315.247       |     ms |
|                                   90th percentile service time | index-append |   573.935       |     ms |
|                                   99th percentile service time | index-append |  1139.89        |     ms |
|                                  100th percentile service time | index-append |  1153.22        |     ms |
|                                                     error rate | index-append |     0           |      % |


---------------------------------
[INFO] SUCCESS (took 255 seconds)
---------------------------------
</code></pre>
<p>Stop the Elasticsearch node</p>
<pre><code class="language-shell">$ esrally stop --installation-id=&quot;${INSTALLATION_ID}&quot;
</code></pre>
<p>If you only want to shutdown the node but don&#x2019;t want to delete the node and the data, pass --preserve-install additionally.</p>
<h2 id="benchmarking-a-cluster">Benchmarking a Cluster</h2>
<p>Install and start Elasticsearch on each cluster node</p>
<pre><code class="language-shell">$ esrally install --distribution-version=7.17.0 --node-name=&quot;rally-node-0&quot; --network-host=&quot;10.10.10.2&quot; --http-port=39200 --master-nodes=&quot;rally-node-0,rally-node-1,rally-node-2&quot; --seed-hosts=&quot;10.10.10.2:39300,10.10.10.3:39300,10.10.10.4:39300&quot;
[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)  [100%]
{
  &quot;installation-id&quot;: &quot;aa826112-d371-4f09-9b68-f9084e7c9e0b&quot;
}
</code></pre>
<p>Generate a race id on one of the nodes</p>
<pre><code class="language-shell">$ uuidgen
734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
</code></pre>
<p><strong>Note</strong>: The same race id is set on all the nodes including the one where will generate load.</p>
<p>Start the cluster by running the following command on each node</p>
<pre><code class="language-shell">$ export INSTALLATION_ID=aa826112-d371-4f09-9b68-f9084e7c9e0b
$ export RACE_ID=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
$ esrally start --installation-id=&quot;${INSTALLATION_ID}&quot; --race-id=&quot;${RACE_ID}&quot;
</code></pre>
<p><strong>Note</strong>: The INSTALLATION_ID is specific to each node and the RACI_ID is identical for all the nodes.</p>
<p>Once the cluster is started, check the cluster status with the _cat/health API</p>
<pre><code class="language-shell">[es@node1 ~]$ curl http://10.10.10.2:39200/_cat/health\?v
epoch      timestamp cluster         status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1667015754 03:55:54  rally-benchmark green           3         3      6   3    0    0        0             0                  -                100.0%
</code></pre>
<p>On each cluster node, check the elastic process and port</p>
<pre><code class="language-shell">$ ps -ef | egrep -i &quot;rally|elastic&quot; | grep -v grep
es        2258     1 91 20:52 ?        00:58:39 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,JRE -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-1322059221495755520 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/heapdump -XX:ErrorFile=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/hs_err_pid%p.log -XX:+ExitOnOutOfMemoryError -XX:MaxDirectMemorySize=536870912 -Des.path.home=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0 -Des.path.conf=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/lib/* org.elasticsearch.bootstrap.Elasticsearch -d -p ./pid
es        2285  2258  0 20:52 ?        00:00:00 /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/modules/x-pack-ml/platform/linux-x86_64/bin/controller

$ netstat -anop | grep 39200
tcp6       0      0 10.10.10.2:39200       :::*                    LISTEN      2258/java            off (0.00/0/0)
</code></pre>
<p>Start the benchmark on the load generator node (remember to set the race id there)</p>
<pre><code class="language-shell">[es@node1 ~]$ export RACE_ID=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
[es@node1 ~]$ esrally race --pipeline=benchmark-only --target-host=10.10.10.2:39200,10.10.10.3:39200,10.10.10.4:39200 --track=geonames --challenge=append-no-conflicts --on-error=abort --race-id=${RACE_ID}
    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&apos;external&apos;] with version [7.17.0].

[WARNING] merges_total_time is 420149 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] merges_total_throttled_time is 81765 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] indexing_total_time is 825388 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] refresh_total_time is 76340 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] flush_total_time is 10787 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |          Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|---------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |   13.3055      |     min |
|             Min cumulative indexing time across primary shards |                                |    0           |     min |
|          Median cumulative indexing time across primary shards |                                |    2.68572     |     min |
|             Max cumulative indexing time across primary shards |                                |    2.74885     |     min |
|            Cumulative indexing throttle time of primary shards |                                |    0           |     min |
|    Min cumulative indexing throttle time across primary shards |                                |    0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |    0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |    0           |     min |
|                        Cumulative merge time of primary shards |                                |    4.82182     |     min |
|                       Cumulative merge count of primary shards |                                |   57           |         |
|                Min cumulative merge time across primary shards |                                |    0           |     min |
|             Median cumulative merge time across primary shards |                                |    0.984917    |     min |
|                Max cumulative merge time across primary shards |                                |    1.06472     |     min |
|               Cumulative merge throttle time of primary shards |                                |    0.978367    |     min |
|       Min cumulative merge throttle time across primary shards |                                |    0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |    0.195508    |     min |
|       Max cumulative merge throttle time across primary shards |                                |    0.265933    |     min |
|                      Cumulative refresh time of primary shards |                                |    1.20573     |     min |
|                     Cumulative refresh count of primary shards |                                |  148           |         |
|              Min cumulative refresh time across primary shards |                                |    3.33333e-05 |     min |
|           Median cumulative refresh time across primary shards |                                |    0.258775    |     min |
|              Max cumulative refresh time across primary shards |                                |    0.283433    |     min |
|                        Cumulative flush time of primary shards |                                |    0.172783    |     min |
|                       Cumulative flush count of primary shards |                                |   11           |         |
|                Min cumulative flush time across primary shards |                                |    1.66667e-05 |     min |
|             Median cumulative flush time across primary shards |                                |    0.0345      |     min |
|                Max cumulative flush time across primary shards |                                |    0.0385167   |     min |
|                                        Total Young Gen GC time |                                |   16.263       |       s |
|                                       Total Young Gen GC count |                                | 2821           |         |
|                                          Total Old Gen GC time |                                |    2.312       |       s |
|                                         Total Old Gen GC count |                                |   41           |         |
|                                                     Store size |                                |    3.03867     |      GB |
|                                                  Translog size |                                |    3.58559e-07 |      GB |
|                                         Heap used for segments |                                |    0.701981    |      MB |
|                                       Heap used for doc values |                                |    0.0314178   |      MB |
|                                            Heap used for terms |                                |    0.54541     |      MB |
|                                            Heap used for norms |                                |    0.0736694   |      MB |
|                                           Heap used for points |                                |    0           |      MB |
|                                    Heap used for stored fields |                                |    0.0514832   |      MB |
|                                                  Segment count |                                |  102           |         |
|                                    Total Ingest Pipeline count |                                |    0           |         |
|                                     Total Ingest Pipeline time |                                |    0           |       s |
|                                   Total Ingest Pipeline failed |                                |    0           |         |
|                                                     error rate |                   index-append |    0           |       % |
|                                                 Min Throughput |                    index-stats |   90.01        |   ops/s |
|                                                Mean Throughput |                    index-stats |   90.02        |   ops/s |
|                                              Median Throughput |                    index-stats |   90.02        |   ops/s |
|                                                 Max Throughput |                    index-stats |   90.04        |   ops/s |
|                                        50th percentile latency |                    index-stats |    5.16153     |      ms |
|                                        90th percentile latency |                    index-stats |    6.00114     |      ms |
|                                        99th percentile latency |                    index-stats |    6.61081     |      ms |
|                                      99.9th percentile latency |                    index-stats |   10.4064      |      ms |
|                                       100th percentile latency |                    index-stats |   10.8105      |      ms |
|                                   50th percentile service time |                    index-stats |    4.00402     |      ms |
|                                   90th percentile service time |                    index-stats |    4.6339      |      ms |
|                                   99th percentile service time |                    index-stats |    5.10083     |      ms |
|                                 99.9th percentile service time |                    index-stats |    9.17415     |      ms |
|                                  100th percentile service time |                    index-stats |    9.22474     |      ms |
[..]

[WARNING] No throughput metrics available for [index-append]. Likely cause: The benchmark ended already during warmup.

----------------------------------
[INFO] SUCCESS (took 4008 seconds)
----------------------------------
</code></pre>
<p>Shutdown the cluster on each node</p>
<pre><code class="language-shell">$ esrally stop --installation-id=&quot;${INSTALLATION_ID}&quot;
</code></pre>
<p><strong>Note</strong>: If you only want to shutdown the node but don&#x2019;t want to delete the node and the data, add the option &quot;--preserve-install&quot; additionally.</p>
<h2 id="troubleshooting">Troubleshooting</h2>
<ul>
<li>
<p>Elasticsearch start failure due to max virtual memory is too low</p>
<pre><code class="language-shell">$ cat /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/rally-benchmark.log
[..]
[2022-10-28T16:58:41,041][ERROR][o.e.b.Bootstrap          ] [rally-node-0] node validation exception
[1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.
bootstrap check failure [1] of [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
[..]

$ sysctl -a | grep max_map_count
vm.max_map_count = 65530
</code></pre>
<p>To fix this issue, change the kernel parameter</p>
<pre><code class="language-shell">$ vim /etc/sysctl.conf
vm.max_map_count=1048576
$ sysctl -p
vm.max_map_count = 1048576
</code></pre>
<p>Restart Elasticsearch and verify the process and port</p>
<pre><code class="language-shell">$ esrally stop --installation-id=&quot;${INSTALLATION_ID}&quot; --preserve-install
$ esrally start --installation-id=&quot;${INSTALLATION_ID}&quot; --race-id=&quot;${RACE_ID}&quot;

$ netstat -anop | grep 39200
tcp6       0      0 10.10.10.2:39200       :::*                    LISTEN      23726/java           off (0.00/0/0)

$ ps -ef | egrep -i &quot;rally|elastic&quot; | grep -v grep
es       23726     1 18 18:01 ?        00:00:42 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,JRE -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-7969870787666953814 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/heapdump -XX:ErrorFile=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/hs_err_pid%p.log -XX:+ExitOnOutOfMemoryError -XX:MaxDirectMemorySize=536870912 -Des.path.home=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0 -Des.path.conf=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/lib/* org.elasticsearch.bootstrap.Elasticsearch -d -p ./pid
es       23752 23726  0 18:01 ?        00:00:00 /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/modules/x-pack-ml/platform/linux-x86_64/bin/controller
</code></pre>
</li>
<li>
<p>Elasticsearch start failure due to max file descriptors is too low</p>
<pre><code class="language-shell">$ cat /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/rally-benchmark.log
[..]
[2022-10-28T18:02:09,107][ERROR][o.e.b.Bootstrap          ] [rally-node-1] node validation exception
[1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.
bootstrap check failure [1] of [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]
[..]
</code></pre>
<p>To fix this issue, change the max open files value in /etc/security/limits.conf</p>
<pre><code class="language-shell">$ vim /etc/security/limits.conf
*              soft     nofile          1048576
*              hard     nofile          1048576
</code></pre>
<p>Exit and login back to the shell to see the changed value</p>
<pre><code class="language-shell">ulimit -a  | grep &quot;open files&quot;
open files                      (-n) 1048576
</code></pre>
</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://esrally.readthedocs.io/en/stable/">Rally User Guide</a></li>
<li><a href="https://elasticsearch-benchmarks.elastic.co/#">https://elasticsearch-benchmarks.elastic.co/#</a></li>
<li><a href="https://github.com/elastic/rally-eventdata-track">https://github.com/elastic/rally-eventdata-track</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/configuration.html#reporting">https://esrally.readthedocs.io/en/stable/configuration.html#reporting</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Getting started with Elastic Rally benchmark]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Rally is the macrobenchmarking framework for Elasticsearch. This post follows the instructions <a href="https://esrally.readthedocs.io/en/stable/install.html">here</a> to install Rally and run the very first benchmark(aka race).</p>
<h3 id="install-python">Install python</h3>
<p>Python 3.8+ including pip3 is required for Rally.</p>
<pre><code class="language-shell">$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;

$ curl -O</code></pre>]]></description><link>https://relentlesstorm.github.io/getting-started-with-esrally/</link><guid isPermaLink="false">63afc8ed96dbf412f1f803b5</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 25 Oct 2022 05:30:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1496065187959-7f07b8353c55?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDd8fHRlY2hub2xvZ3l8ZW58MHx8fHwxNjcyNDY0Njk1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1496065187959-7f07b8353c55?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDd8fHRlY2hub2xvZ3l8ZW58MHx8fHwxNjcyNDY0Njk1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Getting started with Elastic Rally benchmark"><p>Rally is the macrobenchmarking framework for Elasticsearch. This post follows the instructions <a href="https://esrally.readthedocs.io/en/stable/install.html">here</a> to install Rally and run the very first benchmark(aka race).</p>
<h3 id="install-python">Install python</h3>
<p>Python 3.8+ including pip3 is required for Rally.</p>
<pre><code class="language-shell">$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;

$ curl -O https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz
$ tar zxf Python-3.8.1.tgz
$ mv Python-3.8.1 /usr/src
$ cd /usr/src/Python-3.8.1/
$ vim Modules/Setup 
SSL=/usr/local/ssl
_ssl _ssl.c \
        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        -L$(SSL)/lib -lssl -lcrypto

$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl

$ pip3 -V
pip 22.3 from /usr/local/lib/python3.8/site-packages/pip (python 3.8)
</code></pre>
<p><strong>Note</strong>: If you do not uncomment the 4 lines in Modules/Setup, you would fail to install Rally with the ssl module unavailable issue as mentioned in the following troubleshooting section.</p>
<h3 id="install-git">Install git</h3>
<p>Git is not required if all of the following conditions are met:</p>
<ul>
<li>You are using Rally only as a load generator (--pipeline=benchmark-only) or you are referring to Elasticsearch configurations with --team-path.</li>
<li>You create your own tracks and refer to them with --track-path.</li>
<li>In all other cases, Rally requires git 1.9 or better. Verify with git --version</li>
</ul>
<pre><code class="language-shell">$ yum -y remove git
$ yum -y remove git-*
$ yum install git
$ git version
git version 2.38.1
</code></pre>
<h2 id="install-jdk">Install JDK</h2>
<p>A JDK is required on all machines where you want to launch Elasticsearch. If you use Rally just as a load generator to benchmark remote clusters, no JDK is required. Refer to <a href="https://www.elastic.co/support/matrix#matrix_jvm">here</a> to determine the appropriate JDK version to run Elasticsearch.</p>
<pre><code class="language-shell">$ yum install java
$ java -version
openjdk version &quot;1.8.0_345&quot;
OpenJDK Runtime Environment (build 1.8.0_345-b01)
OpenJDK 64-Bit Server VM (build 25.345-b01, mixed mode)
</code></pre>
<p>To <a href="https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html">download</a> and install a specific java version:</p>
<pre><code class="language-shell">$ wget https://download.oracle.com/java/17/archive/jdk-17.0.5_linux-x64_bin.rpm
$ rpm -ivh jdk-17.0.5_linux-x64_bin.rpm
$ java -version
java version &quot;17.0.5&quot; 2022-10-18 LTS
Java(TM) SE Runtime Environment (build 17.0.5+9-LTS-191)
Java HotSpot(TM) 64-Bit Server VM (build 17.0.5+9-LTS-191, mixed mode, sharing)

$ rpm -qi jdk-17-17.0.5-ga.x86_64
Name        : jdk-17
Epoch       : 2000
Version     : 17.0.5
Release     : ga
Architecture: x86_64
Install Date: Wed 02 Nov 2022 03:58:42 PM PDT
Group       : Development/Tools
Size        : 316751437
License     : https://java.com/freeuselicense
Signature   : RSA/SHA256, Tue 13 Sep 2022 09:36:17 AM PDT, Key ID 72f97b74ec551f03
Source RPM  : jdk-17-17.0.5-ga.src.rpm
Build Date  : Tue 13 Sep 2022 09:35:27 AM PDT
Build Host  : java.com
Relocations : /usr/java
Vendor      : Oracle Corporation
URL         : http://www.oracle.com/technetwork/java/javase/overview/index.html
Summary     : Java Platform Standard Edition Development Kit
Description :
The Java Platform Standard Edition Development Kit (JDK) includes both
the runtime environment (Java virtual machine, the Java platform classes
and supporting files) and development tools (compilers, debuggers,
tool libraries and other tools).
</code></pre>
<p>With java 1.8.0, you can run Elasticsearch 7.17.x or lower version.</p>
<p>To find the JDK, Rally expects the environment variable JAVA_HOME to be set on all targeted machines. To have more specific control, for example when you want to benchmark across a wide range of Elasticsearch releases, you can also set JAVAx_HOME where x is the major version of a JDK (e.g. JAVA8_HOME would point to a JDK 8 installation). Rally will then choose the highest supported JDK per version of Elasticsearch that is available.</p>
<pre><code class="language-shell">$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
$ echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
</code></pre>
<h2 id="install-rally">Install Rally</h2>
<pre><code class="language-shell">$ pip3.8 install --upgrade pip

$ pip3.8 install esrally

$ esrally -h
usage: esrally [-h] [--version] {race,list,info,create-track,generate,compare,download,install,start,stop} ...

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

 You Know, for Benchmarking Elasticsearch.

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit

subcommands:
  {race,list,info,create-track,generate,compare,download,install,start,stop}
    race                Run a benchmark
    list                List configuration options
    info                Show info about a track
    create-track        Create a Rally track from existing data
    generate            Generate artifacts
    compare             Compare two races
    download            Downloads an artifact
    install             Installs an Elasticsearch node locally
    start               Starts an Elasticsearch node locally
    stop                Stops an Elasticsearch node locally

Find out more about Rally at https://esrally.readthedocs.io/en/2.6.0/
</code></pre>
<pre><code class="language-shell">$ esrally list tracks

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available tracks:

Name              Description                                                              Documents    Compressed Size    Uncompressed Size    Default Challenge        All Challenges
----------------  -----------------------------------------------------------------------  -----------  -----------------  -------------------  -----------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
dense_vector      Benchmark for dense vector indexing and search                           10,000,000   7.2 GB             19.5 GB              index-and-search         index-and-search
elastic/endpoint  Endpoint track                                                           0            0 bytes            0 bytes              default                  default
elastic/logs      Track for simulating logging workloads                                   14,009,078   N/A                N/A                  logging-indexing         cross-clusters-search,logging-disk-usage,logging-indexing-querying,logging-indexing,logging-querying,logging-snapshot-mount,logging-snapshot-restore,logging-snapshot,many-shards-quantitative,many-shards-snapshots
elastic/security  Track for simulating Elastic Security workloads                          77,513,777   N/A                N/A                  security-querying        index-alert-source-events,security-indexing-querying,security-indexing,security-querying
eql               EQL benchmarks based on endgame index of SIEM demo cluster               60,782,211   4.5 GB             109.2 GB             default                  default,index-sorting
geonames          POIs from Geonames                                                       11,396,503   252.9 MB           3.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts,significant-text
geopoint          Point coordinates from PlanetOSM                                         60,844,404   482.1 MB           2.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geopointshape     Point coordinates from PlanetOSM indexed as geoshapes                    60,844,404   470.8 MB           2.6 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geoshape          Shapes from PlanetOSM                                                    84,220,567   17.0 GB            58.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-big
http_logs         HTTP server log data                                                     247,249,096  1.2 GB             31.1 GB              append-no-conflicts      append-no-conflicts,runtime-fields,append-no-conflicts-index-only,append-sorted-no-conflicts,append-index-only-with-ingest-pipeline,update,append-no-conflicts-index-reindex-only
metricbeat        Metricbeat data                                                          1,079,600    87.7 MB            1.2 GB               append-no-conflicts      append-no-conflicts
nested            StackOverflow Q&amp;A stored as nested docs                                  11,203,029   663.3 MB           3.4 GB               nested-search-challenge  nested-search-challenge,index-only
noaa              Global daily weather measurements from NOAA                              33,659,481   949.4 MB           9.0 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,aggs,filter-aggs
nyc_taxis         Taxi rides in New York in 2015                                           165,346,692  4.5 GB             74.3 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts-index-only,update,append-ml,aggs
percolator        Percolator benchmark based on AOL queries                                2,000,000    121.1 kB           104.9 MB             append-no-conflicts      append-no-conflicts
pmc               Full text benchmark with academic papers from PMC                        574,199      5.5 GB             21.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts,indexing-querying
so                Indexing benchmark using up to questions and answers from StackOverflow  36,062,278   8.9 GB             33.1 GB              append-no-conflicts      append-no-conflicts,transform,frequent-items
so_vector         Benchmark for vector search with StackOverflow data                      2,000,000    12.3 GB            32.2 GB              index-and-search         index-and-search
sql               SQL query performance based on NOAA Weather data                         33,659,481   949.4 MB           9.0 GB               sql                      sql
tsdb              metricbeat information for elastic-app k8s cluster                       116,633,698  N/A                123.0 GB             append-no-conflicts      append-no-conflicts,downsample

-------------------------------
[INFO] SUCCESS (took 3 seconds)
-------------------------------
</code></pre>
<pre><code class="language-shell">$ esrally list cars

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available cars:

Name                     Type    Description
-----------------------  ------  --------------------------------------
16gheap                  car     Sets the Java heap to 16GB
1gheap                   car     Sets the Java heap to 1GB
24gheap                  car     Sets the Java heap to 24GB
2gheap                   car     Sets the Java heap to 2GB
4gheap                   car     Sets the Java heap to 4GB
8gheap                   car     Sets the Java heap to 8GB
defaults                 car     Sets the Java heap to 1GB
basic-license            mixin   Basic License
debug-non-safepoints     mixin   More accurate CPU profiles
ea                       mixin   Enables Java assertions
fp                       mixin   Preserves frame pointers
g1gc                     mixin   Enables the G1 garbage collector
parallelgc               mixin   Enables the Parallel garbage collector
trial-license            mixin   Trial License
unpooled                 mixin   Enables Netty&apos;s unpooled allocator
x-pack-ml                mixin   X-Pack Machine Learning
x-pack-monitoring-http   mixin   X-Pack Monitoring (HTTP exporter)
x-pack-monitoring-local  mixin   X-Pack Monitoring (local exporter)
x-pack-security          mixin   X-Pack Security
zgc                      mixin   Enables the ZGC garbage collector

-------------------------------
[INFO] SUCCESS (took 6 seconds)
-------------------------------
</code></pre>
<h2 id="run-the-first-race-with-rally">Run the first race with Rally</h2>
<p>A &#x201C;race&#x201D; in Rally is the execution of a benchmarking experiment. You can choose different benchmarking scenarios (called tracks) for your benchmarks.</p>
<p>Rally should be run as a non-root user. We create a user &quot;es&quot; to run the following race.</p>
<pre><code class="language-shell">$ groupadd es
$ useradd es -g es
$ passwd es
$ cd /home/es
</code></pre>
<pre><code class="language-shell">$ su - es
$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
$ esrally race --distribution-version=7.17.0 --track=geonames
    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [d3102b91-ac10-4383-b6a4-7b98d2831af7]
[INFO] Preparing for race ...
[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)                       [100%]
[INFO] Downloading track data (252.9 MB total size)                               [100.0%]
[INFO] Decompressing track data from [/home/es/.rally/benchmarks/data/geonames/documents-2.json.bz2] to [/home/es/.rally/benchmarks/data/geonames/documents-2.json] (resulting size: [3.30] GB) ...  [OK]
[INFO] Preparing file offset table for [/home/es/.rally/benchmarks/data/geonames/documents-2.json] ... [OK]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&apos;defaults&apos;] with version [7.17.0].

Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |           Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|----------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |    12.7087      |     min |
|             Min cumulative indexing time across primary shards |                                |     0.0196167   |     min |
|          Median cumulative indexing time across primary shards |                                |     2.5376      |     min |
|             Max cumulative indexing time across primary shards |                                |     2.58153     |     min |
|            Cumulative indexing throttle time of primary shards |                                |     0.0108667   |     min |
|    Min cumulative indexing throttle time across primary shards |                                |     0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |     0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |     0.00731667  |     min |
|                        Cumulative merge time of primary shards |                                |     7.13778     |     min |
|                       Cumulative merge count of primary shards |                                |    52           |         |
|                Min cumulative merge time across primary shards |                                |     0           |     min |
|             Median cumulative merge time across primary shards |                                |     1.20084     |     min |
|                Max cumulative merge time across primary shards |                                |     2.20237     |     min |
|               Cumulative merge throttle time of primary shards |                                |     1.59453     |     min |
|       Min cumulative merge throttle time across primary shards |                                |     0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |     0.2181      |     min |
|       Max cumulative merge throttle time across primary shards |                                |     0.569333    |     min |
|                      Cumulative refresh time of primary shards |                                |     2.89785     |     min |
|                     Cumulative refresh count of primary shards |                                |   256           |         |
|              Min cumulative refresh time across primary shards |                                |     0.00186667  |     min |
|           Median cumulative refresh time across primary shards |                                |     0.577325    |     min |
|              Max cumulative refresh time across primary shards |                                |     0.599333    |     min |
|                        Cumulative flush time of primary shards |                                |     0.2239      |     min |
|                       Cumulative flush count of primary shards |                                |    14           |         |
|                Min cumulative flush time across primary shards |                                |     0.0023      |     min |
|             Median cumulative flush time across primary shards |                                |     0.0471833   |     min |
|                Max cumulative flush time across primary shards |                                |     0.05055     |     min |
|                                        Total Young Gen GC time |                                |    18.57        |       s |
|                                       Total Young Gen GC count |                                |  2243           |         |
|                                          Total Old Gen GC time |                                |     3.541       |       s |
|                                         Total Old Gen GC count |                                |    66           |         |
|                                                     Store size |                                |     2.82211     |      GB |
|                                                  Translog size |                                |     3.07336e-07 |      GB |
|                                         Heap used for segments |                                |     0.733753    |      MB |
|                                       Heap used for doc values |                                |     0.0489769   |      MB |
|                                            Heap used for terms |                                |     0.557007    |      MB |
|                                            Heap used for norms |                                |     0.0753784   |      MB |
|                                           Heap used for points |                                |     0           |      MB |
|                                    Heap used for stored fields |                                |     0.0523911   |      MB |
|                                                  Segment count |                                |   103           |         |
|                                    Total Ingest Pipeline count |                                |     0           |         |
|                                     Total Ingest Pipeline time |                                |     0           |       s |
|                                   Total Ingest Pipeline failed |                                |     0           |         |
|                                                 Min Throughput |                   index-append | 87315.7         |  docs/s |
|                                                Mean Throughput |                   index-append | 87373.7         |  docs/s |
|                                              Median Throughput |                   index-append | 87368.1         |  docs/s |
|                                                 Max Throughput |                   index-append | 87440.9         |  docs/s |
|                                        50th percentile latency |                   index-append |   316.067       |      ms |
|                                        90th percentile latency |                   index-append |   458.448       |      ms |
|                                        99th percentile latency |                   index-append |  1152.53        |      ms |
|                                       100th percentile latency |                   index-append |  1316.07        |      ms |
|                                   50th percentile service time |                   index-append |   316.067       |      ms |
|                                   90th percentile service time |                   index-append |   458.448       |      ms |
|                                   99th percentile service time |                   index-append |  1152.53        |      ms |
|                                  100th percentile service time |                   index-append |  1316.07        |      ms |
|                                                     error rate |                   index-append |     0           |       % |
|                                                 Min Throughput |                    index-stats |    89.91        |   ops/s |
|                                                Mean Throughput |                    index-stats |    89.95        |   ops/s |
|                                              Median Throughput |                    index-stats |    89.95        |   ops/s |
|                                                 Max Throughput |                    index-stats |    89.97        |   ops/s |
|                                        50th percentile latency |                    index-stats |     4.36948     |      ms |
|                                        90th percentile latency |                    index-stats |     5.06188     |      ms |
|                                        99th percentile latency |                    index-stats |     5.51726     |      ms |
|                                      99.9th percentile latency |                    index-stats |     7.79772     |      ms |
|                                       100th percentile latency |                    index-stats |     9.64821     |      ms |
|                                   50th percentile service time |                    index-stats |     3.16338     |      ms |
|                                   90th percentile service time |                    index-stats |     3.67796     |      ms |
|                                   99th percentile service time |                    index-stats |     3.86689     |      ms |
|                                 99.9th percentile service time |                    index-stats |     4.13559     |      ms |
|                                  100th percentile service time |                    index-stats |     6.9374      |      ms |
[..]                                                 

----------------------------------
[INFO] SUCCESS (took 4199 seconds)
----------------------------------
</code></pre>
<p>You can save this report also to a file by using --report-file=/path/to/your/report.md and save it as CSV with --report-format=csv.</p>
<p>What did Rally just do?</p>
<ul>
<li>It downloaded and started Elasticsearch 7.17.0</li>
<li>It downloaded the relevant data for the geonames track</li>
<li>It ran the actual benchmark</li>
<li>And finally it reported the results</li>
</ul>
<h2 id="rally-configuration">Rally Configuration</h2>
<p>Rally stores its configuration in the file .rally/rally.ini which is automatically created the first time Rally is executed.</p>
<pre><code class="language-shell">$ pwd
/home/es/.rally/benchmarks
$ ls
data  distributions  races  teams  tracks
$ ls distributions/
elasticsearch-7.17.0-linux-x86_64.tar.gz

$ ls tracks/default/
download.sh  elastic  eql  geonames  geopoint  geopointshape  geoshape  http_logs  metricbeat  nested  noaa  nyc_taxis  percolator  pmc  README.md  so  sql

$ ls races/d3102b91-ac10-4383-b6a4-7b98d2831af7/
race.json  rally-node-0

$ ls teams/default/
cars  LICENSE  NOTICE  plugins  README.md

$ ls -l data/geonames/
total 3723472
-rw-rw-r-- 1 es es 3547613828 Oct 27 17:01 documents-2.json
-rw-rw-r-- 1 es es  265208777 Oct 27 17:00 documents-2.json.bz2
-rw-rw-r-- 1 es es       4250 Oct 27 17:02 documents-2.json.offset
</code></pre>
<pre><code class="language-shell">$ cat /home/es/.rally/rally.ini
[meta]
config.version = 17

[system]
env.name = local

[node]
root.dir = /home/es/.rally/benchmarks
src.root.dir = /home/es/.rally/benchmarks/src

[source]
remote.repo.url = https://github.com/elastic/elasticsearch.git
elasticsearch.src.subdir = elasticsearch

[benchmarks]
local.dataset.cache = /home/es/.rally/benchmarks/data

[reporting]
datastore.type = in-memory
datastore.host =
datastore.port =
datastore.secure = False
datastore.user =
datastore.password =


[tracks]
default.url = https://github.com/elastic/rally-tracks

[teams]
default.url = https://github.com/elastic/rally-teams

[defaults]
preserve_benchmark_candidate = false

[distributions]
release.cache = true
</code></pre>
<p>The benchmark data directory can be changed by modifying root.dir in rally.ini.</p>
<ul>
<li>root.dir (default: &#x201C;~/.rally/benchmarks&#x201D;): Rally uses this directory to store all benchmark-related data. It assumes that it has complete control over this directory and any of its subdirectories.</li>
<li>src.root.dir (default: &#x201C;~/.rally/benchmarks/src&#x201D;): The directory where the source code of Elasticsearch or any plugins is checked out. Only relevant for benchmarks from sources.</li>
</ul>
<h2 id="uninstall-python3">Uninstall python3</h2>
<p>The following are the optional commands in the case you need to uninstall python3 in CentOS.</p>
<pre><code class="language-shell">$ whereis python3
python3: /usr/bin/python3 /usr/bin/python3.6 /usr/bin/python3.6m /usr/lib/python3.6 /usr/lib64/python3.6 /usr/local/bin/python3.11 /usr/local/bin/python3.11-config /usr/local/lib/python3.11 /usr/include/python3.6m /usr/share/man/man1/python3.1.gz

$ whereis pip3
pip3: /usr/local/bin/pip3 /usr/local/bin/pip3.8 /usr/local/bin/pip3.10

$ rpm -qa | grep python3 --&gt; Only needed if you installed python3 by yum package installer.

$ whereis python3 |xargs rm -frv
$ whereis pip3 |xargs rm -frv
</code></pre>
<h2 id="troubleshooting">Troubleshooting</h2>
<ul>
<li>
<p>&quot;WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.&quot;</p>
<pre><code class="language-shell">$ pip3.8 install esrally
WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by &apos;SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)&apos;: /simple/esrally/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by &apos;SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)&apos;: /simple/esrally/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by &apos;SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)&apos;: /simple/esrally/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by &apos;SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)&apos;: /simple/esrally/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by &apos;SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)&apos;: /simple/esrally/
Could not fetch URL https://pypi.org/simple/esrally/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host=&apos;pypi.org&apos;, port=443): Max retries exceeded with url: /simple/esrally/ (Caused by SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)) - skipping
ERROR: Could not find a version that satisfies the requirement esrally (from versions: none)
ERROR: No matching distribution found for esrally
WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.
Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host=&apos;pypi.org&apos;, port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)) - skipping
WARNING: There was an error checking the latest version of pip.
</code></pre>
<p>To fix this, install the python after un-commenting the four lines in Modules/Setup.</p>
<pre><code class="language-shell">$ curl -O https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz
$ tar zxf Python-3.8.1.tgz
$ mv Python-3.8.1 /usr/src
$ cd /usr/src/Python-3.8.1/
$ vim Modules/Setup 
SSL=/usr/local/ssl
_ssl _ssl.c \
    -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
    -L$(SSL)/lib -lssl -lcrypto

$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl
</code></pre>
</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.elastic.co/blog/announcing-rally-benchmarking-for-elasticsearch?spm=a2c65.11461447.0.0.e26a498cbHyowi">https://www.elastic.co/blog/announcing-rally-benchmarking-for-elasticsearch?spm=a2c65.11461447.0.0.e26a498cbHyowi</a></li>
<li><a href="https://github.com/elastic/rally">https://github.com/elastic/rally</a></li>
<li><a href="https://github.com/elastic/rally-tracks">https://github.com/elastic/rally-tracks</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/quickstart.html">https://esrally.readthedocs.io/en/stable/quickstart.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/install.html">https://esrally.readthedocs.io/en/stable/install.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/configuration.html">https://esrally.readthedocs.io/en/stable/configuration.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/race.html">https://esrally.readthedocs.io/en/stable/race.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/faq.html">https://esrally.readthedocs.io/en/stable/faq.html</a></li>
<li><a href="https://www.python.org/ftp/python/">https://www.python.org/ftp/python/</a></li>
<li><a href="https://www.elastic.co/support/matrix#matrix_jvm">https://www.elastic.co/support/matrix#matrix_jvm</a></li>
<li><a href="https://stackoverflow.com/questions/56552390/how-to-fix-ssl-module-in-python-is-not-available-in-centos">https://stackoverflow.com/questions/56552390/how-to-fix-ssl-module-in-python-is-not-available-in-centos</a></li>
<li><a href="https://blog.searchhub.io/how-to-setup-elasticsearch-benchmarking">https://blog.searchhub.io/how-to-setup-elasticsearch-benchmarking</a></li>
<li><a href="https://techviewleo.com/install-java-openjdk-on-rocky-linux-centos/">https://techviewleo.com/install-java-openjdk-on-rocky-linux-centos/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Overwrite the Elastic Rally track parameters]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Each track allows to overwrite the corresponding parameters using --track-params.</p>
<p>For example, the track <a href="https://github.com/elastic/rally-tracks/tree/master/geonames">geonames</a> allows to overwrite the following parameters.</p>
<ul>
<li>bulk_size (default: 5000)</li>
<li>bulk_indexing_clients (default: 8): Number of clients that issue bulk indexing requests.</li>
<li>ingest_percentage (default: 100): A number between 0 and 100 that defines</li></ul>]]></description><link>https://relentlesstorm.github.io/overwrite-esrally-track-params/</link><guid isPermaLink="false">63afc85696dbf412f1f803a4</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 24 Oct 2022 05:28:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1591683583663-a5996be1a012?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJlbmNobWFya3xlbnwwfHx8fDE2NzI0NjI4MjE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1591683583663-a5996be1a012?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJlbmNobWFya3xlbnwwfHx8fDE2NzI0NjI4MjE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Overwrite the Elastic Rally track parameters"><p>Each track allows to overwrite the corresponding parameters using --track-params.</p>
<p>For example, the track <a href="https://github.com/elastic/rally-tracks/tree/master/geonames">geonames</a> allows to overwrite the following parameters.</p>
<ul>
<li>bulk_size (default: 5000)</li>
<li>bulk_indexing_clients (default: 8): Number of clients that issue bulk indexing requests.</li>
<li>ingest_percentage (default: 100): A number between 0 and 100 that defines how much of the document corpus should be ingested.</li>
<li>conflicts (default: &quot;random&quot;): Type of id conflicts to simulate. Valid values are: &apos;sequential&apos; (A document id is replaced with a document id with a sequentially increasing id), &apos;random&apos; (A document id is replaced with a document id with a random other id).</li>
<li>conflict_probability (default: 25): A number between 0 and 100 that defines the probability of id conflicts. This requires to run the respective challenge. Combining conflicts=sequential and conflict-probability=0 makes Rally generate index ids by itself, instead of relying on Elasticsearch&apos;s automatic id generation <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_automatic_id_generation">https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_automatic_id_generation</a>_.</li>
<li>on_conflict (default: &quot;index&quot;): Whether to use an &quot;index&quot; or an &quot;update&quot; action when simulating an id conflict.</li>
<li>recency (default: 0): A number between 0 and 1 that defines whether to bias towards more recent ids when simulating conflicts. See the Rally docs for the full definition of this parameter. This requires to run the respective challenge.</li>
<li>number_of_replicas (default: 0)</li>
<li>number_of_shards (default: 5)</li>
<li>max_num_segments: The maximum number of segments to force-merge to.</li>
<li>source_enabled (default: true): A boolean defining whether the _source field is stored in the index.</li>
<li>index_settings: A list of index settings. Index settings defined elsewhere (e.g. number_of_replicas) need to be overridden explicitly.</li>
<li>cluster_health (default: &quot;green&quot;): The minimum required cluster health.</li>
<li>error_level (default: &quot;non-fatal&quot;): Available for bulk operations only to specify ignore-response-error-level.</li>
</ul>
<p>The default track parameters can be verified in the index.json.</p>
<pre><code class="language-shell">$ cat .rally/benchmarks/tracks/default/geonames/index.json
{
  &quot;settings&quot;: {
    &quot;index.number_of_shards&quot;: {{number_of_shards | default(5)}},
    &quot;index.number_of_replicas&quot;: {{number_of_replicas | default(0)}},
    &quot;index.store.type&quot;: &quot;{{store_type | default(&apos;fs&apos;)}}&quot;,
    &quot;index.requests.cache.enable&quot;: false
  },
  [..]
}
</code></pre>
<p>You can change the track parameters when you run the race as below.</p>
<pre><code class="language-shell">$ esrally race --pipeline=benchmark-only --target-host=10.10.10.1:39200,10.10.10.2:39200,10.10.10.3:39200 --track=geonames --track-params=&quot;number_of_shards:3,number_of_replicas:1&quot; --challenge=append-no-conflicts --on-error=abort --race-id=${RACE_ID}

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&apos;external&apos;] with version [7.17.0].

[WARNING] indexing_total_time is 19846 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] refresh_total_time is 5970 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |           Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|----------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |    14.3313      |     min |
|             Min cumulative indexing time across primary shards |                                |     0           |     min |
|          Median cumulative indexing time across primary shards |                                |     0.00696667  |     min |
|             Max cumulative indexing time across primary shards |                                |     4.85003     |     min |
|            Cumulative indexing throttle time of primary shards |                                |     0           |     min |
|    Min cumulative indexing throttle time across primary shards |                                |     0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |     0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |     0           |     min |
|                        Cumulative merge time of primary shards |                                |     9.33973     |     min |
|                       Cumulative merge count of primary shards |                                |    93           |         |
|                Min cumulative merge time across primary shards |                                |     0           |     min |
|             Median cumulative merge time across primary shards |                                |     0.00276667  |     min |
|                Max cumulative merge time across primary shards |                                |     3.20492     |     min |
|               Cumulative merge throttle time of primary shards |                                |     2.39933     |     min |
|       Min cumulative merge throttle time across primary shards |                                |     0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |     0           |     min |
|       Max cumulative merge throttle time across primary shards |                                |     0.8346      |     min |
|                      Cumulative refresh time of primary shards |                                |     1.4791      |     min |
|                     Cumulative refresh count of primary shards |                                |   408           |         |
|              Min cumulative refresh time across primary shards |                                |     0           |     min |
|           Median cumulative refresh time across primary shards |                                |     0.0191417   |     min |
|              Max cumulative refresh time across primary shards |                                |     0.532383    |     min |
|                        Cumulative flush time of primary shards |                                |     0.143       |     min |
|                       Cumulative flush count of primary shards |                                |    14           |         |
|                Min cumulative flush time across primary shards |                                |     0           |     min |
|             Median cumulative flush time across primary shards |                                |     0.000191667 |     min |
|                Max cumulative flush time across primary shards |                                |     0.0622167   |     min |
|                                        Total Young Gen GC time |                                |    30.76        |       s |
|                                       Total Young Gen GC count |                                |  3790           |         |
|                                          Total Old Gen GC time |                                |     7.34        |       s |
|                                         Total Old Gen GC count |                                |   107           |         |
|                                                     Store size |                                |     5.79046     |      GB |
|                                                  Translog size |                                |     8.19564e-07 |      GB |
|                                         Heap used for segments |                                |     0.452423    |      MB |
|                                       Heap used for doc values |                                |     0.0294952   |      MB |
|                                            Heap used for terms |                                |     0.340546    |      MB |
|                                            Heap used for norms |                                |     0.0440674   |      MB |
|                                           Heap used for points |                                |     0           |      MB |
|                                    Heap used for stored fields |                                |     0.0383148   |      MB |
|                                                  Segment count |                                |    74           |         |
|                                    Total Ingest Pipeline count |                                |     0           |         |
|                                     Total Ingest Pipeline time |                                |     0           |       s |
|                                   Total Ingest Pipeline failed |                                |     0           |         |
|                                                 Min Throughput |                   index-append | 80680.2         |  docs/s |
|                                                Mean Throughput |                   index-append | 81105.1         |  docs/s |
|                                              Median Throughput |                   index-append | 81128.5         |  docs/s |
|                                                 Max Throughput |                   index-append | 81286.2         |  docs/s |
|                                        50th percentile latency |                   index-append |   338.821       |      ms |
|                                        90th percentile latency |                   index-append |   843.169       |      ms |
|                                        99th percentile latency |                   index-append |  1132.18        |      ms |
|                                       100th percentile latency |                   index-append |  1158.98        |      ms |
|                                   50th percentile service time |                   index-append |   338.821       |      ms |
|                                   90th percentile service time |                   index-append |   843.169       |      ms |
|                                   99th percentile service time |                   index-append |  1132.18        |      ms |
|                                  100th percentile service time |                   index-append |  1158.98        |      ms |
|                                                     error rate |                   index-append |     0           |       % |
[..]                                                 

----------------------------------
[INFO] SUCCESS (took 4314 seconds)
----------------------------------
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://esrally.readthedocs.io/en/stable/configuration.html">https://esrally.readthedocs.io/en/stable/configuration.html</a></li>
<li><a href="https://github.com/elastic/rally-tracks/tree/master/geonames">https://github.com/elastic/rally-tracks/tree/master/geonames</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Key concepts in Elastic Rally]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="pipelines">Pipelines</h2>
<p>A pipeline is a series of steps that are performed to get benchmark results. This is not intended to customize the actual benchmark but rather what happens before and after a benchmark.</p>
<p>An example will clarify the concept: If you want to benchmark a binary distribution of Elasticsearch, Rally</p>]]></description><link>https://relentlesstorm.github.io/key-concepts-in-esrally/</link><guid isPermaLink="false">63afc78896dbf412f1f80391</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 23 Oct 2022 05:24:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1634042148057-32603ec95d95?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE5fHxlbGFzdGljfGVufDB8fHx8MTY3MjQ2NDA5Nw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="pipelines">Pipelines</h2>
<img src="https://images.unsplash.com/photo-1634042148057-32603ec95d95?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE5fHxlbGFzdGljfGVufDB8fHx8MTY3MjQ2NDA5Nw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Key concepts in Elastic Rally"><p>A pipeline is a series of steps that are performed to get benchmark results. This is not intended to customize the actual benchmark but rather what happens before and after a benchmark.</p>
<p>An example will clarify the concept: If you want to benchmark a binary distribution of Elasticsearch, Rally has to download a distribution archive, decompress it, start Elasticsearch and then run the benchmark. However, if you want to benchmark a source build of Elasticsearch, it first has to build a distribution using the Gradle Wrapper. So, in both cases, different steps are involved and that&#x2019;s what pipelines are for.</p>
<p>You can get a list of all pipelines with <em>esrally list pipelines</em></p>
<pre><code class="language-shell">$ esrally --version
esrally 2.6.0

$ esrally list pipelines

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available pipelines:

Name               Description
-----------------  ---------------------------------------------------------------------------------------------
from-sources       Builds and provisions Elasticsearch, runs a benchmark and reports results.
from-distribution  Downloads an Elasticsearch distribution, provisions it, runs a benchmark and reports results.
benchmark-only     Assumes an already running Elasticsearch instance, runs a benchmark and reports results

-------------------------------
[INFO] SUCCESS (took 0 seconds)
-------------------------------
</code></pre>
<h3 id="benchmark-only">benchmark-only</h3>
<p>This is intended if you want to provision a cluster by yourself. Do not use this pipeline unless you are absolutely sure you need to. As Rally has not provisioned the cluster, results are not easily reproducable and it also cannot gather a lot of metrics (like CPU usage).</p>
<p>To benchmark a cluster, you also have to specify the hosts to connect to.</p>
<pre><code class="language-shell">$ esrally race --pipeline=benchmark-only --target-host=10.10.10.1:39200,10.10.10.2:39200,10.10.10.3:39200 --track=geonames
</code></pre>
<h3 id="from-distribution">from-distribution</h3>
<p>This pipeline allows to benchmark an official Elasticsearch distribution which will be automatically downloaded by Rally. An example invocation:</p>
<pre><code class="language-shell">$ esrally race --track=geonames --pipeline=from-distribution --distribution-version=7.17.0
</code></pre>
<h3 id="from-sources">from-sources</h3>
<p>You should use this pipeline when you want to build and benchmark Elasticsearch from sources.</p>
<p>Remember that you also need git installed. You have to specify a revision.</p>
<pre><code class="language-shell">$ esrally race --track=geonames --pipeline=from-sources --revision=latest
</code></pre>
<h2 id="track">track</h2>
<p>A track is the description of one or more benchmarking scenarios with a specific document corpus. It defines for example the involved indices, data files and which operations are invoked. List the available tracks with <em>esrally list tracks</em>. Although Rally ships with some tracks out of the box, you should usually create your own track based on your own data.</p>
<ul>
<li>Geonames: for evaluating the performance of structured data.</li>
<li>Geopoint: for evaluating the performance of geopoint queries.</li>
<li>Geopointshape: for evaluating the performance of geopointshape queries.</li>
<li>Geoshape: for evaluating the performance of geoshape data.</li>
<li>Percolator: for evaluating the performance of percolation queries.</li>
<li>PMC: for evaluating the performance of full text search.</li>
<li>NYC taxis: for evaluating the performance for highly structured data.</li>
<li>NYC taxis (ARM): for evaluating the performance for highly structured data and detect ARM-specific regressions.</li>
<li>Nested: for evaluating the performance for nested documents.</li>
<li>HTTP Logs: for evaluating the performance of (Web) server logs.</li>
<li>NOAA: for evaluating the performance of range fields.</li>
<li>EQL: for evaluating the performance of EQL.</li>
<li>SQL: for evaluating the performance of SQL.</li>
<li>SO (Transform): for evaluating the performance of the transform feature.</li>
<li>SO (Frequent Items): for evaluating the performance of the frequent items aggregation.</li>
<li>Logging: for evaluating the performance of the Log Monitoring part of Elastic&apos;s Observability solution.</li>
<li>Logging (Many Shards): for evaluating the performance of the Log Monitoring part of Elastic&apos;s Observability solution with a large amount of shards.</li>
<li>Logging (Snapshots): for evaluating snapshot performance when taking snapshot with a large of amount of shards.</li>
<li>Dense Vector: for evaluating the performance of vectors search.</li>
<li>Security: for evaluating the performance of Elastic&apos;s Security solution.</li>
<li>TSDB: for evaluating the performance of TSDB (time series database).</li>
<li>Logging (CCS): for evaluating the performance of CCS (cross cluster search).</li>
</ul>
<pre><code class="language-shell">$ esrally list tracks

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available tracks:

Name              Description                                                              Documents    Compressed Size    Uncompressed Size    Default Challenge        All Challenges
----------------  -----------------------------------------------------------------------  -----------  -----------------  -------------------  -----------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
elastic/logs      Track for simulating logging workloads                                   14,009,078   N/A                N/A                  logging-indexing         logging-indexing-querying,logging-indexing,logging-querying,logging-snapshot-mount,logging-snapshot-restore,logging-snapshot,cross-clusters-search,logging-disk-usage,many-shards-quantitative,many-shards-snapshots
elastic/security  Track for simulating Elastic Security workloads                          77,513,777   N/A                N/A                  security-querying        security-indexing-querying,security-indexing,security-querying,index-alert-source-events
elastic/endpoint  Endpoint track                                                           0            0 bytes            0 bytes              default                  default
eql               EQL benchmarks based on endgame index of SIEM demo cluster               60,782,211   4.5 GB             109.2 GB             default                  default,index-sorting
geonames          POIs from Geonames                                                       11,396,503   252.9 MB           3.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts,significant-text
geopoint          Point coordinates from PlanetOSM                                         60,844,404   482.1 MB           2.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geopointshape     Point coordinates from PlanetOSM indexed as geoshapes                    60,844,404   470.8 MB           2.6 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geoshape          Shapes from PlanetOSM                                                    84,220,567   17.0 GB            58.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-big
http_logs         HTTP server log data                                                     247,249,096  1.2 GB             31.1 GB              append-no-conflicts      append-no-conflicts,runtime-fields,append-no-conflicts-index-only,append-sorted-no-conflicts,append-index-only-with-ingest-pipeline,update,append-no-conflicts-index-reindex-only
metricbeat        Metricbeat data                                                          1,079,600    87.7 MB            1.2 GB               append-no-conflicts      append-no-conflicts
nested            StackOverflow Q&amp;A stored as nested docs                                  11,203,029   663.3 MB           3.4 GB               nested-search-challenge  nested-search-challenge,index-only
noaa              Global daily weather measurements from NOAA                              33,659,481   949.4 MB           9.0 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,aggs,filter-aggs
nyc_taxis         Taxi rides in New York in 2015                                           165,346,692  4.5 GB             74.3 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts-index-only,update,append-ml,aggs
percolator        Percolator benchmark based on AOL queries                                2,000,000    121.1 kB           104.9 MB             append-no-conflicts      append-no-conflicts
pmc               Full text benchmark with academic papers from PMC                        574,199      5.5 GB             21.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts,indexing-querying
so                Indexing benchmark using up to questions and answers from StackOverflow  36,062,278   8.9 GB             33.1 GB              append-no-conflicts      append-no-conflicts,transform,frequent-items
sql               SQL query performance based on NOAA Weather data                         33,659,481   949.4 MB           9.0 GB               sql                      sql
dense_vector      Benchmark for dense vector indexing and search                           10,000,000   7.2 GB             19.5 GB              index-and-search         index-and-search
so_vector         Benchmark for vector search with StackOverflow data                      2,000,000    12.3 GB            32.2 GB              index-and-search         index-and-search
tsdb              metricbeat information for elastic-app k8s cluster                       116,633,698  N/A                123.0 GB             append-no-conflicts      append-no-conflicts,downsample

-------------------------------
[INFO] SUCCESS (took 2 seconds)
-------------------------------
</code></pre>
<pre><code class="language-shell">$ esrally info --track=geonames

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Showing details for track [geonames]:

* Description: POIs from Geonames
* Documents: 11,396,503
* Compressed Size: 252.9 MB
* Uncompressed Size: 3.3 GB

================================================
Challenge [append-no-conflicts] (run by default)
================================================

Indexes the whole document corpus using Elasticsearch default settings. We only adjust the number of replicas as we benchmark a single node cluster and Rally will only start the benchmark if the cluster turns green. Document ids are unique so all index operations are append only. After that a couple of queries are run.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-append (8 clients)
5. refresh-after-index
6. force-merge
7. refresh-after-force-merge
8. wait-until-merges-finish
9. index-stats
10. node-stats
11. default
12. term
13. phrase
14. country_agg_uncached
15. country_agg_cached
16. scroll
17. expression
18. painless_static
19. painless_dynamic
20. decay_geo_gauss_function_score
21. decay_geo_gauss_script_score
22. field_value_function_score
23. field_value_script_score
24. large_terms
25. large_filtered_terms
26. large_prohibited_terms
27. desc_sort_population
28. asc_sort_population
29. asc_sort_with_after_population
30. desc_sort_geonameid
31. desc_sort_with_after_geonameid
32. asc_sort_geonameid
33. asc_sort_with_after_geonameid

==========================================
Challenge [append-no-conflicts-index-only]
==========================================

Indexes the whole document corpus using Elasticsearch default settings. We only adjust the number of replicas as we benchmark a single node cluster and Rally will only start the benchmark if the cluster turns green. Document ids are unique so all index operations are append only.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-append (8 clients)
5. force-merge
6. wait-until-merges-finish

======================================
Challenge [append-fast-with-conflicts]
======================================

Indexes the whole document corpus using a setup that will lead to a larger indexing throughput than the default settings. Rally will produce duplicate ids in 25% of all documents (not configurable) so we can simulate a scenario with appends most of the time and some updates in between.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-update (8 clients)
5. force-merge
6. wait-until-merges-finish

============================
Challenge [significant-text]
============================

Indexes the whole document corpus using Elasticsearch default settings. We only adjust the number of replicas as we benchmark a single node cluster and Rally will only start the benchmark if the cluster turns green. Document ids are unique so all index operations are append only.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-append (8 clients)
5. force-merge
6. wait-until-merges-finish
7. significant_text_selective
8. significant_text_sampled_selective
9. significant_text_unselective
10. significant_text_sampled_unselective


-------------------------------
[INFO] SUCCESS (took 1 seconds)
-------------------------------
</code></pre>
<p>A track is specified in a JSON file. A track JSON file can include the following sections:</p>
<ul>
<li>indices/templates: define the relevant indices and index templates</li>
<li>data-streams: define the relevant data streams</li>
<li>composable-templates/component-templates: define the relevant composable and component templates</li>
<li>corpora: define all document corpora (i.e. data files) that Rally should use for this track</li>
<li>challenge(s): describe more than one set of operations, in the event your track needs to test more than one set of scenarios</li>
<li>schedule: describe the workload for the benchmark, for example index with two clients at maximum throughput while searching with another two clients with ten operations per second</li>
<li>operations: describe which operations are available for this track and how they are parametrized</li>
<li>dependencies</li>
</ul>
<pre><code class="language-shell">$ ls .rally/benchmarks/tracks/default/geonames
challenges  files.txt  index.json  operations  __pycache__  README.md  terms.txt  track.json  track.py

$ cat .rally/benchmarks/tracks/default/geonames/track.json
{
  &quot;version&quot;: 2,
  &quot;description&quot;: &quot;POIs from Geonames&quot;,
  &quot;data-url&quot;: &quot;http://benchmarks.elasticsearch.org.s3.amazonaws.com/corpora/geonames&quot;,
  &quot;indices&quot;: [
    {
      &quot;name&quot;: &quot;geonames&quot;,
      &quot;body&quot;: &quot;index.json&quot;
    }
  ],
  &quot;corpora&quot;: [
    {
      &quot;name&quot;: &quot;geonames&quot;,
      &quot;base-url&quot;: &quot;https://rally-tracks.elastic.co/geonames&quot;,
      &quot;documents&quot;: [
        {
          &quot;source-file&quot;: &quot;documents-2.json.bz2&quot;,
          &quot;document-count&quot;: 11396503,
          &quot;compressed-bytes&quot;: 265208777,
          &quot;uncompressed-bytes&quot;: 3547613828
        }
      ]
    }
  ],
  &quot;operations&quot;: [
    {{ rally.collect(parts=&quot;operations/*.json&quot;) }}
  ],
  &quot;challenges&quot;: [
    {{ rally.collect(parts=&quot;challenges/*.json&quot;) }}
  ]
}

$ cat .rally/benchmarks/tracks/default/geonames/index.json
{
  &quot;settings&quot;: {
    &quot;index.number_of_shards&quot;: {{number_of_shards | default(5)}},
    &quot;index.number_of_replicas&quot;: {{number_of_replicas | default(0)}},
    &quot;index.store.type&quot;: &quot;{{store_type | default(&apos;fs&apos;)}}&quot;,
    &quot;index.requests.cache.enable&quot;: false
  },
  &quot;mappings&quot;: {
    &quot;dynamic&quot;: &quot;strict&quot;,
    &quot;_source&quot;: {
      &quot;enabled&quot;: {{ source_enabled | default(true) | tojson }}
    },
    &quot;properties&quot;: {
    [..]
  }
}
</code></pre>
<h2 id="challenge">challenge</h2>
<p>A challenge describes one benchmarking scenario, for example indexing documents at maximum throughput with 4 clients while issuing term and phrase queries from another two clients rate-limited at 10 queries per second each. It is always specified in the context of a track. See the available challenges by listing the corresponding tracks with <em>esrally list tracks</em>.</p>
<h2 id="car">car</h2>
<p>A car is a specific configuration of an Elasticsearch cluster that is benchmarked, for example the out-of-the-box configuration, a configuration with a specific heap size or a custom logging configuration. List the available cars with <em>esrally list cars</em>.</p>
<pre><code class="language-shell">$ esrally list cars

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available cars:

Name                     Type    Description
-----------------------  ------  --------------------------------------
16gheap                  car     Sets the Java heap to 16GB
1gheap                   car     Sets the Java heap to 1GB
24gheap                  car     Sets the Java heap to 24GB
2gheap                   car     Sets the Java heap to 2GB
4gheap                   car     Sets the Java heap to 4GB
8gheap                   car     Sets the Java heap to 8GB
defaults                 car     Sets the Java heap to 1GB
basic-license            mixin   Basic License
debug-non-safepoints     mixin   More accurate CPU profiles
ea                       mixin   Enables Java assertions
fp                       mixin   Preserves frame pointers
g1gc                     mixin   Enables the G1 garbage collector
parallelgc               mixin   Enables the Parallel garbage collector
trial-license            mixin   Trial License
unpooled                 mixin   Enables Netty&apos;s unpooled allocator
x-pack-ml                mixin   X-Pack Machine Learning
x-pack-monitoring-http   mixin   X-Pack Monitoring (HTTP exporter)
x-pack-monitoring-local  mixin   X-Pack Monitoring (local exporter)
x-pack-security          mixin   X-Pack Security
zgc                      mixin   Enables the ZGC garbage collector

-------------------------------
[INFO] SUCCESS (took 1 seconds)
-------------------------------
</code></pre>
<h2 id="telemetry">telemetry</h2>
<p>Telemetry is used in Rally to gather metrics about the car, for example CPU usage or index size.</p>
<pre><code class="language-shell">$ esrally list telemetry

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available telemetry devices:

Command                     Name                        Description
--------------------------  --------------------------  --------------------------------------------------------------------
jit                         JIT Compiler Profiler       Enables JIT compiler logs.
gc                          GC log                      Enables GC logs.
jfr                         Flight Recorder             Enables Java Flight Recorder (requires an Oracle JDK or OpenJDK 11+)
heapdump                    Heap Dump                   Captures a heap dump.
node-stats                  Node Stats                  Regularly samples node stats
recovery-stats              Recovery Stats              Regularly samples shard recovery stats
ccr-stats                   CCR Stats                   Regularly samples Cross Cluster Replication (CCR) related stats
segment-stats               Segment Stats               Determines segment stats at the end of the benchmark.
transform-stats             Transform Stats             Regularly samples transform stats
searchable-snapshots-stats  Searchable Snapshots Stats  Regularly samples searchable snapshots stats
shard-stats                 Shard Stats                 Regularly samples nodes stats at shard level
data-stream-stats           Data Stream Stats           Regularly samples data stream stats
ingest-pipeline-stats       Ingest Pipeline Stats       Reports Ingest Pipeline stats at the end of the benchmark.
disk-usage-stats            Disk usage of each field    Runs the indices disk usage API after benchmarking

Keep in mind that each telemetry device may incur a runtime overhead which can skew results.

-------------------------------
[INFO] SUCCESS (took 0 seconds)
-------------------------------
</code></pre>
<h2 id="race">race</h2>
<p>A race is one invocation of the Rally binary. Another name for that is one &#x201C;benchmarking trial&#x201D;. During a race, Rally runs one challenge on a track with the given car.</p>
<pre><code class="language-shell">$ esrally race --pipeline=benchmark-only --target-host=10.10.10.1:39200,10.10.10.2:39200,10.10.10.3:39200 --track=geonames --track-params=&quot;number_of_shards:3,number_of_replicas:1&quot; --challenge=append-no-conflicts --on-error=abort --race-id=${RACE_ID}

$ esrally list races

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/


Recent races:

Race ID                               Race Timestamp    Track     Track Parameters                          Challenge            Car       User Tags                  Track Revision    Team Revision
------------------------------------  ----------------  --------  ----------------------------------------  -------------------  --------  -------------------------  ----------------  ---------------
066e02fa-a71a-4239-b515-984f705d5f02  20221031T230044Z  geonames  number_of_replicas=1, number_of_shards=3  append-no-conflicts  external  intention=3shards1replica  a64a92a
734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f  20221031T214254Z  geonames  number_of_replicas=0, number_of_shards=3  append-no-conflicts  external  intention=3shards0replica  a64a92a
</code></pre>
<h2 id="tournament">tournament</h2>
<p>A tournament is a comparison of two races. Looks like <a href="https://github.com/elastic/rally/issues/57">Rally</a> doesn&apos;t have the tournament mode support yet. Instead, the comparison can be made as the following command between two races. Note that, we should NOT run the same benchmark multiple times without data cleanup between the benchmarks. It will give us unreproducible results.</p>
<pre><code class="language-shell">$ esrally compare --baseline=066e02fa-a71a-4239-b515-984f705d5f02 --contender=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/


Comparing baseline
  Race ID: 066e02fa-a71a-4239-b515-984f705d5f02
  Race timestamp: 2022-10-31 23:00:44
  Challenge: append-no-conflicts
  Car: external
  User tags: intention=3shards1replica

with contender
  Race ID: 734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
  Race timestamp: 2022-10-31 21:42:54
  Challenge: append-no-conflicts
  Car: external
  User tags: intention=3shards0replica

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                        Metric |                           Task |        Baseline |       Contender |         Diff |    Unit |   Diff % |
|--------------------------------------------------------------:|-------------------------------:|----------------:|----------------:|-------------:|--------:|---------:|
|                    Cumulative indexing time of primary shards |                                |    14.3493      |    14.0125      |     -0.33683 |     min |   -2.35% |
|             Min cumulative indexing time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|          Median cumulative indexing time across primary shard |                                |     0.00696667  |     0.00696667  |      0       |     min |    0.00% |
|             Max cumulative indexing time across primary shard |                                |     4.954       |     4.73868     |     -0.21532 |     min |   -4.35% |
|           Cumulative indexing throttle time of primary shards |                                |     0           |     0           |      0       |     min |    0.00% |
|    Min cumulative indexing throttle time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
| Median cumulative indexing throttle time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|    Max cumulative indexing throttle time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|                       Cumulative merge time of primary shards |                                |     9.5478      |     4.86773     |     -4.68007 |     min |  -49.02% |
|                      Cumulative merge count of primary shards |                                |    93           |    70           |    -23       |         |  -24.73% |
|                Min cumulative merge time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|             Median cumulative merge time across primary shard |                                |     0.00276667  |     0.00276667  |      0       |     min |    0.00% |
|                Max cumulative merge time across primary shard |                                |     3.61232     |     1.71572     |     -1.8966  |     min |  -52.50% |
|              Cumulative merge throttle time of primary shards |                                |     2.62635     |     0.569683    |     -2.05667 |     min |  -78.31% |
|       Min cumulative merge throttle time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|    Median cumulative merge throttle time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|       Max cumulative merge throttle time across primary shard |                                |     1.06415     |     0.21315     |     -0.851   |     min |  -79.97% |
|                     Cumulative refresh time of primary shards |                                |     1.37218     |     0.45075     |     -0.92143 |     min |  -67.15% |
|                    Cumulative refresh count of primary shards |                                |   421           |   327           |    -94       |         |  -22.33% |
|              Min cumulative refresh time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|           Median cumulative refresh time across primary shard |                                |     0.0191417   |     0.0191417   |      0       |     min |    0.00% |
|              Max cumulative refresh time across primary shard |                                |     0.4631      |     0.152433    |     -0.31067 |     min |  -67.08% |
|                       Cumulative flush time of primary shards |                                |     0.144633    |     0.185383    |      0.04075 |     min |  +28.17% |
|                      Cumulative flush count of primary shards |                                |    14           |    14           |      0       |         |    0.00% |
|                Min cumulative flush time across primary shard |                                |     0           |     0           |      0       |     min |    0.00% |
|             Median cumulative flush time across primary shard |                                |     0.000191667 |     0.000191667 |      0       |     min |    0.00% |
|                Max cumulative flush time across primary shard |                                |     0.0575833   |     0.0674167   |      0.00983 |     min |  +17.08% |
|                                       Total Young Gen GC time |                                |    30.494       |    14.511       |    -15.983   |       s |  -52.41% |
|                                      Total Young Gen GC count |                                |  3888           |  2381           |  -1507       |         |  -38.76% |
|                                         Total Old Gen GC time |                                |     7.329       |     3.247       |     -4.082   |       s |  -55.70% |
|                                        Total Old Gen GC count |                                |   109           |    48           |    -61       |         |  -55.96% |
|                                                    Store size |                                |     5.78897     |     3.01044     |     -2.77854 |      GB |  -48.00% |
|                                                 Translog size |                                |     8.19564e-07 |     6.65896e-07 |     -0       |      GB |  -18.75% |
|                                        Heap used for segments |                                |     0.513947    |     0.385025    |     -0.12892 |      MB |  -25.08% |
|                                      Heap used for doc values |                                |     0.037796    |     0.0160522   |     -0.02174 |      MB |  -57.53% |
|                                           Heap used for terms |                                |     0.383759    |     0.296478    |     -0.08728 |      MB |  -22.74% |
|                                           Heap used for norms |                                |     0.0499268   |     0.0380859   |     -0.01184 |      MB |  -23.72% |
|                                          Heap used for points |                                |     0           |     0           |      0       |      MB |    0.00% |
|                                   Heap used for stored fields |                                |     0.0424652   |     0.0344086   |     -0.00806 |      MB |  -18.97% |
|                                                 Segment count |                                |    82           |    66           |    -16       |         |  -19.51% |
|                                   Total Ingest Pipeline count |                                |     0           |     0           |      0       |         |    0.00% |
|                                    Total Ingest Pipeline time |                                |     0           |     0           |      0       |      ms |    0.00% |
|                                  Total Ingest Pipeline failed |                                |     0           |     0           |      0       |         |    0.00% |
|                                                    error rate |                   index-append |     0           |     0           |      0       |       % |    0.00% |
|                                                Min Throughput |                    index-stats |    90.001       |    90.0126      |      0.01165 |   ops/s |   +0.01% |
|                                               Mean Throughput |                    index-stats |    90.0055      |    90.0241      |      0.01854 |   ops/s |   +0.02% |
|                                             Median Throughput |                    index-stats |    90.0048      |    90.0219      |      0.01715 |   ops/s |   +0.02% |
|                                                Max Throughput |                    index-stats |    90.0142      |    90.0422      |      0.02796 |   ops/s |   +0.03% |
|                                       50th percentile latency |                    index-stats |     5.80729     |     5.40389     |     -0.40339 |      ms |   -6.95% |
|                                       90th percentile latency |                    index-stats |     6.71673     |     6.22462     |     -0.49211 |      ms |   -7.33% |
|                                       99th percentile latency |                    index-stats |     7.26607     |     6.8574      |     -0.40867 |      ms |   -5.62% |
|                                     99.9th percentile latency |                    index-stats |    11.3788      |    11.1477      |     -0.23112 |      ms |   -2.03% |
|                                      100th percentile latency |                    index-stats |    13.6309      |    14.0192      |      0.38829 |      ms |   +2.85% |
|                                  50th percentile service time |                    index-stats |     4.61601     |     4.24021     |     -0.37579 |      ms |   -8.14% |
|                                  90th percentile service time |                    index-stats |     5.40477     |     4.93823     |     -0.46654 |      ms |   -8.63% |
|                                  99th percentile service time |                    index-stats |     5.85565     |     5.34821     |     -0.50744 |      ms |   -8.67% |
|                                99.9th percentile service time |                    index-stats |     9.8608      |     7.74023     |     -2.12057 |      ms |  -21.51% |
|                                 100th percentile service time |                    index-stats |    12.6355      |     9.44686     |     -3.18866 |      ms |  -25.24% |
|                                                    error rate |                    index-stats |     0           |     0           |      0       |
[..]

-------------------------------
[INFO] SUCCESS (took 0 seconds)
-------------------------------
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://esrally.readthedocs.io/en/stable/track.html">https://esrally.readthedocs.io/en/stable/track.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/tournament.html">https://esrally.readthedocs.io/en/stable/tournament.html</a></li>
<li><a href="https://github.com/elastic/rally/issues/57">https://github.com/elastic/rally/issues/57</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Getting started with Elasticsearch and Kibana]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-elasticsearch">Install elasticsearch</h2>
<h3 id="add-user-for-elasticsearch">Add user for elasticsearch</h3>
<pre><code class="language-shell">[root@vm1 home]# groupadd es
[root@vm1 home]# useradd es -g es
[root@vm1 home]# passwd es
[root@vm1 home]# cd es
</code></pre>
<h3 id="download-elasticsearch">Download elasticsearch</h3>
<pre><code class="language-shell">[root@vm1 es]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.4.3-linux-x86_64.tar.gz
[root@vm1 es]# tar</code></pre>]]></description><link>https://relentlesstorm.github.io/getting-started-with-elasticsearch/</link><guid isPermaLink="false">63afc69196dbf412f1f80378</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 17 Oct 2022 05:20:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1643042945810-1119948eeabc?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDd8fGVsYXN0aWN8ZW58MHx8fHwxNjcyNDY0MDk3&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-elasticsearch">Install elasticsearch</h2>
<h3 id="add-user-for-elasticsearch">Add user for elasticsearch</h3>
<pre><code class="language-shell">[root@vm1 home]# groupadd es
[root@vm1 home]# useradd es -g es
[root@vm1 home]# passwd es
[root@vm1 home]# cd es
</code></pre>
<h3 id="download-elasticsearch">Download elasticsearch</h3>
<pre><code class="language-shell">[root@vm1 es]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.4.3-linux-x86_64.tar.gz
[root@vm1 es]# tar zxf elasticsearch-8.4.3-linux-x86_64.tar.gz
[root@vm1 es]# chown -R es:es /home/es
[root@vm1 es]# su es
[es@vm1 ~]$ cd elasticsearch-8.4.3/
</code></pre>
<h3 id="start-elasticsearch">Start elasticsearch</h3>
<pre><code class="language-shell">[es@vm1 elasticsearch-8.4.3]$ bin/elasticsearch
[...]
[2022-10-17T17:07:49,984][INFO ][o.e.h.AbstractHttpServerTransport] [vm1] publish_address {127.0.0.1:9200}, bound_addresses {[::]:9200}
[...]

&#x2705; Elasticsearch security features have been automatically configured!
&#x2705; Authentication is enabled and cluster connections are encrypted.

&#x2139;&#xFE0F;  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  [...]

&#x2139;&#xFE0F;  HTTP CA certificate SHA-256 fingerprint:
  [...]

&#x2139;&#xFE0F;  Configure Kibana to use this cluster:
&#x2022; Run Kibana and click the configuration link in the terminal when Kibana starts.
&#x2022; Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  [...]

&#x2139;&#xFE0F;  Configure other nodes to join this cluster:
&#x2022; On this node:
  &#x2043; Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
  &#x2043; Uncomment the transport.host setting at the end of config/elasticsearch.yml.
  &#x2043; Restart Elasticsearch.
&#x2022; On other nodes:
  &#x2043; Start Elasticsearch with `bin/elasticsearch --enrollment-token &lt;token&gt;`, using the enrollment token that you generated.
</code></pre>
<h3 id="verify-elasticsearch">Verify elasticsearch</h3>
<pre><code class="language-shell">[root@vm2 es]# curl 10.10.10.1:9200
curl: (52) Empty reply from server
</code></pre>
<img src="https://images.unsplash.com/photo-1643042945810-1119948eeabc?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDd8fGVsYXN0aWN8ZW58MHx8fHwxNjcyNDY0MDk3&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Getting started with Elasticsearch and Kibana"><p>Modify elastic config file:</p>
<pre><code class="language-shell">[es@vm1 elasticsearch-8.4.3]$ vim config/elasticsearch.yml
# By default Elasticsearch is only accessible on localhost. Set a different
# address here to expose this node on the network:
#
#network.host: 192.168.0.1
network.host: 10.10.10.1

# Enable security features
xpack.security.enabled: false
</code></pre>
<p>Restart elasticsearch process and verify again:</p>
<pre><code class="language-shell">[root@vm2 es]# curl 10.10.10.1:9200
{
  &quot;name&quot; : &quot;vm1&quot;,
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;cluster_uuid&quot; : &quot;ZZ_MBiS5Qi-3RFSdyk_-Kg&quot;,
  &quot;version&quot; : {
    &quot;number&quot; : &quot;8.4.3&quot;,
    &quot;build_flavor&quot; : &quot;default&quot;,
    &quot;build_type&quot; : &quot;tar&quot;,
    &quot;build_hash&quot; : &quot;42f05b9372a9a4a470db3b52817899b99a76ee73&quot;,
    &quot;build_date&quot; : &quot;2022-10-04T07:17:24.662462378Z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;9.3.0&quot;,
    &quot;minimum_wire_compatibility_version&quot; : &quot;7.17.0&quot;,
    &quot;minimum_index_compatibility_version&quot; : &quot;7.0.0&quot;
  },
  &quot;tagline&quot; : &quot;You Know, for Search&quot;
}
</code></pre>
<h2 id="install-kibana">Install Kibana</h2>
<h3 id="download-kibana">Download Kibana</h3>
<pre><code class="language-shell">[root@vm2 es]# wget https://artifacts.elastic.co/downloads/kibana/kibana-8.4.3-linux-x86_64.tar.gz
[root@vm2 es]# tar zxf kibana-8.4.3-linux-x86_64.tar.gz
[root@vm2 es]# cd kibana-8.4.3/
[root@vm2 kibana-8.4.3]# chown -R es:es /home/es
</code></pre>
<h3 id="start-kibana">Start Kibana</h3>
<pre><code class="language-shell">[root@vm2 kibana-8.4.3]# bin/kibana
Kibana should not be run as root.  Use --allow-root to continue.

[root@vm2 kibana-8.4.3]# su es
[es@vm2 kibana-8.4.3]$ bin/kibana
[2022-10-17T17:41:59.539-07:00][INFO ][node] Kibana process configured with roles: [background_tasks, ui]
[2022-10-17T17:42:06.604-07:00][INFO ][http.server.Preboot] http server running at http://localhost:5601
[2022-10-17T17:42:06.644-07:00][INFO ][plugins-system.preboot] Setting up [1] plugins: [interactiveSetup]
[2022-10-17T17:42:06.646-07:00][INFO ][preboot] &quot;interactiveSetup&quot; plugin is holding setup: Validating Elasticsearch connection configuration&#x2026;
[2022-10-17T17:42:06.681-07:00][INFO ][root] Holding setup until preboot stage is completed.

i Kibana has not been configured.

Go to http://localhost:5601/?code=263178 to get started.
</code></pre>
<p>Allow connections from remote users:</p>
<pre><code class="language-shell">[root@vm2 kibana-8.4.3]# vim config/kibana.yml
# To allow connections from remote users, set this parameter to a non-loopback address.
#server.host: &quot;localhost&quot;
server.host: &quot;10.10.10.2&quot;
</code></pre>
<p>Restart the kibana to reflect the change:</p>
<pre><code class="language-shell">[root@vm2 kibana-8.4.3]# su es
[es@vm2 kibana-8.4.3]$ bin/kibana
[...]
Go to http://10.10.10.2:5601/?code=293334 to get started.
</code></pre>
<h2 id="connect-kibana-to-elasticsearch">Connect Kibana to Elasticsearch</h2>
<p>From the Browser, enter &quot;<a href="http://10.10.10.2:5601/?code=293334">http://10.10.10.2:5601/?code=293334</a>&quot;.</p>
<p>If you run into the following issue when to connect to elasticsearch server from kibana web UI, you can change the URL from &quot;<a href="https://10.10.10.1:9200">https://10.10.10.1:9200</a>&quot; to &quot;<a href="http://10.10.10.1:9200">http://10.10.10.1:9200</a>&quot; for testing purpose.</p>
<pre><code>[2022-11-09T10:55:02.691-08:00][ERROR][plugins.interactiveSetup.elasticsearch] Unable to connect to host &quot;https://10.10.10.1:9200&quot;: write EPROTO 139880583923648:error:1408F10B:SSL routines:ssl3_get_record:wrong version number:../deps/openssl/openssl/ssl/record/ssl3_record.c:332:
</code></pre>
<h3 id="use-dev-tools-in-kibana">Use Dev Tools in Kibana</h3>
<p><img src="https://relentlesstorm.github.io/assets/images/posts/kifana_dev_tools.png" alt="Getting started with Elasticsearch and Kibana" loading="lazy">{:.shadow}</p>
<pre><code>GET _cluster/health
{
  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,
  &quot;status&quot;: &quot;green&quot;,
  &quot;timed_out&quot;: false,
  &quot;number_of_nodes&quot;: 1,
  &quot;number_of_data_nodes&quot;: 1,
  &quot;active_primary_shards&quot;: 10,
  &quot;active_shards&quot;: 10,
  &quot;relocating_shards&quot;: 0,
  &quot;initializing_shards&quot;: 0,
  &quot;unassigned_shards&quot;: 0,
  &quot;delayed_unassigned_shards&quot;: 0,
  &quot;number_of_pending_tasks&quot;: 0,
  &quot;number_of_in_flight_fetch&quot;: 0,
  &quot;task_max_waiting_in_queue_millis&quot;: 0,
  &quot;active_shards_percent_as_number&quot;: 100
}
GET _nodes/stats
{
  &quot;_nodes&quot;: {
    &quot;total&quot;: 1,
    &quot;successful&quot;: 1,
    &quot;failed&quot;: 0
  },
  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,
  &quot;nodes&quot;: {
    [...]
  }
}
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.elastic.co/downloads/">https://www.elastic.co/downloads/</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[How to setup the Grafana and Prometheus dashboard]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-node-exporter-on-each-target-nodes">Install node-exporter on each target nodes</h2>
<pre><code class="language-shell">$ docker run --restart=always --name nd-export -d -p 9100:9100 -v /proc:/host/proc -v /sys:/host/sys -v /:/rootfs:ro,rslave --net=host quay.io/prometheus/node-exporter
$ docker ps -a
CONTAINER ID   IMAGE                              COMMAND                CREATED          STATUS          PORTS     NAMES
d8c7fc8a4e40   quay.io/prometheus/node-exporter</code></pre>]]></description><link>https://relentlesstorm.github.io/how-to-setup-grafana-and-prometheus/</link><guid isPermaLink="false">63afcb9596dbf412f1f803eb</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sat, 15 Oct 2022 05:42:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1608222351212-18fe0ec7b13b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGRhc2hib2FyZHxlbnwwfHx8fDE2NzI0NjUzODU&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-node-exporter-on-each-target-nodes">Install node-exporter on each target nodes</h2>
<pre><code class="language-shell">$ docker run --restart=always --name nd-export -d -p 9100:9100 -v /proc:/host/proc -v /sys:/host/sys -v /:/rootfs:ro,rslave --net=host quay.io/prometheus/node-exporter
$ docker ps -a
CONTAINER ID   IMAGE                              COMMAND                CREATED          STATUS          PORTS     NAMES
d8c7fc8a4e40   quay.io/prometheus/node-exporter   &quot;/bin/node_exporter&quot;   32 minutes ago   Up 32 minutes             nd-export
</code></pre>
<h2 id="deploy-prometheus-on-a-vm">Deploy Prometheus on a VM</h2>
<pre><code class="language-shell">$ vim /etc/prometheus/prometheus.yml
global:
scrape_configs:
  - job_name: &apos;node&apos;
    scrape_interval: 10s
    static_configs:
     - targets: [&apos;localhost:9100&apos;,&apos;host1-ip:9100&apos;,&apos;host2-ip:9100&apos;,&apos;host3-ip:9100&apos;,&apos;host4-ip:9100&apos;]

$ docker run --restart=always --name prometheus -d -p 9090:9090 -v /etc/prometheus:/etc/prometheus prom/prometheus:v1.8.2

$ docker ps -a
CONTAINER ID   IMAGE                              COMMAND                  CREATED          STATUS          PORTS                                       NAMES
3700700b61c9   prom/prometheus:v1.8.2         &quot;/bin/prometheus -co&#x2026;&quot;   57 minutes ago   Up 22 minutes   0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp   prometheus
</code></pre>
<img src="https://images.unsplash.com/photo-1608222351212-18fe0ec7b13b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGRhc2hib2FyZHxlbnwwfHx8fDE2NzI0NjUzODU&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="How to setup the Grafana and Prometheus dashboard"><p>Verify the Prometheus by accessing <a href="http://vm-ip:9090/targets">http://vm-ip:9090/targets</a>. You should be able to see the target nodes from the web UI.</p>
<h2 id="deploy-grafana-on-a-vm">Deploy Grafana on a VM</h2>
<pre><code class="language-shell">$ docker run --restart=always --name grafana -d -p 3000:3000 grafana/grafana

$ docker ps -a
CONTAINER ID   IMAGE                              COMMAND                  CREATED          STATUS          PORTS                                       NAMES
da48a274ecc0   quay.io/prometheus/node-exporter   &quot;/bin/node_exporter&quot;     27 minutes ago   Up 27 minutes                                               nd-export
814511410bff   grafana/grafana                    &quot;/run.sh&quot;                57 minutes ago   Up 57 minutes   0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp   grafana
3700700b61c9   prom/prometheus:v1.8.2             &quot;/bin/prometheus -co&#x2026;&quot;   57 minutes ago   Up 22 minutes   0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp   prometheus
</code></pre>
<h2 id="configure-the-promethous-data-source-in-grafana">Configure the Promethous data source in Grafana</h2>
<p>Login to the Grafana web UI by accessing <a href="http://vm-ip:3000/">http://vm-ip:3000/</a>. The default username and password is admin/admin.</p>
<p>On the left corner, click &quot;Add sources&quot;.</p>
<p><img src="https://relentlesstorm.github.io/assets/images/posts/grafana-datasource.png" alt="How to setup the Grafana and Prometheus dashboard" loading="lazy">{:.shadow}</p>
<p>Then add &quot;Prometheus&quot; as data source and fill in the prometheus data source name and the URL. Click Save &amp; test. The data source name will be used when you create the Grafana dashboard later.</p>
<p><img src="https://relentlesstorm.github.io/assets/images/posts/grafana-datasource-1.png" alt="How to setup the Grafana and Prometheus dashboard" loading="lazy">{:.shadow}</p>
<h2 id="create-the-grafana-dashboard">Create the Grafana dashboard</h2>
<p>The Grafana dashboard can be created by importing an json template and binding to the previous prometheus data source.</p>
<p><img src="https://relentlesstorm.github.io/assets/images/posts/grafana-dashboard.png" alt="How to setup the Grafana and Prometheus dashboard" loading="lazy">{:.shadow}</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>