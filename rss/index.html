<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Flamingbytes]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>https://relentlesstorm.github.io/</link><image><url>https://relentlesstorm.github.io/favicon.png</url><title>Flamingbytes</title><link>https://relentlesstorm.github.io/</link></image><generator>Ghost 5.26</generator><lastBuildDate>Wed, 04 Jan 2023 23:30:16 GMT</lastBuildDate><atom:link href="https://relentlesstorm.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Using virtctl to access virtual machine in Kubernetes]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-the-virtctl-client-tool">Install the virtctl client tool</h2>
<p>Basic VirtualMachineInstance operations can be performed with the stock kubectl utility. However, the virtctl binary utility is required to use advanced features such as:</p>
<ul>
<li>Serial and graphical console access</li>
</ul>
<p>It also provides convenience commands for:</p>
<ul>
<li>Starting and stopping VirtualMachineInstances</li>
<li>Live migrating VirtualMachineInstances</li>
<li>Uploading virtual machine</li></ul>]]></description><link>https://relentlesstorm.github.io/using-virtctl-to-access-virtual-machine-in-kubernetes/</link><guid isPermaLink="false">63b4b4a91527a00d97449fdf</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 02 Jan 2023 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372335936-3dc4ff716017?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-the-virtctl-client-tool">Install the virtctl client tool</h2>
<img src="https://images.unsplash.com/photo-1667372335936-3dc4ff716017?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using virtctl to access virtual machine in Kubernetes"><p>Basic VirtualMachineInstance operations can be performed with the stock kubectl utility. However, the virtctl binary utility is required to use advanced features such as:</p>
<ul>
<li>Serial and graphical console access</li>
</ul>
<p>It also provides convenience commands for:</p>
<ul>
<li>Starting and stopping VirtualMachineInstances</li>
<li>Live migrating VirtualMachineInstances</li>
<li>Uploading virtual machine disk images</li>
</ul>
<p>There are two ways to get it:</p>
<ul>
<li>the most recent version of the tool can be retrieved from the official release page</li>
<li>it can be installed as a kubectl plugin using krew</li>
</ul>
<p>Example:</p>
<pre><code class="language-shell">$ export VERSION=v0.48.1
$ wget https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64
$ ln -s virtctl-v0.48.1-linux-amd64 virtctl
$ chmod +x virtctl-v0.48.1-linux-amd64
$ ./virtctl version
Client Version: version.Info{GitVersion:&quot;v0.48.1&quot;, ...}
</code></pre>
<h2 id="access-the-virtual-machine-console">Access the virtual machine console</h2>
<pre><code class="language-shell">$ ./virtctl -h
virtctl controls virtual machine related operations on your kubernetes cluster.

Available Commands:
  addvolume         add a volume to a running VM
  console           Connect to a console of a virtual machine instance.
  expose            Expose a virtual machine instance, virtual machine, or virtual machine instance replica set as a new service.
  fslist            Return full list of filesystems available on the guest machine.
  guestfs           Start a shell into the libguestfs pod
  guestosinfo       Return guest agent info about operating system.
  help              Help about any command
  image-upload      Upload a VM image to a DataVolume/PersistentVolumeClaim.
  migrate           Migrate a virtual machine.
  pause             Pause a virtual machine
  permitted-devices List the permitted devices for vmis.
  port-forward      Forward local ports to a virtualmachine or virtualmachineinstance.
  removevolume      remove a volume from a running VM
  restart           Restart a virtual machine.
  soft-reboot       Soft reboot a virtual machine instance
  ssh               Open a SSH connection to a virtual machine instance.
  start             Start a virtual machine.
  stop              Stop a virtual machine.
  unpause           Unpause a virtual machine
  usbredir          Redirect a usb device to a virtual machine instance.
  userlist          Return full list of logged in users on the guest machine.
  version           Print the client and server version information.
  vnc               Open a vnc connection to a virtual machine instance.

Use &quot;virtctl &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;virtctl options&quot; for a list of global command-line options (applies to all commands).

$ ./virtctl console vm1
[root@vm1 output]# hostname
vm1
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/kubevirt/kubevirt/releases">https://github.com/kubevirt/kubevirt/releases</a></li>
<li><a href="https://kubevirt.io/user-guide/operations/virtctl_client_tool/">https://kubevirt.io/user-guide/operations/virtctl_client_tool/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using node selector to assign virtual machines to a node]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>In some circumstances, we want to control which node the virtual machine or pod deploys to. The <strong>node selector</strong> can be used to assign virtual machine or pod to a node.</p>
<h2 id="add-label-to-a-node">Add label to a node</h2>
<p>The label can be added to a node from either command line or openshift</p>]]></description><link>https://relentlesstorm.github.io/using-node-selector-to-assign-virtual-mahcines-to-a-node/</link><guid isPermaLink="false">63b4b4011527a00d97449fb7</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Fri, 30 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372335962-5fd503a8ae5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1667372335962-5fd503a8ae5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using node selector to assign virtual machines to a node"><p>In some circumstances, we want to control which node the virtual machine or pod deploys to. The <strong>node selector</strong> can be used to assign virtual machine or pod to a node.</p>
<h2 id="add-label-to-a-node">Add label to a node</h2>
<p>The label can be added to a node from either command line or openshift web console.</p>
<pre><code class="language-shell">$ oc get nodes
NAME                   STATUS                     ROLES    AGE   VERSION
master1   Ready                      master   46h   v1.20.11+63f841a
master2   Ready                      master   46h   v1.20.11+63f841a
master3   Ready                      master   46h   v1.20.11+63f841a
worker1    Ready                      worker   45h   v1.20.11+63f841a
worker2    Ready                      worker   45h   v1.20.11+63f841a
worker3    Ready,SchedulingDisabled   worker   45h   v1.20.11+63f841a

$ oc label nodes worker1 worker-node-name=w1

$ oc describe node worker1 | grep worker-node-name
                    worker-node-name=w1

$ oc get nodes --show-labels
NAME       STATUS                     ROLES    AGE   VERSION            LABELS
worker1    Ready                      worker   45h   v1.20.11+63f841a   &lt;omitted..&gt;worker-node-name=w1
&lt;omitted..&gt;
</code></pre>
<p>This can also be done through openshift web console by clicking the &quot;edit labels&quot; option on a node.<br>
<img src="https://relentlesstorm.github.io/content/images/posts/edit-labels.png" alt="Using node selector to assign virtual machines to a node" loading="lazy"></p>
<h2 id="add-a-nodeselector-field-to-the-virtual-machine">Add a nodeSelector field to the virtual machine</h2>
<p>A node selector can be added through openshift web console.</p>
<p><img src="https://relentlesstorm.github.io/content/images/posts/node-selector-1.png" alt="Using node selector to assign virtual machines to a node" loading="lazy"></p>
<p><img src="https://relentlesstorm.github.io/content/images/posts/nodeselector.png" alt="Using node selector to assign virtual machines to a node" loading="lazy"></p>
<pre><code class="language-shell">$ oc describe vm vm1
    Spec:
      Node Selector:
        Worker - Node - Name:            w1
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Deploy application with kubernetes statefulset]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>StatefulSet is the workload API object used to manage stateful applications.</p>
<p>Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are</p>]]></description><link>https://relentlesstorm.github.io/deploy-application-with-kubernetes-statefulset/</link><guid isPermaLink="false">63b4b0631527a00d97449efe</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Thu, 29 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372525724-16cede94db0b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1667372525724-16cede94db0b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Deploy application with kubernetes statefulset"><p>StatefulSet is the workload API object used to manage stateful applications.</p>
<p>Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p>
<p>If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.</p>
<p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">Source</a></p>
<p>The example below demonstrates the components of a StatefulSet.</p>
<pre><code class="language-shell">$ cat perfbench-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: perfbench
spec:
  serviceName: perfbench
  replicas: 1
  selector:
    matchLabels:
      app: perfbench
  template:
    metadata:
      labels:
        app: perfbench
    spec:
      containers:
      - name: perfbench
        image: noname/perfbench:latest
        volumeMounts:
        - name: perfbench-data
          mountPath: /perfdata
        - name: perfbench-log
          mountPath: /perflog
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: perfbench-data
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 20Gi
  - metadata:
      name: perfbench-log
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
</code></pre>
<p>The statefulset can be created as below.</p>
<pre><code class="language-shell">$ kubectl apply -f perfbench-statefulset.yaml
</code></pre>
<p>In case of any failure to create the statefulset, you can check the events with the following commands.</p>
<pre><code class="language-shell">$ kubectl describe pod perfbench-0
Name:         perfbench-0
Namespace:    default
Priority:     0
Node:         &lt;hostname&gt;/&lt;ip-address&gt;
Start Time:   Sat, 26 Feb 2022 06:22:27 +0000
Labels:       app=perfbench
              controller-revision-hash=perfbench-7657fb8779
              statefulset.kubernetes.io/pod-name=perfbench-0
Annotations:  cni.projectcalico.org/containerID: c8569d9a3f01f546ca92fb2d0cba98d4d971933e53ab83373ed34c91040d92bc
              cni.projectcalico.org/podIP: 192.168.201.145/32
              cni.projectcalico.org/podIPs: 192.168.201.145/32
Status:       Running
IP:           192.168.201.145
IPs:
  IP:           192.168.201.145
Controlled By:  StatefulSet/perfbench
Containers:
  perfbench:
    Container ID:   docker://b6a2c838837395c1c85913d4735e3167d3cea6b5ed3ade276584461d643eaee5
    Image:          noname/perfbench:latest
    Image ID:       docker-pullable://&lt;noname/perfbench@sha256:5460e3c04ea972afcde3db092b514919867f87974d012f046c538ac816c7aaae
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Sat, 26 Feb 2022 06:22:35 +0000
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /perfdata from perfbench-data (rw)
      /perflog from perfbench-log (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x8qhw (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  perfbench-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  perfbench-data-perfbench-0
    ReadOnly:   false
  perfbench-log:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  perfbench-log-perfbench-0
    ReadOnly:   false
  kube-api-access-x8qhw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  20s (x2 over 21s)  default-scheduler  0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims.
  Normal   Scheduled         18s                default-scheduler  Successfully assigned default/perfbench-0 to &lt;hostname&gt;
  Normal   Pulling           16s                kubelet            Pulling image &quot;&lt;noname/perfbench:latest&quot;
  Normal   Pulled            11s                kubelet            Successfully pulled image &quot;&lt;noname/perfbench:latest&quot; in 4.368378217s
  Normal   Created           10s                kubelet            Created container perfbench
  Normal   Started           10s                kubelet            Started container perfbench
</code></pre>
<p>You can check the pod status and login to the pod container as below.</p>
<pre><code class="language-shell">$ kubectl get pod
NAME         READY   STATUS    RESTARTS   AGE
perfbench-0   1/1     Running   0          3m44s

$ kubectl exec -it perfbench-0 -- bash
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/containers/images/">https://kubernetes.io/docs/concepts/containers/images/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Keep a docker container running and not exiting]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>In the kubernetes environment, we can keep a container(pod) alive and avoid it exits immediately after starting.</p>
<h1 id="method-one">Method one</h1>
<ol>
<li>
<p>Use a long-time-run command in Dockerfile</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
</code></pre>
<p>or</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep infinity&</code></pre></li></ol>]]></description><link>https://relentlesstorm.github.io/keep-a-docker-container-running-and-not-exiting/</link><guid isPermaLink="false">63b4b02a1527a00d97449ef1</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 28 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372335937-d03be6fb0a9c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1667372335937-d03be6fb0a9c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Keep a docker container running and not exiting"><p>In the kubernetes environment, we can keep a container(pod) alive and avoid it exits immediately after starting.</p>
<h1 id="method-one">Method one</h1>
<ol>
<li>
<p>Use a long-time-run command in Dockerfile</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
</code></pre>
<p>or</p>
<pre><code class="language-shell">CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;]
</code></pre>
</li>
<li>
<p>Build the docker image and push to docker repository</p>
</li>
<li>
<p>Launch the container</p>
<pre><code class="language-shell">$ kubectl run mycontainer -it --image=&lt;docker-image-name&gt;
</code></pre>
</li>
</ol>
<h1 id="method-two">Method two</h1>
<p>When to deploy an application with kubernetes statefulset, we also can add it to the statefulset yaml file instead of adding it to the docker image through Dockerfile.</p>
<pre><code class="language-shell">$ cat myapp.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp
spec:
  serviceName: myapp
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: noname/myapp:latest
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
        imagePullPolicy: Always
        volumeMounts:
        - name: myapp-data
          mountPath: /data
        - name: myapp-log
          mountPath: /log
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: myapp-data
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 20Gi
  - metadata:
      name: myapp-log
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi

$ kubectl apply -f myapp.yaml          
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Hosting the Ghost blog on GitHub]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h1 id="install-ghost-locally">Install Ghost locally</h1>
<p><a href="https://pages.github.com/">GitHub Pages</a> is a great solution for hosting static website. If you use the Ghost to manage your website, you can <a href="https://ghost.org/docs/install/local/">install Ghost locally</a> and convert it to a static website in order to host it on Github.</p>
<h2 id="install-ubuntu-virtual-machine-on-windowsoptional">Install Ubuntu virtual machine on Windows(optional)</h2>
<h2 id="install-ghost-cli">Install Ghost-CLI</h2>]]></description><link>https://relentlesstorm.github.io/ghost-github/</link><guid isPermaLink="false">63afb59696dbf412f1f80301</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 27 Dec 2022 04:07:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1543616991-75a2c125ff5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1MXx8YmxvZ3xlbnwwfHx8fDE2NzI0NjIyOTE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h1 id="install-ghost-locally">Install Ghost locally</h1>
<img src="https://images.unsplash.com/photo-1543616991-75a2c125ff5b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1MXx8YmxvZ3xlbnwwfHx8fDE2NzI0NjIyOTE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Hosting the Ghost blog on GitHub"><p><a href="https://pages.github.com/">GitHub Pages</a> is a great solution for hosting static website. If you use the Ghost to manage your website, you can <a href="https://ghost.org/docs/install/local/">install Ghost locally</a> and convert it to a static website in order to host it on Github.</p>
<h2 id="install-ubuntu-virtual-machine-on-windowsoptional">Install Ubuntu virtual machine on Windows(optional)</h2>
<h2 id="install-ghost-cli">Install Ghost-CLI</h2>
<p>Ghost-CLI is a commandline tool to help you get Ghost installed and configured for use, quickly and easily.</p>
<pre><code>$ npm install ghost-cli@latest -g
</code></pre>
<p>Ghost runs as background process and remains running until you stop or restart it. The following are some useful commands:</p>
<pre><code>ghost help
ghost ls
ghost log
ghost stop
ghost start
</code></pre>
<h2 id="install-ghost">Install Ghost</h2>
<pre><code>$ mkdir my-ghost-website
$ cd my-ghost-website
$ ghost install local
</code></pre>
<p>Once the Ghost is installed you can access the website on <a href="https://relentlesstorm.github.io/">https://relentlesstorm.github.io</a> and <a href="https://relentlesstorm.github.io/ghost">https://relentlesstorm.github.io/ghost</a> for Ghost Admin.</p>
<h1 id="generate-the-static-website">Generate the static website</h1>
<p>In order to generate the static website, we&apos;ll use the Ghost static site generator. For the Linux and macOS, this tool can be used directly.</p>
<pre><code>$ sudo npm install -g ghost-static-site-generator
</code></pre>
<p>Now, we can push the generated static pages to the Github repository named <strong>username.github.io</strong>.</p>
<p>To generate the static pages, we can run the following command. The static pages are generated in a folder called <em>static</em>.</p>
<pre><code>$ gssg --url https://username.github.io
</code></pre>
<h1 id="push-the-static-pages-to-github">Push the static pages to Github</h1>
<p>Before pushing the static pages to Github, a repository called <em>username.github.io</em> should be created on Github website.</p>
<p>The static pages can be pushed to Github repository as below.</p>
<pre><code>$ cd static
$ git init .
$ git remote add origin https://github.com/username/username.github.io.git
$ git branch -M main
$ git add .
$ git commit -m &apos;Init my website&apos;
$ git push -u origin main
</code></pre>
<p>After the push, Github will build and deploy the pages automatically. And the updated website will be available to access in a few minutes.</p>
<h1 id="reference">Reference</h1>
<ul>
<li><a href="https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl">https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl</a></li>
<li><a href="https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text=Install%20and%20run%20Ghost%20locally,static%20website%20to%20GitHub%20Pages">https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text=Install and run Ghost locally,static website to GitHub Pages</a></li>
<li><a href="https://ghost.org/docs/reinstall/">https://ghost.org/docs/reinstall/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Pull an image from a private docker registry in Kubernetes Pod]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="log-in-to-docker-hub">Log in to Docker Hub</h2>
<p>In order to pull a image from Docker Hub, you must authenticate with a registry. Use <strong>docker</strong> tool to log in to the Docker Hub as below. A username and password is needed to log in.</p>
<pre><code class="language-shell">$ docker login
</code></pre>
<p>The login process creates or updates the</p>]]></description><link>https://relentlesstorm.github.io/pull-an-image-from-a-private-docker-registry-in-kubernetes-pod/</link><guid isPermaLink="false">63b4afde1527a00d97449ee4</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 27 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372459510-55b5e2087cd0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="log-in-to-docker-hub">Log in to Docker Hub</h2>
<img src="https://images.unsplash.com/photo-1667372459510-55b5e2087cd0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxrdWJlcm5ldGVzfGVufDB8fHx8MTY3Mjc3NzAxNg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Pull an image from a private docker registry in Kubernetes Pod"><p>In order to pull a image from Docker Hub, you must authenticate with a registry. Use <strong>docker</strong> tool to log in to the Docker Hub as below. A username and password is needed to log in.</p>
<pre><code class="language-shell">$ docker login
</code></pre>
<p>The login process creates or updates the config.json file which holds an authorization token.</p>
<pre><code class="language-shell">$ cat /root/.docker/config.json
{
	&quot;auths&quot;: {
		&quot;https://index.docker.io/v1/&quot;: {
			&quot;auth&quot;: &quot;xxx=&quot;
		}
	}
}
</code></pre>
<h2 id="create-a-secret-based-on-existing-credentials">Create a Secret based on existing credentials</h2>
<p>A Kubernetes cluster uses the Secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.</p>
<p>If you already ran docker login, you can copy that credential into Kubernetes:</p>
<pre><code class="language-shell">$ kubectl create secret generic regcred --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson
</code></pre>
<p>You can inspect the Secret as below.</p>
<pre><code class="language-shell">$ kubectl get secret regcred --output=yaml

apiVersion: v1
data:
  .dockerconfigjson: &lt;base64-formatted-docker-credentials&gt;
kind: Secret
metadata:
  creationTimestamp: &quot;2022-02-28T22:25:43Z&quot;
  name: regcred
  namespace: default
  resourceVersion: &quot;1503624&quot;
  uid: yyy
type: kubernetes.io/dockerconfigjson
</code></pre>
<p>The value of the .dockerconfigjson field is a base64 representation of your Docker credentials. To understand what is in the .dockerconfigjson field, convert the secret data to a readable format:</p>
<pre><code class="language-shell">$ kubectl get secret regcred --output=&quot;jsonpath={.data.\.dockerconfigjson}&quot; | base64 --decode
{
	&quot;auths&quot;: {
		&quot;https://index.docker.io/v1/&quot;: {
			&quot;auth&quot;: &quot;xxx=&quot;
		}
	}
}
</code></pre>
<h2 id="create-a-pod-that-uses-the-secret-to-pull-image">Create a Pod that uses the Secret to pull image</h2>
<pre><code class="language-shell">$ vi my-private-reg-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: &lt;your-private-image&gt;
  imagePullSecrets:
  - name: regcred

$ kubectl apply -f my-private-reg-pod.yaml
$ kubectl get pod private-reg  
</code></pre>
<p>Note that the <strong>imagePullSecrets</strong> field specifies that Kubernetes should get the credentials from a Secret named <em>regcred</em> in order to pull a container image from Docker Hub.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Deploy kubernetes statefulset with affinity]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Here is an example to demonstrate how to assign statefulset and pod to the target worker node.</p>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">affinity</a> helps define which node will be assigned for the pod by using the node labels.</p>
<pre><code class="language-shell">$ kubectl get nodes -o wide --show-labels

$ kubectl label nodes &lt;node-name&gt; portworx.io/fio=true

$ cat</code></pre>]]></description><link>https://relentlesstorm.github.io/deploy-kubernetes-statefulset-with-affinity/</link><guid isPermaLink="false">63b495a11527a00d97449da1</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 27 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1605745341075-1b7460b99df8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRvY2tlcnxlbnwwfHx8fDE2NzI3NzcyNzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1605745341075-1b7460b99df8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRvY2tlcnxlbnwwfHx8fDE2NzI3NzcyNzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Deploy kubernetes statefulset with affinity"><p>Here is an example to demonstrate how to assign statefulset and pod to the target worker node.</p>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">affinity</a> helps define which node will be assigned for the pod by using the node labels.</p>
<pre><code class="language-shell">$ kubectl get nodes -o wide --show-labels

$ kubectl label nodes &lt;node-name&gt; portworx.io/fio=true

$ cat statefulset-node-select.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: fiobench
spec:
  serviceName: fiobench
  replicas: 1
  selector:
    matchLabels:
      app: fiobench
  template:
    metadata:
      labels:
        app: fiobench
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: portworx.io/fio
                operator: In
                values:
                - &quot;true&quot;
      containers:
      - name: fiobench
        image: pwxbuild/perfbench:fio
        imagePullPolicy: Always
        volumeMounts:
        - name: fiobench-data-1
          mountPath: /fiodata1
        - name: fiobench-data-2
          mountPath: /fiodata2
        securityContext:
          privileged: true
      imagePullSecrets:
      - name: regcred
  volumeClaimTemplates:
  - metadata:
      name: fiobench-data-1
    spec:
      storageClassName: fio-repl-node-select
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1400Gi
  - metadata:
      name: fiobench-data-2
    spec:
      storageClassName: fio-repl-node-select
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1400Gi

$ kubectl apply -f statefulset-node-select.yaml
$ kubectl get statefulset
$ kubectl get pod -o wide
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using kubeconfig to configure access to remote Kubernetes cluster]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-kubectl-binary">Install kubectl binary</h2>
<p>The Kubernetes command-line tool, <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.</p>
<pre><code class="language-shell">[root@localhost ~]# curl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.</code></pre>]]></description><link>https://relentlesstorm.github.io/using-kubeconfig-to-configure-access-to-remote-kubernetes-cluster/</link><guid isPermaLink="false">63b48cf81527a00d97449c75</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 26 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1667372459534-848ec00d4da7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-kubectl-binary">Install kubectl binary</h2>
<img src="https://images.unsplash.com/photo-1667372459534-848ec00d4da7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNjcyNzc3MDE2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using kubeconfig to configure access to remote Kubernetes cluster"><p>The Kubernetes command-line tool, <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.</p>
<pre><code class="language-shell">[root@localhost ~]# curl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;
[root@localhost ~]# sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
[root@localhost ~]# ls -la /usr/local/bin/kubectl
[root@localhost ~]# kubectl version --output=yaml
clientVersion:
  buildDate: &quot;2022-09-21T14:33:49Z&quot;
  compiler: gc
  gitCommit: 5835544ca568b757a8ecae5c153f317e5736700e
  gitTreeState: clean
  gitVersion: v1.25.2
  goVersion: go1.19.1
  major: &quot;1&quot;
  minor: &quot;25&quot;
  platform: linux/amd64
kustomizeVersion: v4.5.7

The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
<h2 id="use-kubeconfig-file-to-access-remote-kubernetes-cluster">Use kubeconfig file to access remote Kubernetes cluster</h2>
<p>A <em>kubeconfig</em> is a YAML file with all the Kubernetes cluster details, certificate, and secret token to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using managed Kubernetes cluster. A <em>kubeconfig</em> file is a file to configure access to Kubernetes when to use with <em>kubectl</em> cli tool.</p>
<p>When to deploy Kubernetes cluster, a <em>kubeconfig</em> is automatically generated. It&apos;s saved in ~/.kube/config drectory.</p>
<pre><code class="language-shell">[root@host1 ~]#  ls -la ~/.kube/config
-rw------- 1 root root 5577 May  9 02:25 /root/.kube/config
</code></pre>
<p>You can access the Kubernetes cluster by providing the kubeconfig file remotely.</p>
<pre><code class="language-shell">[root@localhost ~]# scp host1:/root/.kube/config ./kubeconfig-remote
[root@localhost ~]# kubectl --kubeconfig=./kubeconfig-remote get nodes
NAME            STATUS   ROLES    AGE    VERSION
host0   Ready    &lt;none&gt;   135d   v1.19.2
host1   Ready    &lt;none&gt;   135d   v1.19.2
host2   Ready    &lt;none&gt;   135d   v1.19.2
host3   Ready    master   135d   v1.19.2
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Lambda architecture in big data system]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="what-is-lambda-architecture">What is Lambda Architecture</h2>
<p>When working with very large data sets, it can take a long time to run the sort of queries that clients need. These queries can&apos;t be performed in real time, and often require algorithms such as MapReduce that operate in parallel across the entire</p>]]></description><link>https://relentlesstorm.github.io/lambda-architecture-in-big-data-system/</link><guid isPermaLink="false">63b497f11527a00d97449dea</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 25 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1609788063095-d71bf3c1f01f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDYzfHxiaWclMjBkYXRhfGVufDB8fHx8MTY3Mjc3OTg0Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="what-is-lambda-architecture">What is Lambda Architecture</h2>
<img src="https://images.unsplash.com/photo-1609788063095-d71bf3c1f01f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDYzfHxiaWclMjBkYXRhfGVufDB8fHx8MTY3Mjc3OTg0Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Lambda architecture in big data system"><p>When working with very large data sets, it can take a long time to run the sort of queries that clients need. These queries can&apos;t be performed in real time, and often require algorithms such as MapReduce that operate in parallel across the entire data set. The results are then stored separately from the raw data and used for querying.</p>
<p>One drawback to this approach is that it introduces latency &#x2014; if processing takes a few hours, a query may return results that are several hours old. Ideally, you would like to get some results in real time (perhaps with some loss of accuracy), and combine these results with the results from the batch analytics.</p>
<p>The lambda architecture, first proposed by Nathan Marz, addresses this problem by creating two paths for data flow. All data coming into the system goes through these two paths:</p>
<ul>
<li>A batch layer (cold path) stores all of the incoming data in its raw form and performs batch processing on the data. The result of this processing is stored as a batch view.</li>
<li>A speed layer (hot path) analyzes data in real time. This layer is designed for low latency, at the expense of accuracy.</li>
</ul>
<p>The batch layer feeds into a serving layer that indexes the batch view for efficient querying. The speed layer updates the serving layer with incremental updates based on the most recent data.</p>
<p><img src="https://relentlesstorm.github.io/content/images/posts/lambda.png" alt="Lambda architecture in big data system" loading="lazy">{:.shadow}</p>
<p><a href="https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/">Source</a></p>
<p>Data that flows into the hot path is constrained by latency requirements imposed by the speed layer, so that it can be processed as quickly as possible. Often, this requires a tradeoff of some level of accuracy in favor of data that is ready as quickly as possible. For example, consider an IoT scenario where a large number of temperature sensors are sending telemetry data. The speed layer may be used to process a sliding time window of the incoming data.</p>
<p>Data flowing into the cold path, on the other hand, is not subject to the same low latency requirements. This allows for high accuracy computation across large data sets, which can be very time intensive.</p>
<p>Eventually, the hot and cold paths converge at the analytics client application. If the client needs to display timely, yet potentially less accurate data in real time, it will acquire its result from the hot path. Otherwise, it will select results from the cold path to display less timely but more accurate data. In other words, the hot path has data for a relatively small window of time, after which the results can be updated with more accurate data from the cold path.</p>
<p>The raw data stored at the batch layer is immutable. Incoming data is always appended to the existing data, and the previous data is never overwritten. Any changes to the value of a particular datum are stored as a new timestamped event record. This allows for recomputation at any point in time across the history of the data collected. The ability to recompute the batch view from the original raw data is important, because it allows for new views to be created as the system evolves.</p>
<h2 id="batch-layer">Batch Layer</h2>
<p>New data comes continuously, as a feed to the data system. It gets fed to the batch layer and the speed layer simultaneously. It looks at all the data at once and eventually corrects the data in the stream layer.  Here we can find lots of ETL and a traditional data warehouse. This layer is built using a predefined schedule, usually once or twice a day. The batch layer has two very important functions:</p>
<ul>
<li>To manage the master dataset</li>
<li>To pre-compute the batch views.</li>
</ul>
<h2 id="serving-layer">Serving Layer</h2>
<p>The outputs from the batch layer in the form of batch views and those coming from the speed layer in the form of near real-time views get forwarded to the serving.  This layer indexes the batch views so that they can be queried in low-latency on an ad-hoc basis.</p>
<h2 id="speed-layer-stream-layer">Speed Layer (Stream Layer)</h2>
<p>This layer handles the data that are not already delivered in the batch view due to the latency of the batch layer. In addition, it only deals with recent data in order to provide a complete view of the data to the user by creating real-time views.</p>
<h2 id="benefits-of-lambda-architectures">Benefits of lambda architectures</h2>
<p>Here are the main benefits of lambda architectures:</p>
<ul>
<li>No Server Management &#x2013; you do not have to install, maintain, or administer any software.</li>
<li>Flexible Scaling &#x2013; your application can be either automatically scaled or scaled by the adjustment of its capacity</li>
<li>Automated High Availability &#x2013; refers to the fact that serverless applications have already built-in availability and faults tolerance. It represents a guarantee that all requests will get a response about whether they were successful or not.</li>
<li>Business Agility &#x2013; React in real-time to changing business/market scenarios</li>
</ul>
<h2 id="challenges-with-lambda-architectures">Challenges with lambda architectures</h2>
<ul>
<li>Complexity &#x2013; lambda architectures can be highly complex. Administrators must typically maintain two separate code bases for batch and streaming layers, which can make debugging difficult.</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li>&lt;Big Data - Priciples and best practices of scalable real-time data systems&gt;</li>
<li><a href="https://databricks.com/glossary/lambda-architecture">https://databricks.com/glossary/lambda-architecture</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/">https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Use sysbench for CockroachDB performance benchmarking]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro">Intro</h2>
<p>Cockroach uses TPC-C as the official OLTP workload benchmark since it&apos;s a more realistic measurement by modeling the real world applications.</p>
<p>However, <a href="https://github.com/akopytov/sysbench">sysbench</a> is a straight-forward throughput/latency benchmarking tool. It is a scriptable multi-threaded benchmark tool based on LuaJIT. It is most frequently used for database</p>]]></description><link>https://relentlesstorm.github.io/crdb-sysbench/</link><guid isPermaLink="false">63afb5b296dbf412f1f8030c</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Thu, 08 Dec 2022 04:08:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1597852074816-d933c7d2b988?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro">Intro</h2>
<img src="https://images.unsplash.com/photo-1597852074816-d933c7d2b988?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Use sysbench for CockroachDB performance benchmarking"><p>Cockroach uses TPC-C as the official OLTP workload benchmark since it&apos;s a more realistic measurement by modeling the real world applications.</p>
<p>However, <a href="https://github.com/akopytov/sysbench">sysbench</a> is a straight-forward throughput/latency benchmarking tool. It is a scriptable multi-threaded benchmark tool based on LuaJIT. It is most frequently used for database benchmarks, but can also be used to create arbitrarily complex workloads that do not involve a database server.</p>
<ul>
<li>oltp_*.lua: a collection of OLTP-like database benchmarks</li>
<li>fileio: a filesystem-level benchmark</li>
<li>cpu: a simple CPU benchmark</li>
<li>memory: a memory access benchmark</li>
<li>threads: a thread-based scheduler benchmark</li>
<li>mutex: a POSIX mutex benchmark</li>
</ul>
<p>This Cockroach <a href="https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/">blog</a> explains why we use sysbench for competitive benchmarks.</p>
<p>Sysbench contains a collection of simple SQL workloads. These workloads perform low-level SQL operations. For example, they run concurrent INSERT or UPDATE statements on rows as fast as possible. It gives us a picture on the system performance under different access patterns. Unlike TPC-C, Sysbench does not attempt to model a real application.</p>
<p>Sysbench includes the following workloads:</p>
<ul>
<li>oltp_point_select: single-row point selects</li>
<li>oltp_insert: single-row inserts</li>
<li>oltp_delete: single-row deletes</li>
<li>oltp_update_index: single-row update on column that requires update to secondary index</li>
<li>oltp_update_non_index single-row: update on column that does not require update to secondary index</li>
<li>oltp_read_only: transactions that run collection of small scans</li>
<li>oltp_read_write: transactions that run collection of small scans and writes</li>
<li>oltp_write_only: transactions that run collection of writes</li>
</ul>
<p>Sysbench supports the following two database drivers.</p>
<ul>
<li>mysql - MySQL driver</li>
<li>pgsql - PostgreSQL driver</li>
</ul>
<p>From this Cockroach <a href="https://www.cockroachlabs.com/blog/why-postgres/">blog</a>, CockroachDB is compatible with PostgreSQL.</p>
<p>In this article, we will explore how to run sysbench with CockroachDB using pqsql driver.</p>
<h2 id="install-the-cockroachdb-cluster">Install the CockroachDB cluster</h2>
<p>Refer to this [post]({% post_url 2022-09-01-cockroachdb-tpcc %}) on how to deploy CockroachDB cluster.</p>
<pre><code class="language-shell">[root@crdb_node1 ~]# cockroach version
Build Tag:        v22.1.6
Build Time:       2022/08/23 17:05:04
Distribution:     CCL
Platform:         linux amd64 (x86_64-pc-linux-gnu)
Go Version:       go1.17.11
C Compiler:       gcc 6.5.0
Build Commit ID:  760a8253ae6478d69da0330133e3efec8e950e4e
Build Type:       release
</code></pre>
<h2 id="interact-with-the-cockroachdb">Interact with the CockroachDB</h2>
<h3 id="use-the-cockroachdb-built-in-client">Use the CockroachDB built-in client</h3>
<p>CockroachDB comes with a built-in client for executing SQL statements from an interactive shell or directly from the command line.</p>
<p>To use this client, run the cockroach sql command as following:</p>
<pre><code class="language-shell">[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;create database testdb&apos;

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;show databases&apos;
  database_name | owner | primary_region | regions | survival_goal
----------------+-------+----------------+---------+----------------
  defaultdb     | root  | NULL           | {}      | NULL
  postgres      | root  | NULL           | {}      | NULL
  system        | node  | NULL           | {}      | NULL
  testdb        | root  | NULL           | {}      | NULL
(4 rows)

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;show tables from testdb&apos;
SHOW TABLES 0

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;drop database testdb&apos;
</code></pre>
<h3 id="use-the-postgres-client">Use the Postgres client</h3>
<p>The cockroachDB can also be interacted by using Postgres client.</p>
<p>To install the Postgres client:</p>
<pre><code class="language-shell">[root@node0 ~]# yum -y install postgresql postgresql-libs
Installed:
  postgresql.x86_64 0:9.2.24-8.el7_9
</code></pre>
<p>To create the database and user:</p>
<pre><code class="language-shell">[root@node0 ~]# psql -h node1 -U root -p 26257
psql (9.2.24, server 13.0.0)

root=&gt; create database testdb;
root=&gt; create user tester;
root=&gt; grant all on database testdb to tester;

root=&gt; show databases;
 database_name | owner | primary_region | regions | survival_goal
---------------+-------+----------------+---------+---------------
 defaultdb     | root  |                | {}      |
 postgres      | root  |                | {}      |
 system        | node  |                | {}      |
 testdb        | root  |                | {}      |
(4 rows)

root=&gt; show users;
 username | options | member_of
----------+---------+-----------
 admin    |         | {}
 root     |         | {admin}
 tester   |         | {}
(3 rows)

root=&gt; \c testdb;
psql (9.2.24, server 13.0.0)
WARNING: psql version 9.2, server version 13.0.
         Some psql features might not work.
You are now connected to database &quot;testdb&quot; as user &quot;root&quot;.

testdb=&gt; \dt;
                          List of relations
    Schema    |       Name        | Type  |          Owner
--------------+-------------------+-------+--------------------------
 pg_extension | geography_columns | table | unknown (OID=3233629770)
 pg_extension | geometry_columns  | table | unknown (OID=3233629770)
 pg_extension | spatial_ref_sys   | table | unknown (OID=3233629770)
(3 rows)

testdb=&gt;  SELECT * FROM pg_catalog.pg_tables where schemaname != &apos;pg_catalog&apos; AND schemaname != &apos;information_schema&apos; and schemaname != &apos;crdb_internal&apos;;
  schemaname  |     tablename     | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity
--------------+-------------------+------------+------------+------------+----------+-------------+-------------
 pg_extension | geography_columns | node       |            | f          | f        | f           | f
 pg_extension | geometry_columns  | node       |            | f          | f        | f           | f
 pg_extension | spatial_ref_sys   | node       |            | f          | f        | f           | f
(3 rows)
</code></pre>
<h2 id="install-sysbench">Install sysbench</h2>
<p>To install sysbench:</p>
<pre><code class="language-shell">[root@node0 ~]# curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash^C
[root@node0 ~]# sudo yum -y install sysbench
[root@node0 ~]# sysbench --version
sysbench 1.0.20
</code></pre>
<p>To get familiar with the sysbench parameters:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench --help
Usage:
  sysbench [options]... [testname] [command]

Commands implemented by most tests: prepare run cleanup help

General options:
  --threads=N                     number of threads to use [1]
  --events=N                      limit for total number of events [0]
  --time=N                        limit for total execution time in seconds [10]
  --forced-shutdown=STRING        number of seconds to wait after the --time limit before forcing shutdown, or &apos;off&apos; to disable [off]
  --thread-stack-size=SIZE        size of stack per thread [64K]
  --rate=N                        average transactions rate. 0 for unlimited rate [0]
  --report-interval=N             periodically report intermediate statistics with a specified interval in seconds. 0 disables intermediate reports [0]
  --report-checkpoints=[LIST,...] dump full statistics and reset all counters at specified points in time. The argument is a list of comma-separated values representing the amount of time in seconds elapsed from start of test when report checkpoint(s) must be performed. Report checkpoints are off by default. []
  --debug[=on|off]                print more debugging info [off]
  --validate[=on|off]             perform validation checks where possible [off]
  --help[=on|off]                 print help and exit [off]
  --version[=on|off]              print version and exit [off]
  --config-file=FILENAME          File containing command line options
  --tx-rate=N                     deprecated alias for --rate [0]
  --max-requests=N                deprecated alias for --events [0]
  --max-time=N                    deprecated alias for --time [0]
  --num-threads=N                 deprecated alias for --threads [1]

Pseudo-Random Numbers Generator options:
  --rand-type=STRING random numbers distribution {uniform,gaussian,special,pareto} [special]
  --rand-spec-iter=N number of iterations used for numbers generation [12]
  --rand-spec-pct=N  percentage of values to be treated as &apos;special&apos; (for special distribution) [1]
  --rand-spec-res=N  percentage of &apos;special&apos; values to use (for special distribution) [75]
  --rand-seed=N      seed for random number generator. When 0, the current time is used as a RNG seed. [0]
  --rand-pareto-h=N  parameter h for pareto distribution [0.2]

Log options:
  --verbosity=N verbosity level {5 - debug, 0 - only critical messages} [3]

  --percentile=N       percentile to calculate in latency statistics (1-100). Use the special value of 0 to disable percentile calculations [95]
  --histogram[=on|off] print latency histogram in report [off]

General database options:

  --db-driver=STRING  specifies database driver to use (&apos;help&apos; to get list of available drivers) [mysql]
  --db-ps-mode=STRING prepared statements usage mode {auto, disable} [auto]
  --db-debug[=on|off] print database-specific debug information [off]


Compiled-in database drivers:
  mysql - MySQL driver
  pgsql - PostgreSQL driver

mysql options:
  --mysql-host=[LIST,...]          MySQL server host [localhost]
  --mysql-port=[LIST,...]          MySQL server port [3306]
  --mysql-socket=[LIST,...]        MySQL socket
  --mysql-user=STRING              MySQL user [sbtest]
  --mysql-password=STRING          MySQL password []
  --mysql-db=STRING                MySQL database name [sbtest]
  --mysql-ssl[=on|off]             use SSL connections, if available in the client library [off]
  --mysql-ssl-cipher=STRING        use specific cipher for SSL connections []
  --mysql-compression[=on|off]     use compression, if available in the client library [off]
  --mysql-debug[=on|off]           trace all client library calls [off]
  --mysql-ignore-errors=[LIST,...] list of errors to ignore, or &quot;all&quot; [1213,1020,1205]
  --mysql-dry-run[=on|off]         Dry run, pretend that all MySQL client API calls are successful without executing them [off]

pgsql options:
  --pgsql-host=STRING     PostgreSQL server host [localhost]
  --pgsql-port=N          PostgreSQL server port [5432]
  --pgsql-user=STRING     PostgreSQL user [sbtest]
  --pgsql-password=STRING PostgreSQL password []
  --pgsql-db=STRING       PostgreSQL database name [sbtest]

Compiled-in tests:
  fileio - File I/O test
  cpu - CPU performance test
  memory - Memory functions speed test
  threads - Threads subsystem performance test
  mutex - Mutex performance test

See &apos;sysbench &lt;testname&gt; help&apos; for a list of options for each test.
</code></pre>
<p>Sysbench includes the following lua scripts to simulate OLTP workloads.</p>
<pre><code class="language-shell">[root@node0 ~]# ls -l /usr/share/sysbench/
total 60
-rwxr-xr-x   1 root root  1452 Apr 24  2020 bulk_insert.lua
-rw-r--r--   1 root root 14369 Apr 24  2020 oltp_common.lua
-rwxr-xr-x   1 root root  1290 Apr 24  2020 oltp_delete.lua
-rwxr-xr-x   1 root root  2415 Apr 24  2020 oltp_insert.lua
-rwxr-xr-x   1 root root  1265 Apr 24  2020 oltp_point_select.lua
-rwxr-xr-x   1 root root  1649 Apr 24  2020 oltp_read_only.lua
-rwxr-xr-x   1 root root  1824 Apr 24  2020 oltp_read_write.lua
-rwxr-xr-x   1 root root  1118 Apr 24  2020 oltp_update_index.lua
-rwxr-xr-x   1 root root  1127 Apr 24  2020 oltp_update_non_index.lua
-rwxr-xr-x   1 root root  1440 Apr 24  2020 oltp_write_only.lua
-rwxr-xr-x   1 root root  1919 Apr 24  2020 select_random_points.lua
-rwxr-xr-x   1 root root  2118 Apr 24  2020 select_random_ranges.lua
drwxr-xr-x   4 root root    49 Dec  8 20:39 tests

[root@node0 ~]# ls -l /usr/share/sysbench/tests/include/oltp_legacy/
total 52
-rw-r--r-- 1 root root 1195 Apr 24  2020 bulk_insert.lua
-rw-r--r-- 1 root root 4696 Apr 24  2020 common.lua
-rw-r--r-- 1 root root  366 Apr 24  2020 delete.lua
-rw-r--r-- 1 root root 1171 Apr 24  2020 insert.lua
-rw-r--r-- 1 root root 3004 Apr 24  2020 oltp.lua
-rw-r--r-- 1 root root  368 Apr 24  2020 oltp_simple.lua
-rw-r--r-- 1 root root  527 Apr 24  2020 parallel_prepare.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 select.lua
-rw-r--r-- 1 root root 1448 Apr 24  2020 select_random_points.lua
-rw-r--r-- 1 root root 1556 Apr 24  2020 select_random_ranges.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 update_index.lua
-rw-r--r-- 1 root root  578 Apr 24  2020 update_non_index.lua
</code></pre>
<p>To get more options for each specific lua workload:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_insert.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_insert.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_write_only.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_read_only.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_read_only.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_point_select.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_point_select.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &apos;redshift&apos;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&apos;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]
</code></pre>
<h2 id="prepare-for-the-benchmark">Prepare for the benchmark</h2>
<p>Before running the benchmark, the database should be created and the data tables should be populated.</p>
<p>To create the database:</p>
<pre><code class="language-shell">[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &apos;create database testdb;&apos;
</code></pre>
<p>To populate the database:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --db-driver=pgsql prepare
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Creating table &apos;sbtest1&apos;...
Inserting 100000 records into &apos;sbtest1&apos;
Creating a secondary index on &apos;sbtest1&apos;...
Creating table &apos;sbtest2&apos;...
Inserting 100000 records into &apos;sbtest2&apos;
Creating a secondary index on &apos;sbtest2&apos;...
Creating table &apos;sbtest3&apos;...
Inserting 100000 records into &apos;sbtest3&apos;
Creating a secondary index on &apos;sbtest3&apos;...
Creating table &apos;sbtest4&apos;...
Inserting 100000 records into &apos;sbtest4&apos;
Creating a secondary index on &apos;sbtest4&apos;...
Creating table &apos;sbtest5&apos;...
Inserting 100000 records into &apos;sbtest5&apos;
Creating a secondary index on &apos;sbtest5&apos;...
Creating table &apos;sbtest6&apos;...
Inserting 100000 records into &apos;sbtest6&apos;
Creating a secondary index on &apos;sbtest6&apos;...
Creating table &apos;sbtest7&apos;...
Inserting 100000 records into &apos;sbtest7&apos;
Creating a secondary index on &apos;sbtest7&apos;...
Creating table &apos;sbtest8&apos;...
Inserting 100000 records into &apos;sbtest8&apos;
Creating a secondary index on &apos;sbtest8&apos;...
Creating table &apos;sbtest9&apos;...
Inserting 100000 records into &apos;sbtest9&apos;
Creating a secondary index on &apos;sbtest9&apos;...
Creating table &apos;sbtest10&apos;...
Inserting 100000 records into &apos;sbtest10&apos;
Creating a secondary index on &apos;sbtest10&apos;...
Creating table &apos;sbtest11&apos;...
Inserting 100000 records into &apos;sbtest11&apos;
Creating a secondary index on &apos;sbtest11&apos;...
Creating table &apos;sbtest12&apos;...
Inserting 100000 records into &apos;sbtest12&apos;
Creating a secondary index on &apos;sbtest12&apos;...
Creating table &apos;sbtest13&apos;...
Inserting 100000 records into &apos;sbtest13&apos;
Creating a secondary index on &apos;sbtest13&apos;...
Creating table &apos;sbtest14&apos;...
Inserting 100000 records into &apos;sbtest14&apos;
Creating a secondary index on &apos;sbtest14&apos;...
Creating table &apos;sbtest15&apos;...
Inserting 100000 records into &apos;sbtest15&apos;
Creating a secondary index on &apos;sbtest15&apos;...
Creating table &apos;sbtest16&apos;...
Inserting 100000 records into &apos;sbtest16&apos;
Creating a secondary index on &apos;sbtest16&apos;...
Creating table &apos;sbtest17&apos;...
Inserting 100000 records into &apos;sbtest17&apos;
Creating a secondary index on &apos;sbtest17&apos;...
Creating table &apos;sbtest18&apos;...
Inserting 100000 records into &apos;sbtest18&apos;
Creating a secondary index on &apos;sbtest18&apos;...
Creating table &apos;sbtest19&apos;...
Inserting 100000 records into &apos;sbtest19&apos;
Creating a secondary index on &apos;sbtest19&apos;...
Creating table &apos;sbtest20&apos;...
Inserting 100000 records into &apos;sbtest20&apos;
Creating a secondary index on &apos;sbtest20&apos;...
Creating table &apos;sbtest21&apos;...
Inserting 100000 records into &apos;sbtest21&apos;
Creating a secondary index on &apos;sbtest21&apos;...
Creating table &apos;sbtest22&apos;...
Inserting 100000 records into &apos;sbtest22&apos;
Creating a secondary index on &apos;sbtest22&apos;...
Creating table &apos;sbtest23&apos;...
Inserting 100000 records into &apos;sbtest23&apos;
Creating a secondary index on &apos;sbtest23&apos;...
Creating table &apos;sbtest24&apos;...
Inserting 100000 records into &apos;sbtest24&apos;

testdb=&gt;  show tables from testdb;
 schema_name | table_name | type  | owner  | estimated_row_count | locality
-------------+------------+-------+--------+---------------------+----------
 public      | sbtest1    | table | tester |              100000 |
 public      | sbtest10   | table | tester |              100000 |
 public      | sbtest11   | table | tester |              100000 |
 public      | sbtest12   | table | tester |              100000 |
 public      | sbtest13   | table | tester |              100000 |
 public      | sbtest14   | table | tester |              100000 |
 public      | sbtest15   | table | tester |              100000 |
 public      | sbtest16   | table | tester |              100000 |
 public      | sbtest17   | table | tester |              100000 |
 public      | sbtest18   | table | tester |              100000 |
 public      | sbtest19   | table | tester |              100000 |
 public      | sbtest2    | table | tester |              100000 |
 public      | sbtest20   | table | tester |              100000 |
 public      | sbtest21   | table | tester |              100000 |
 public      | sbtest22   | table | tester |              100000 |
 public      | sbtest23   | table | tester |              100000 |
 public      | sbtest24   | table | tester |              100000 |
 public      | sbtest3    | table | tester |              100000 |
 public      | sbtest4    | table | tester |              100000 |
 public      | sbtest5    | table | tester |              100000 |
 public      | sbtest6    | table | tester |              100000 |
 public      | sbtest7    | table | tester |              100000 |
 public      | sbtest8    | table | tester |              100000 |
 public      | sbtest9    | table | tester |              100000 |
(24 rows)

testdb=&gt; select sum(range_size)/1000 from crdb_internal.ranges where database_name=&apos;testdb&apos;;
       ?column?
-----------------------
 602418.53400000000000
(1 row)
</code></pre>
<p>To populate the database with 2 or more threads:</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=root --pgsql-password= --table_size=1000000 --tables=24 --threads=2 --db-driver=pgsql prepare
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Initializing worker threads...

Creating table &apos;sbtest2&apos;...
Creating table &apos;sbtest1&apos;...
Inserting 1000000 records into &apos;sbtest1&apos;
Inserting 1000000 records into &apos;sbtest2&apos;
Creating a secondary index on &apos;sbtest2&apos;...
Creating table &apos;sbtest4&apos;...
Inserting 1000000 records into &apos;sbtest4&apos;
Creating a secondary index on &apos;sbtest1&apos;...
Creating table &apos;sbtest3&apos;...
Inserting 1000000 records into &apos;sbtest3&apos;
Creating a secondary index on &apos;sbtest4&apos;...
Creating a secondary index on &apos;sbtest3&apos;...
Creating table &apos;sbtest6&apos;...
Inserting 1000000 records into &apos;sbtest6&apos;
Creating table &apos;sbtest5&apos;...
Inserting 1000000 records into &apos;sbtest5&apos;
Creating a secondary index on &apos;sbtest5&apos;...
Creating a secondary index on &apos;sbtest6&apos;...
Creating table &apos;sbtest7&apos;...
Inserting 1000000 records into &apos;sbtest7&apos;
Creating table &apos;sbtest8&apos;...
Inserting 1000000 records into &apos;sbtest8&apos;
Creating a secondary index on &apos;sbtest7&apos;...
Creating a secondary index on &apos;sbtest8&apos;...
Creating table &apos;sbtest9&apos;...
Inserting 1000000 records into &apos;sbtest9&apos;
Creating table &apos;sbtest10&apos;...
Inserting 1000000 records into &apos;sbtest10&apos;
Creating a secondary index on &apos;sbtest10&apos;...
Creating table &apos;sbtest12&apos;...
Inserting 1000000 records into &apos;sbtest12&apos;
Creating a secondary index on &apos;sbtest9&apos;...
Creating table &apos;sbtest11&apos;...
Inserting 1000000 records into &apos;sbtest11&apos;
Creating a secondary index on &apos;sbtest12&apos;...
Creating table &apos;sbtest14&apos;...
Inserting 1000000 records into &apos;sbtest14&apos;
Creating a secondary index on &apos;sbtest11&apos;...
Creating table &apos;sbtest13&apos;...
Inserting 1000000 records into &apos;sbtest13&apos;
Creating a secondary index on &apos;sbtest14&apos;...
Creating table &apos;sbtest16&apos;...
Inserting 1000000 records into &apos;sbtest16&apos;
Creating a secondary index on &apos;sbtest13&apos;...
Creating table &apos;sbtest15&apos;...
Inserting 1000000 records into &apos;sbtest15&apos;
Creating a secondary index on &apos;sbtest16&apos;...
Creating table &apos;sbtest18&apos;...
Inserting 1000000 records into &apos;sbtest18&apos;
Creating a secondary index on &apos;sbtest15&apos;...
Creating table &apos;sbtest17&apos;...
Inserting 1000000 records into &apos;sbtest17&apos;
Creating a secondary index on &apos;sbtest18&apos;...
Creating table &apos;sbtest20&apos;...
Inserting 1000000 records into &apos;sbtest20&apos;
Creating a secondary index on &apos;sbtest17&apos;...
Creating table &apos;sbtest19&apos;...
Inserting 1000000 records into &apos;sbtest19&apos;
Creating a secondary index on &apos;sbtest20&apos;...
Creating table &apos;sbtest22&apos;...
Inserting 1000000 records into &apos;sbtest22&apos;
Creating a secondary index on &apos;sbtest19&apos;...
Creating table &apos;sbtest21&apos;...
Inserting 1000000 records into &apos;sbtest21&apos;
Creating a secondary index on &apos;sbtest22&apos;...
Creating table &apos;sbtest24&apos;...
Inserting 1000000 records into &apos;sbtest24&apos;
Creating a secondary index on &apos;sbtest21&apos;...
Creating table &apos;sbtest23&apos;...
Inserting 1000000 records into &apos;sbtest23&apos;
Creating a secondary index on &apos;sbtest24&apos;...
Creating a secondary index on &apos;sbtest23&apos;...
</code></pre>
<h2 id="run-sysbench">Run sysbench</h2>
<p>Now we can run sysbench on the populated database. We can increase the threads to stress the database with more workloads in order to measure the system performance limit.</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --time=60 --report-interval=10 --db-driver=pgsql run
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Report intermediate results every 10 second(s)
Initializing random number generator from current time

Initializing worker threads...

Threads started!

[ 10s ] thds: 1 tps: 80.86 qps: 485.66 (r/w/o: 0.00/84.66/401.00) lat (ms,95%): 20.37 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 1 tps: 82.60 qps: 495.41 (r/w/o: 0.00/92.00/403.41) lat (ms,95%): 20.74 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 1 tps: 81.00 qps: 485.79 (r/w/o: 0.00/94.10/391.69) lat (ms,95%): 20.74 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 1 tps: 81.00 qps: 486.21 (r/w/o: 0.00/102.90/383.31) lat (ms,95%): 20.37 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 1 tps: 79.20 qps: 474.90 (r/w/o: 0.00/103.10/371.80) lat (ms,95%): 21.50 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 1 tps: 78.20 qps: 469.39 (r/w/o: 0.00/107.30/362.09) lat (ms,95%): 21.11 err/s: 0.00 reconn/s: 0.00
SQL statistics:
    queries performed:
        read:                            0
        write:                           5842
        other:                           23138
        total:                           28980
    transactions:                        4830   (80.48 per sec.)
    queries:                             28980  (482.90 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          60.0081s
    total number of events:              4830

Latency (ms):
         min:                                    6.87
         avg:                                   12.42
         max:                                   32.90
         95th percentile:                       21.11
         sum:                                59978.07

Threads fairness:
    events (avg/stddev):           4830.0000/0.00
    execution time (avg/stddev):   59.9781/0.00
</code></pre>
<h2 id="cleanup-the-database">Cleanup the database</h2>
<p>Once the sysbench is done, we can cleanup the test data as below.</p>
<pre><code class="language-shell">[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --db-driver=pgsql cleanup
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Dropping table &apos;sbtest1&apos;...
Dropping table &apos;sbtest2&apos;...
Dropping table &apos;sbtest3&apos;...
Dropping table &apos;sbtest4&apos;...
Dropping table &apos;sbtest5&apos;...
Dropping table &apos;sbtest6&apos;...
Dropping table &apos;sbtest7&apos;...
Dropping table &apos;sbtest8&apos;...
Dropping table &apos;sbtest9&apos;...
Dropping table &apos;sbtest10&apos;...
Dropping table &apos;sbtest11&apos;...
Dropping table &apos;sbtest12&apos;...
Dropping table &apos;sbtest13&apos;...
Dropping table &apos;sbtest14&apos;...
Dropping table &apos;sbtest15&apos;...
Dropping table &apos;sbtest16&apos;...
Dropping table &apos;sbtest17&apos;...
Dropping table &apos;sbtest18&apos;...
Dropping table &apos;sbtest19&apos;...
Dropping table &apos;sbtest20&apos;...
Dropping table &apos;sbtest21&apos;...
Dropping table &apos;sbtest22&apos;...
Dropping table &apos;sbtest23&apos;...
Dropping table &apos;sbtest24&apos;...

testdb=&gt; show tables from testdb;
 schema_name | table_name | type | owner | estimated_row_count | locality
-------------+------------+------+-------+---------------------+----------
(0 rows)

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &quot;drop database testdb&quot;
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/akopytov/sysbench">https://github.com/akopytov/sysbench</a></li>
<li><a href="https://www.cockroachlabs.com/blog/why-postgres/">https://www.cockroachlabs.com/blog/why-postgres/</a></li>
<li><a href="https://www.cockroachlabs.com/compare/cockroachdb-vs-postgresql/">https://www.cockroachlabs.com/compare/cockroachdb-vs-postgresql/</a></li>
<li><a href="https://www.cockroachlabs.com/blog/postgresql-vs-cockroachdb/">https://www.cockroachlabs.com/blog/postgresql-vs-cockroachdb/</a></li>
<li><a href="https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/">https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/</a></li>
<li><a href="https://blog.purestorage.com/purely-informational/how-to-benchmark-mysql-performance/">https://blog.purestorage.com/purely-informational/how-to-benchmark-mysql-performance/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using cgroups to limit block device bandwidth]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>The block I/O controller specifies upper IO rate limits on devices.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/blkio&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)

$ lscgroup  | grep blkio
blkio:/
</code></pre>
<p>In</p>]]></description><link>https://relentlesstorm.github.io/cgroups-limit-io/</link><guid isPermaLink="false">63afc44996dbf412f1f80353</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 05 Dec 2022 05:11:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1646226343350-1ee5021e342a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGhhcmRkaXNrfGVufDB8fHx8MTY3MjQ2MzUxOQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1646226343350-1ee5021e342a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGhhcmRkaXNrfGVufDB8fHx8MTY3MjQ2MzUxOQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using cgroups to limit block device bandwidth"><p>The block I/O controller specifies upper IO rate limits on devices.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/blkio&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)

$ lscgroup  | grep blkio
blkio:/
</code></pre>
<p>In this post, we will learn how to use the following control files to limit block device bandwidth for the user tasks.</p>
<ul>
<li>blkio.throttle.read_bps_device - Specifies upper limit on READ rate from the device. IO rate is specified in bytes per second. Rules are per device.</li>
<li>blkio.throttle.read_iops_device - Specifies upper limit on READ rate from the device. IO rate is specified in IO per second. Rules are per device.</li>
<li>blkio.throttle.write_bps_device - Specifies upper limit on WRITE rate to the device. IO rate is specified in bytes per second. Rules are per device.</li>
<li>blkio.throttle.write_iops_device - Specifies upper limit on WRITE rate to the device. IO rate is specified in io per second. Rules are per device.</li>
</ul>
<h2 id="create-blkio-control-group">Create blkio control group</h2>
<p>Install libcgroup package to manage cgroups:</p>
<pre><code class="language-shell">$ yum install libcgroup libcgroup-tools
</code></pre>
<p>Create blkio control group:</p>
<pre><code class="language-shell">$ cgcreate -g blkio:/blkiolimited

$ lscgroup | grep blkio
blkio:/
blkio:/blkiolimited

$ ls /sys/fs/cgroup/blkio/blkiolimited/
blkio.bfq.io_service_bytes            blkio.bfq.weight                 blkio.throttle.io_service_bytes_recursive  blkio.throttle.read_iops_device   cgroup.procs
blkio.bfq.io_service_bytes_recursive  blkio.bfq.weight_device          blkio.throttle.io_serviced                 blkio.throttle.write_bps_device   notify_on_release
blkio.bfq.io_serviced                 blkio.reset_stats                blkio.throttle.io_serviced_recursive       blkio.throttle.write_iops_device  tasks
blkio.bfq.io_serviced_recursive       blkio.throttle.io_service_bytes  blkio.throttle.read_bps_device             cgroup.clone_children
</code></pre>
<h2 id="limit-the-block-device-bandwidth">Limit the block device bandwidth</h2>
<h3 id="limit-block-device-bandwidth-for-root-group">Limit block device bandwidth for root group</h3>
<p>To specify a bandwidth rate on particular device for root group, we can use the policy format as &#x201C;major:minor bytes_per_second&#x201D;.</p>
<p>The following example puts a bandwidth limit of 1MB/s on writes for root group on device having major/minor number 259:12.</p>
<pre><code class="language-shell">$ ls -la /dev/ | grep &quot;nvme3n1&quot;
brw-rw----   1 root disk    259,  12 Dec 23 21:44 nvme3n1

$ echo &quot;259:12  1048576&quot; &gt; /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device
</code></pre>
<h3 id="limit-block-device-bandwidth-for-user-defined-group">Limit block device bandwidth for user defined group</h3>
<p>The following examples specifies the IOPS limit on the device 259:12 for the user defined cgroups &quot;blkio:/blkiolimited&quot;.</p>
<p>Use control files to specify the limit directly:</p>
<pre><code class="language-shell">$ cat /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
$ echo &quot;259:12  8192&quot; &gt; /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
$ cat /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
259:12 8192
</code></pre>
<p>Use libcgroup tools to specify the limit:</p>
<pre><code class="language-shell">$ cgset -r blkio.throttle.write_iops_device=&quot;259:12 8192&quot; blkiolimited
$ cgget -r blkio.throttle.write_iops_device blkiolimited
blkiolimited:
blkio.throttle.write_iops_device: 259:12 8192
</code></pre>
<h2 id="verify-the-disk-bandwidth-usage">Verify the disk bandwidth usage</h2>
<h3 id="use-fio-to-write-50g-data-on-root-groupunlimited-bandwidth">Use fio to write 50G data on root group(unlimited bandwidth)</h3>
<pre><code class="language-shell">$ fio --blocksize=4k --ioengine=libaio --readwrite=randwrite --filesize=50G --group_reporting --direct=1 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<pre><code class="language-shell">$ iostat -ktdx 5 | grep &quot;nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme3n1           0.00     0.05    0.00    8.02     0.01   508.54   126.83     0.06    7.80    0.08    7.80   0.04   0.03
nvme3n1           0.00     0.00    0.00 133828.60     0.00 535314.40     8.00     1.18    0.01    0.00    0.01   0.00  55.64
nvme3n1           0.00     0.20    0.00 246289.20     0.00 985157.60     8.00     2.21    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 246518.80     0.00 986076.80     8.00     2.23    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 244115.60     0.00 976462.40     8.00     2.24    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 240184.00     0.00 960736.80     8.00     2.23    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 250391.60     0.00 1001567.20     8.00     2.41    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 262449.60     0.00 1049799.20     8.00     2.53    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 252171.20     0.00 1008685.60     8.00     2.30    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 236467.60     0.00 945872.00     8.00     2.16    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 255060.80     0.00 1020244.00     8.00    17.04    0.07    0.00    0.07   0.00 100.00
nvme3n1           0.00     0.20    0.00 235199.60     0.00 940798.40     8.00    44.76    0.19    0.00    0.19   0.00 100.00
nvme3n1           0.00     0.20    0.00 18768.00     0.00 75072.80     8.00     5.81    0.31    0.00    0.31   0.00   8.40
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
^C
</code></pre>
<h3 id="use-fio-to-write-50g-data-on-user-defined-grouplimited-bandwidth">Use fio to write 50G data on user defined group(limited bandwidth)</h3>
<pre><code class="language-shell">$ cgget -r blkio.throttle.write_iops_device blkiolimited
blkiolimited:
blkio.throttle.write_iops_device: 259:12 8192

$ cgexec -g blkio:blkiolimited fio --blocksize=4k --ioengine=libaio --readwrite=randwrite --filesize=50G --group_reporting --direct=1 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<pre><code class="language-shell">$ iostat -ktdx 5 | grep &quot;nvme3n1&quot;
nvme3n1           0.00     0.05    0.00   24.54     0.01   574.51    46.82     0.06    2.57    0.08    2.57   0.01   0.04
nvme3n1           0.00     0.00    0.00 3112.20     0.00 12448.80     8.00     0.04    0.01    0.00    0.01   0.01   4.60
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.11    0.01    0.00    0.01   0.01  11.92
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.02
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.22
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.10
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.02  12.36
^C
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/blkio-controller.html">https://docs.kernel.org/admin-guide/cgroup-v1/blkio-controller.html</a></li>
<li><a href="https://andrestc.com/post/cgroups-io/">https://andrestc.com/post/cgroups-io/</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using cgroups to limit Memory usage]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>The memory controller isolates the memory behaviour of a group of tasks from the rest of the system.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/memory&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,</code></pre>]]></description><link>https://relentlesstorm.github.io/cgroups-limit-memory/</link><guid isPermaLink="false">63afc3ca96dbf412f1f80342</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 04 Dec 2022 05:08:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1589532768434-a92c95dad7cb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE5fHxkYXRhYmFzZXxlbnwwfHx8fDE2NzI0NjI4NzI&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1589532768434-a92c95dad7cb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE5fHxkYXRhYmFzZXxlbnwwfHx8fDE2NzI0NjI4NzI&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using cgroups to limit Memory usage"><p>The memory controller isolates the memory behaviour of a group of tasks from the rest of the system.</p>
<pre><code class="language-shell">$ mount | egrep &quot;/cgroup |/memory&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)

$ lssubsys -am | grep memory
memory /sys/fs/cgroup/memory
</code></pre>
<p>In this post, we will learn how to use the following control files to limit and monitor memory usage for the user tasks.</p>
<ul>
<li>memory.usage_in_bytes - show current usage for memory</li>
<li>memory.limit_in_bytes - set/show limit of memory usage</li>
</ul>
<h2 id="create-memory-control-group">Create memory control group</h2>
<p>Install libcgroup package to manage cgroups:</p>
<pre><code class="language-shell">$ yum install libcgroup libcgroup-tools
</code></pre>
<p>Create memory control group:</p>
<pre><code class="language-shell">$ cgcreate -g memory:/memlimited
$ lscgroup | grep memory
memory:/
memory:/memlimited

$ ls /sys/fs/cgroup/memory/memlimited/
cgroup.clone_children           memory.kmem.slabinfo                memory.memsw.failcnt             memory.soft_limit_in_bytes
cgroup.event_control            memory.kmem.tcp.failcnt             memory.memsw.limit_in_bytes      memory.stat
cgroup.procs                    memory.kmem.tcp.limit_in_bytes      memory.memsw.max_usage_in_bytes  memory.swappiness
memory.failcnt                  memory.kmem.tcp.max_usage_in_bytes  memory.memsw.usage_in_bytes      memory.usage_in_bytes
memory.force_empty              memory.kmem.tcp.usage_in_bytes      memory.move_charge_at_immigrate  memory.use_hierarchy
memory.kmem.failcnt             memory.kmem.usage_in_bytes          memory.numa_stat                 notify_on_release
memory.kmem.limit_in_bytes      memory.limit_in_bytes               memory.oom_control               tasks
memory.kmem.max_usage_in_bytes  memory.max_usage_in_bytes           memory.pressure_level
</code></pre>
<h2 id="limit-the-memory-usage">Limit the memory usage</h2>
<h3 id="using-control-files-directly">Using control files directly</h3>
<pre><code class="language-shell">$ echo 32G &gt; /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
$ cat /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
34359738368
</code></pre>
<h3 id="using-libcgroup-tools">Using libcgroup tools</h3>
<p>Limit the memory usage:</p>
<pre><code class="language-shell">$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<h2 id="verify-the-memory-usage">Verify the memory usage</h2>
<h3 id="unlimit-the-memory-usage">Unlimit the memory usage</h3>
<pre><code class="language-shell">$ cgset -r memory.limit_in_bytes=-1 memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 9223372036854771712
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code class="language-shell">$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is unlimited:</p>
<pre><code class="language-shell">$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
14143488
14143488
819499008
13288558592
25772953600
38258790400
50776608768
55638511616
55638429696
55638478848
55638528000
55638577152
55210229760
55210229760
^C
</code></pre>
<h3 id="limit-the-memory-usage-to-32gb">Limit the memory usage to 32GB</h3>
<pre><code class="language-shell">$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code class="language-shell">$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code class="language-shell">$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
9134080
6819614720
18048208896
29089763328
34359726080
34359672832
34359607296
34359717888
34359635968
34359619584
34280120320
34280120320
^C
</code></pre>
<p>From vmstat output, the cache usage is limited to 32GB. There is also swapping out activity due the memory pressure.</p>
<pre><code class="language-shell">$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0      0 1053891776    968 142992    0    0     0    30    0    0  0  0 100  0  0 2022-12-23 22:04:30
 2  0      0 1045755200    968 8266836    0    0  1697     0 3789  727  2  1 98  0  0 2022-12-23 22:04:35
 1  0      0 1034800320    976 19222628    0    0     0     2 1770  575  0  1 99  0  0 2022-12-23 22:04:40
 1  0      0 1024034304    984 29988256    0    0     0 163843 2999  780  0  1 99  0  0 2022-12-23 22:04:45
 1  1      0 1020353664    992 33665560    0    0     0 985500 8317 2414  0  1 98  0  0 2022-12-23 22:04:50
 1  1 317440 1020354112   1000 33666764    0 63474     3 1518376 18524 2604  0  1 98  1  0 2022-12-23 22:04:55
 1  1 340480 1020353472   1008 33666660    0 4571     0 1602014 15377 2381  0  1 98  1  0 2022-12-23 22:05:00
 2  0 340480 1020355904   1016 33667212   39    0    39 1632866 22166 70618  0  1 98  0  0 2022-12-23 22:05:05
 1  0 340480 1020356032   1020 33667228    0    0     0 1861355 27437 108029  0  1 99  0  0 2022-12-23 22:05:10
 2  0 340480 1020356224   1024 33667236    0    0     0 1874438 28732 111364  0  1 98  0  0 2022-12-23 22:05:15
 0  0    512 1020432704   1024 33599516  212    0   550 915429 13644 56805  0  1 99  0  0 2022-12-23 22:05:20
 0  0    512 1020432960   1028 33599516    0    0     3     4  165  149  0  0 100  0  0 2022-12-23 22:05:25
^C
</code></pre>
<h3 id="limit-the-memory-usage-for-the-tasks-in-the-current-bash">Limit the memory usage for the tasks in the current bash</h3>
<pre><code class="language-shell">$ cat /sys/fs/cgroup/memory/memlimited/tasks
$ echo $$ &gt; /sys/fs/cgroup/memory/memlimited/tasks
$ cat /sys/fs/cgroup/memory/memlimited/tasks
27875
28889

$ ps -ef | egrep &quot;27875|29023&quot; | grep -v grep
root     27875 27873  0 21:11 pts/0    00:00:17 -bash
root     29026 27875  0 22:11 pts/0    00:00:00 ps -ef
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code class="language-shell">$ fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code class="language-shell">$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
22835200
22835200
22835200
10150678528
21453983744
32693850112
34359607296
34359607296
34359738368
34359730176
34359730176
34359660544
34280062976
^C
</code></pre>
<pre><code class="language-shell">$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0    512 1053886784   1004 144860    0    0     0    31    0    0  0  0 100  0  0 2022-12-23 22:09:17
 0  0    512 1053886528   1004 144824    0    0     0     0  143  148  0  0 100  0  0 2022-12-23 22:09:22
 1  0    512 1050907776   1004 3109688    0    0  1660     3 2690  566  1  0 98  0  0 2022-12-23 22:09:27
 1  0    512 1039836288   1012 14180528    0    0     0     2 1699  508  0  1 99  0  0 2022-12-23 22:09:32
 1  0    512 1028822720   1020 25194536    0    0     0     2 1625  494  0  1 99  0  0 2022-12-23 22:09:37
 1  1    512 1020348800   1028 33665604    0    0     0 385026 4478 1313  0  1 99  0  0 2022-12-23 22:09:42
 1  1  46080 1020340224   1036 33676796    0 9090     0 1563114 13781 2301  0  1 98  1  0 2022-12-23 22:09:47
 1  1 340480 1020338688   1044 33676544    0 58904     0 1557223 19212 2580  0  1 98  1  0 2022-12-23 22:09:52
 1  1 340480 1020337792   1052 33676376   33    0    33 1481028 14133 5776  0  1 98  1  0 2022-12-23 22:09:57
 1  0 340480 1020342208   1056 33676396    0    0   184 1877767 27447 114955  0  1 98  0  0 2022-12-23 22:10:02
 1  1 340480 1020341056   1056 33677100    0    0     0 1817494 27635 101675  0  1 98  0  0 2022-12-23 22:10:07
 1  0 195144 1020350208   1060 33676988   83    0    83 1872111 27068 113280  0  1 98  0  0 2022-12-23 22:10:12
^C
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/memory.html">https://docs.kernel.org/admin-guide/cgroup-v1/memory.html</a></li>
<li><a href="https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled">https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[CockroachDB performance benchmarking]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="cockroachdb-key-concepts">CockroachDB key concepts</h2>
<ul>
<li>
<p>Range -	CockroachDB stores all user data (tables, indexes, etc.) and almost all system data in a giant sorted map of key-value pairs. This keyspace is divided into &quot;ranges&quot;, contiguous chunks of the keyspace, so that every key can always be found in a single</p></li></ul>]]></description><link>https://relentlesstorm.github.io/cockroachdb-performance-benchmarking/</link><guid isPermaLink="false">63b48ec21527a00d97449cae</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 04 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1505570554449-69ce7d4fa36b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIxfHwlMjBwZXJmb3JtYW5jZXxlbnwwfHx8fDE2NzI3Nzc0NzQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="cockroachdb-key-concepts">CockroachDB key concepts</h2>
<ul>
<li>
<img src="https://images.unsplash.com/photo-1505570554449-69ce7d4fa36b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIxfHwlMjBwZXJmb3JtYW5jZXxlbnwwfHx8fDE2NzI3Nzc0NzQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="CockroachDB performance benchmarking"><p>Range -	CockroachDB stores all user data (tables, indexes, etc.) and almost all system data in a giant sorted map of key-value pairs. This keyspace is divided into &quot;ranges&quot;, contiguous chunks of the keyspace, so that every key can always be found in a single range.</p>
<p>From a SQL perspective, a table and its secondary indexes initially map to a single range, where each key-value pair in the range represents a single row in the table (also called the primary index because the table is sorted by the primary key) or a single row in a secondary index. As soon as that range reaches 512 MiB in size, it splits into two ranges. This process continues for these new ranges as the table and its indexes continue growing.</p>
<pre><code class="language-shell">[root@host1 ~]# tail -f /var/log/cockroachdb_logs/cockroach.log
I220908 17:54:02.220498 5523606 kv/kvserver/pkg/kv/kvserver/replica_command.go:420 &#x22EE; [n1,split,s1,r499/1:&#x2039;/Table/113/1/4{394/6&#x2026;-693/6&#x2026;}&#x203A;] 1700  initiating a split of this range at key &#x2039;/Table/113/1/4402/59627&#x203A; [r501] (&#x2039;512 MiB above threshold size 512 MiB&#x203A;)&#x2039;&#x203A;
</code></pre>
</li>
<li>
<p>Replica -	CockroachDB replicates each range (3 times by default) and stores each replica on a different node.</p>
</li>
</ul>
<p>Refer to <a href="https://www.cockroachlabs.com/docs/stable/architecture/reads-and-writes-overview.html#cockroachdb-architecture-terms">here</a> for more.</p>
<p><img src="https://relentlesstorm.github.io/content/images/posts/cockroachdb-arch-terms.png" alt="CockroachDB performance benchmarking" loading="lazy"></p>
<h2 id="production-checklist">Production checklist</h2>
<p>Check <a href="https://www.cockroachlabs.com/docs/stable/recommended-production-settings.html">here</a> for the important recommendations for production deployments of CockroachDB. The following only lists some of the recommended settings.</p>
<h3 id="memory">Memory</h3>
<ul>
<li>Disable Linux memory swapping. Over-allocating memory on production machines can lead to unexpected performance issues when pages have to be read back into memory.</li>
<li>For production deployments, set --cache to 25% or higher. Avoid setting --cache and --max-sql-memory to a combined value of more than 75% of a machine&apos;s total RAM. Doing so increases the risk of memory-related failures.</li>
</ul>
<h3 id="storage">Storage</h3>
<ul>
<li>The maximum recommended storage capacity per node is 2.5 TiB, regardless of the number of vCPUs.</li>
<li>Use dedicated volumes for the CockroachDB store. Do not share the store volume with any other I/O activity.</li>
<li>Store CockroachDB log files in a separate volume from the main data store so that logging is not impacted by I/O throttling.</li>
<li>The recommended Linux filesystems are ext4 and XFS.</li>
</ul>
<h3 id="disk-io">Disk I/O</h3>
<ul>
<li>Use sysbench to benchmark IOPS on your cluster. If IOPS decrease, add more nodes to your cluster to increase IOPS.</li>
<li>Do not use LVM in the I/O path. Dynamically resizing CockroachDB store volumes can result in significant performance degradation. Using LVM snapshots in lieu of CockroachDB backup and restore is also not supported.</li>
<li>The optimal configuration for striping more than one device is RAID 10. RAID 0 and 1 are also acceptable from a performance perspective.</li>
</ul>
<h3 id="network">Network</h3>
<p>When starting a node, two main flags are used to control its network connections:</p>
<ul>
<li>--listen-addr determines which address(es) to listen on for connections from other nodes and clients.</li>
<li>--advertise-addr determines which address to tell other nodes to use.</li>
</ul>
<p><img src="https://relentlesstorm.github.io/content/images/posts/cockroachdb-network.png" alt="CockroachDB performance benchmarking" loading="lazy"></p>
<h3 id="load-balancing">Load balancing</h3>
<p>Each CockroachDB node is an equally suitable SQL gateway to a cluster, but to ensure client performance and reliability, it&apos;s important to use load balancing:</p>
<ul>
<li>Performance: Load balancers spread client traffic across nodes. This prevents any one node from being overwhelmed by requests and improves overall cluster performance (queries per second).</li>
<li>Reliability: Load balancers decouple client health from the health of a single CockroachDB node. To ensure that traffic is not directed to failed nodes or nodes that are not ready to receive requests, load balancers should use CockroachDB&apos;s readiness health check.</li>
</ul>
<h3 id="cache-and-sql-memory-size">Cache and SQL memory size</h3>
<p>CockroachDB manages its own memory caches, independently of the operating system. These are configured via the --cache and --max-sql-memory flags.</p>
<p>Each node has a default cache size of 128MiB that is passively consumed. The default was chosen to facilitate development and testing, where users are likely to run multiple CockroachDB nodes on a single machine. Increasing the cache size will generally improve the node&apos;s read performance.</p>
<p>Each node has a default SQL memory size of 25%. This memory is used as-needed by active operations to store temporary data for SQL queries.</p>
<ul>
<li>Increasing a node&apos;s cache size will improve the node&apos;s read performance.</li>
<li>Increasing a node&apos;s SQL memory size will increase the number of simultaneous client connections it allows, as well as the node&apos;s capacity for in-memory processing of rows when using ORDER BY, GROUP BY, DISTINCT, joins, and window functions.</li>
</ul>
<p>You can check cache size and SQL memory pool size in the log. In the following example output, it matches with the specified 25% cache and SQL memory size setting.</p>
<pre><code class="language-shell">$ vim cockroach.log
I220905 17:49:33.671304 1 server/config.go:487 &#x22EE; [n?] 6  system total memory: 1008 GiB
I220905 17:49:33.671318 1 server/config.go:489 &#x22EE; [n?] 7  server configuration:
I220905 17:49:33.671318 1 server/config.go:489 &#x22EE; [n?] 7 +&#x2039;max offset             500000000&#x203A;
I220905 17:49:33.671318 1 server/config.go:489 &#x22EE; [n?] 7 +&#x2039;cache size             252 GiB&#x203A;
I220905 17:49:33.671318 1 server/config.go:489 &#x22EE; [n?] 7 +&#x2039;SQL memory pool size   252 GiB&#x203A;
</code></pre>
<h2 id="cockroachdb-workloads">CockroachDB workloads</h2>
<p><img src="https://relentlesstorm.github.io/content/images/posts/rockroachdb_workloads.png" alt="CockroachDB performance benchmarking" loading="lazy"></p>
<h3 id="bank-workload">bank workload</h3>
<pre><code class="language-shell">$ cockroach workload init bank &apos;postgresql://root@host1:26257?sslmode=disable&apos;
I220831 23:52:14.593170 1 workload/workloadsql/dataload.go:146  [-] 1  imported bank (0s, 1000 rows)
I220831 23:52:14.609421 1 workload/workloadsql/workloadsql.go:136  [-] 2  starting 9 splits
</code></pre>
<pre><code class="language-shell">$ cockroach workload run bank --duration=1m &apos;postgresql://root@host1:26257?sslmode=disable&apos;
I220831 23:52:55.378492 1 workload/cli/run.go:414  [-] 1  creating load generator...
I220831 23:52:55.380594 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 2.103665ms)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
    1.0s        0         2929.8         2936.4      3.0     15.2     50.3    234.9 transfer
    2.0s        0         3387.3         3161.8      3.4     13.6     23.1     54.5 transfer
    3.0s        0         2755.2         3026.3      3.9     13.6     23.1    302.0 transfer
    4.0s        0         3295.5         3093.6      3.3     13.6     33.6    469.8 transfer
    5.0s        0         3536.5         3182.2      3.5     12.1     18.9     35.7 transfer
    6.0s        0         3558.1         3244.8      3.5     11.0     21.0     39.8 transfer
    7.0s        0         3566.9         3290.8      3.7     11.0     17.8     39.8 transfer
    8.0s        0         3317.6         3294.2      3.7     12.6     24.1     62.9 transfer
    9.0s        0         2992.9         3260.7      4.1     14.7     22.0     39.8 transfer
   10.0s        0         3628.2         3297.4      3.5     11.5     19.9     39.8 transfer
   11.0s        0         3604.5         3325.3      3.5     11.5     17.8     37.7 transfer
   12.0s        0         3668.4         3353.9      3.5     11.0     17.8     31.5 transfer
   13.0s        0         3485.9         3364.1      3.7     11.5     18.9     31.5 transfer
   14.0s        0         3377.9         3365.1      3.7     12.6     21.0     56.6 transfer
   15.0s        0         3084.4         3346.3      4.1     13.6     21.0     92.3 transfer
   16.0s        0         3650.1         3365.3      3.4     11.5     17.8     44.0 transfer
   17.0s        0         3662.7         3382.8      3.5     11.5     17.8     37.7 transfer
   18.0s        0         3461.8         3387.2      3.7     11.0     19.9    159.4 transfer
   19.0s        0         3426.3         3389.3      3.7     12.1     19.9     41.9 transfer
   20.0s        0         3196.0         3379.6      3.9     13.1     19.9     35.7 transfer
&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
   60.0s        0         206551         3442.5      4.6      3.7     12.1     19.9    469.8  transfer

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
   60.0s        0         206551         3442.5      4.6      3.7     12.1     19.9    469.8
</code></pre>
<h3 id="tpc-c-workload">TPC-C workload</h3>
<blockquote>
<p>--warehouses</p>
<blockquote>
<p>The number of warehouses for loading initial data, at approximately 200 MB per warehouse.<br>
Applicable commands: init or run<br>
Default: 1</p>
</blockquote>
</blockquote>
<blockquote>
<p>--workers</p>
<blockquote>
<p>The number of concurrent workers.<br>
Applicable commands: init or run<br>
Default: --warehouses * 10</p>
</blockquote>
</blockquote>
<p>The number os wareshouses can be specified by &quot;--warehouses&quot; option. By default, only one warehouse is created.</p>
<pre><code class="language-shell">$ cockroach workload init tpcc &apos;postgresql://root@host1:26257?sslmode=disable&apos;
Error: failed insert into warehouse: pq: duplicate key value violates unique constraint &quot;warehouse_pkey&quot;

$ cockroach workload init tpcc &apos;postgresql://root@host1:26257?sslmode=disable&apos; --drop
I220901 00:32:56.365593 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 1 rows)
I220901 00:32:56.372727 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 10 rows)
I220901 00:32:57.034349 1 workload/workloadsql/dataload.go:146  [-] 3  imported customer (1s, 30000 rows)
I220901 00:32:57.248382 1 workload/workloadsql/dataload.go:146  [-] 4  imported history (0s, 30000 rows)
I220901 00:32:57.462320 1 workload/workloadsql/dataload.go:146  [-] 5  imported order (0s, 30000 rows)
I220901 00:32:57.490476 1 workload/workloadsql/dataload.go:146  [-] 6  imported new_order (0s, 9000 rows)
I220901 00:32:57.921658 1 workload/workloadsql/dataload.go:146  [-] 7  imported item (0s, 100000 rows)
I220901 00:32:59.267268 1 workload/workloadsql/dataload.go:146  [-] 8  imported stock (1s, 100000 rows)
I220901 00:33:00.916392 1 workload/workloadsql/dataload.go:146  [-] 9  imported order_line (2s, 300343 rows)
</code></pre>
<pre><code class="language-shell">$ cockroach workload run tpcc --duration=10m &apos;postgresql://root@host1:26257?sslmode=disable&apos;
I220901 00:34:17.147411 1 workload/cli/run.go:414  [-] 1  creating load generator...
Initializing 2 connections...
Initializing 0 idle connections...
Initializing 10 workers and preparing statements...
I220901 00:34:17.151817 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 4.411152ms)
&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             12            0.0     32.6     31.5     44.0     44.0     44.0  delivery

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0            120            0.2     17.9     17.8     26.2     41.9     54.5  newOrder

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             13            0.0      6.7      5.8     14.2     15.2     15.2  orderStatus

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0            131            0.2     10.8     11.0     14.7     24.1     29.4  payment

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             13            0.0     10.8     10.5     15.7     15.7     15.7  stockLevel

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  600.0s        0            289            0.5     14.4     13.1     28.3     41.9     54.5
Audit check 9.2.1.7: SKIP: not enough delivery transactions to be statistically significant
Audit check 9.2.2.5.1: SKIP: not enough orders to be statistically significant
Audit check 9.2.2.5.2: SKIP: not enough orders to be statistically significant
Audit check 9.2.2.5.5: SKIP: not enough payments to be statistically significant
Audit check 9.2.2.5.6: SKIP: not enough order status transactions to be statistically significant
Audit check 9.2.2.5.3: PASS
Audit check 9.2.2.5.4: PASS

_elapsed_______tpmC____efc__avg(ms)__p50(ms)__p90(ms)__p95(ms)__p99(ms)_pMax(ms)
  600.0s       12.0  93.3%     17.9     17.8     23.1     26.2     41.9     54.5
</code></pre>
<h3 id="ycsb-workload">YCSB workload</h3>
<pre><code class="language-shell">$ cockroach workload init ycsb &apos;postgresql://root@host1:26257?sslmode=disable&apos;
I220831 23:57:53.909658 1 workload/workloadsql/dataload.go:146  [-] 1  imported usertable (2s, 10000 rows)
</code></pre>
<pre><code class="language-shell">$ cockroach workload run ycsb --duration=10m &apos;postgresql://root@host1:26257?sslmode=disable&apos;
I220831 23:58:24.410319 1 workload/cli/run.go:414  [-] 1  creating load generator...
I220831 23:58:24.427528 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 17.212701ms)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
    1.0s        0        11011.5        11243.7      1.1      2.4      3.8     11.0 read
    1.0s        0          581.7          593.9      3.0      5.5      7.3     12.6 update
    2.0s        0        12105.1        11674.4      1.1      1.8      3.4     10.0 read
    2.0s        0          636.9          615.4      2.8      3.9      6.6     11.0 update
    3.0s        0        11590.6        11646.5      1.1      2.0      4.5     14.2 read
    3.0s        0          607.1          612.7      2.8      6.0      9.4     12.1 update
    4.0s        0        11813.5        11688.2      1.1      1.9      3.5     13.1 read
    4.0s        0          622.0          615.0      2.8      4.5      7.6     13.6 update
    5.0s        0        11959.5        11742.5      1.1      2.0      3.3      8.1 read
    5.0s        0          607.0          613.4      2.6      4.7      7.1     12.1 update
    6.0s        0        12186.2        11816.5      1.0      1.8      3.5     12.6 read
    6.0s        0          638.0          617.5      2.6      4.1     10.0     16.8 update
    7.0s        0        11815.8        11816.3      1.1      2.0      3.7      8.4 read
    7.0s        0          646.0          621.6      2.8      4.7      6.8      9.4 update
    8.0s        0        11850.8        11820.6      1.1      2.0      3.3      7.6 read
    8.0s        0          617.9          621.1      2.8      4.5      6.6      9.4 update
    9.0s        0        11713.5        11808.7      1.1      2.0      3.9     11.0 read
    9.0s        0          609.1          619.8      2.8      4.5      7.6     11.5 update
   10.0s        0        11949.6        11822.8      1.1      2.0      3.3     13.1 read
   10.0s        0          622.0          620.0      2.8      4.2      6.0      8.4 update

&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0        8576709        14294.5      1.0      1.0      1.6      2.9     62.9  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0         451026          751.7      2.5      2.5      3.8      6.6     27.3  update

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  600.0s        0        9027735        15046.2      1.1      1.0      2.2      3.5     62.9
</code></pre>
<h2 id="restart-cockroachdb-cluster">Restart CockroachDB cluster</h2>
<ol>
<li>Stop the process on each node</li>
</ol>
<pre><code class="language-shell">$ ps -ef | grep cock | grep -v grep
root     12217     1 99 Sep06 ?        2-20:28:09 cockroach start --log-dir=/var/log/cockroachdb_logs --store=/mnt/cockroachdb_mnt1 --insecure --advertise-addr=host1 --join=host1,host2,host3 --cache=.25 --max-sql-memory=.25

$ kill -9 12217
$ ps -ef | grep cock | grep -v grep
</code></pre>
<ol start="2">
<li>Start the process on each node</li>
</ol>
<pre><code class="language-shell">$ cockroach start --store=/mnt/cockroanchdb_mnt1 --insecure --advertise-addr=host1 --join=host1,host2,host3 --cache=.25 -max-sql-memory=.25 --background
</code></pre>
<h2 id="cockroachdb-commands">CockroachDB commands</h2>
<p>List node IDs:</p>
<pre><code class="language-shell">$ cockroach node ls --insecure
  id
------
   1
   2
   3
(3 rows)
</code></pre>
<p>Show node status:</p>
<pre><code class="language-shell">$ cockroach node status --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:09.022236 |          | true         | true
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:09.048582 |          | true         | true
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:09.402081 |          | true         | true
(3 rows)
</code></pre>
<p>Show status and range/replica details:</p>
<pre><code class="language-shell">$ cockroach node status --ranges --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | replicas_leaders | replicas_leaseholders | ranges | ranges_unavailable | ranges_underreplicated
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:36.022463 |          | true         | true    |         306 |                   306 |    941 |                  0 |                      0
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:36.048508 |          | true         | true    |              310 |                   310 |    941 |                  0 |                      0
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:36.40421  |          | true         | true    |              325 |                   325 |    941 |                  0 |                      0
(3 rows)
</code></pre>
<p>Show status and disk usage details:</p>
<pre><code class="language-shell">$ cockroach node status --stats --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live |  live_bytes  |  key_bytes  | value_bytes  | intent_bytes | system_bytes
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------+--------------+-------------+--------------+--------------+---------------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:45.022008 |          | true         | true    | 246665681135 | 47930398573 | 220956161393 |        40331 |       812555
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:45.048777 |          | true         | true    | 246665683880 | 47930401894 | 220956192532 |        28025 |       817028
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:45.40156  |          | true         | true    | 246666008478 | 47930560065 | 220957658233 |        74382 |       741028
(3 rows)
</code></pre>
<p>Show status and decommissioning details for active and inactive nodes:</p>
<pre><code class="language-shell">$ cockroach node status --decommission --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | gossiped_replicas | is_decommissioning | membership | is_draining
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:03.043432 |          | true         | true    |               941 | false              | active     | false
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:24:03.048948 |          | true         | true    |               941 | false              | active     | false
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:24:03.401953 |          | true         | true    |               941 | false              | active     | false
(3 rows)
</code></pre>
<p>Show complete status details for active and inactive nodes:</p>
<pre><code class="language-shell">$ cockroach node status --all --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | replicas_leaders | replicas_leaseholders | ranges | ranges_unavailable | ranges_underreplicated |  live_bytes  |  key_bytes  | value_bytes  | intent_bytes | system_bytes | gossiped_replicas | is_decommissioning | membership | is_draining
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:07.539077 |          |         true |    true |              306 |                   306 |    942 |                  0 |                      0 | 246677946612 | 47939212836 | 221027696131 |        81398 |       722359 |        941        |              false |   active   |    false
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:24:07.548886 |          |         true |    true |              311 |                   311 |    942 |                  0 |                      0 | 246677952989 | 47939228198 | 221027812276 |        67816 |       731258 |        941        |              false |   active   |    false
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:24:07.901765 |          |         true |    true |              325 |                   325 |    942 |                  0 |                      0 | 246678252011 | 47939394591 | 221029217819 |        55249 |       869542 |        941        |              false |   active   |    false
(3 rows)
</code></pre>
<p>Show status details for a specific node:</p>
<pre><code class="language-shell">$ cockroach node status 1 --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:30.044382 |          | true         | true
(1 row)
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/cockroachdb/cockroach/blob/master/docs/design.md">https://github.com/cockroachdb/cockroach/blob/master/docs/design.md</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using cgroups to limit CPU utilization]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-cgroups">Intro to cgroups</h2>
<p>Cgroups(control groups) make it possible to allocate system resources such as CPU time, memory, disk I/O and network bandwidth, or combinations of them,  among a group of tasks(processes) running on a system.</p>
<p>The following commands output the available subsystems(resource controllers) for the cgroups.</p>]]></description><link>https://relentlesstorm.github.io/cgroup-limit-cpu/</link><guid isPermaLink="false">63afb5c896dbf412f1f80317</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sat, 03 Dec 2022 04:09:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1613068687893-5e85b4638b56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-cgroups">Intro to cgroups</h2>
<img src="https://images.unsplash.com/photo-1613068687893-5e85b4638b56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRhdGFiYXNlfGVufDB8fHx8MTY3MjQ2Mjg3Mg&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using cgroups to limit CPU utilization"><p>Cgroups(control groups) make it possible to allocate system resources such as CPU time, memory, disk I/O and network bandwidth, or combinations of them,  among a group of tasks(processes) running on a system.</p>
<p>The following commands output the available subsystems(resource controllers) for the cgroups. Each subsystem has a bunch of tunables to control the resource allocation.</p>
<pre><code class="language-shell">$ lssubsys -am
cpuset /sys/fs/cgroup/cpuset
cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct
blkio /sys/fs/cgroup/blkio
memory /sys/fs/cgroup/memory
devices /sys/fs/cgroup/devices
freezer /sys/fs/cgroup/freezer
net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio
perf_event /sys/fs/cgroup/perf_event
hugetlb /sys/fs/cgroup/hugetlb
pids /sys/fs/cgroup/pids
rdma /sys/fs/cgroup/rdma

$ mount | grep cgroup
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
</code></pre>
<h2 id="cpu-subsystem-and-tunables">CPU subsystem and tunables</h2>
<h3 id="ceiling-enforcement-parameters">Ceiling enforcement parameters</h3>
<ul>
<li>
<p>cpu.cfs_period_us</p>
<p>specifies a period of time in microseconds (&#xB5;s, represented here as &quot;us&quot;) for how regularly a cgroup&apos;s access to CPU resources should be reallocated. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. The upper limit of the cpu.cfs_quota_us parameter is 1 second and the lower limit is 1000 microseconds.</p>
</li>
<li>
<p>cpu.cfs_quota_us</p>
<p>specifies the total amount of time in microseconds (&#xB5;s, represented here as &quot;us&quot;) for which all tasks in a cgroup can run during one period (as defined by cpu.cfs_period_us). As soon as tasks in a cgroup use up all the time specified by the quota, they are throttled for the remainder of the time specified by the period and not allowed to run until the next period. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. Note that the quota and period parameters operate on a CPU basis. To allow a process to fully utilize two CPUs, for example, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 100000.</p>
<p>Setting the value in cpu.cfs_quota_us to -1 indicates that the cgroup does not adhere to any CPU time restrictions. This is also the default value for every cgroup (except the root cgroup).</p>
</li>
</ul>
<h3 id="relative-shares-parameter">Relative shares parameter</h3>
<ul>
<li>
<p>cpu.shares</p>
<p>contains an integer value that specifies a relative share of CPU time available to the tasks in a cgroup. For example, tasks in two cgroups that have cpu.shares set to 100 will receive equal CPU time, but tasks in a cgroup that has cpu.shares set to 200 receive twice the CPU time of tasks in a cgroup where cpu.shares is set to 100. The value specified in the cpu.shares file must be 2 or higher.</p>
<p>Note that shares of CPU time are distributed per all CPU cores on multi-core systems. Even if a cgroup is limited to less than 100% of CPU on a multi-core system, it may use 100% of each individual CPU core.</p>
<p>Using relative shares to specify CPU access has two implications on resource management that should be considered:</p>
<ul>
<li>
<p>Because the CFS does not demand equal usage of CPU, it is hard to predict how much CPU time a cgroup will be allowed to utilize. When tasks in one   cgroup are idle and are not using any CPU time, the leftover time is collected in a global pool of unused CPU cycles. Other cgroups are allowed to borrow CPU cycles from this pool.</p>
</li>
<li>
<p>The actual amount of CPU time that is available to a cgroup can vary depending on the number of cgroups that exist on the system. If a cgroup has a relative share of 1000 and two other cgroups have a relative share of 500, the first cgroup receives 50% of all CPU time in cases when processes in all cgroups attempt to use 100% of the CPU. However, if another cgroup is added with a relative share of 1000, the first cgroup is only allowed 33% of the CPU (the rest of the cgroups receive 16.5%, 16.5%, and 33% of CPU).</p>
</li>
</ul>
</li>
</ul>
<h2 id="using-libcgroup-tools">Using libcgroup tools</h2>
<p>Install libcgroup package to manage cgroups:</p>
<pre><code class="language-shell">$ yum install libcgroup libcgroup-tools
</code></pre>
<p>List the cgroups:</p>
<pre><code class="language-shell">$ lscgroup
hugetlb:/
cpu,cpuacct:/
cpuset:/
blkio:/
memory:/
freezer:/
net_cls,net_prio:/
pids:/
rdma:/
perf_event:/
devices:/
devices:/system.slice
devices:/system.slice/irqbalance.service
devices:/system.slice/systemd-udevd.service
devices:/system.slice/polkit.service
devices:/system.slice/chronyd.service
devices:/system.slice/auditd.service
devices:/system.slice/tuned.service
devices:/system.slice/systemd-journald.service
devices:/system.slice/sshd.service
devices:/system.slice/crond.service
devices:/system.slice/NetworkManager.service
devices:/system.slice/rsyslog.service
devices:/system.slice/abrtd.service
devices:/system.slice/lvm2-lvmetad.service
devices:/system.slice/postfix.service
devices:/system.slice/dbus.service
devices:/system.slice/system-getty.slice
devices:/system.slice/systemd-logind.service
devices:/system.slice/abrt-oops.service

$ ls /sys/fs/cgroup
blkio  cpuacct      cpuset   freezer  memory   net_cls,net_prio  perf_event  rdma
cpu    cpu,cpuacct  devices  hugetlb  net_cls  net_prio          pids        systemd
</code></pre>
<p>Create the cgroup:</p>
<pre><code class="language-shell">$ cgcreate -g cpu:/cpulimited

$ lscgroup | grep cpulimited
cpu,cpuacct:/cpulimited

$ ls cpulimited/
cgroup.clone_children  cpuacct.usage_percpu       cpu.cfs_period_us  cpu.stat
cgroup.procs           cpuacct.usage_percpu_sys   cpu.cfs_quota_us   notify_on_release
cpuacct.stat           cpuacct.usage_percpu_user  cpu.rt_period_us   tasks
cpuacct.usage          cpuacct.usage_sys          cpu.rt_runtime_us
cpuacct.usage_all      cpuacct.usage_user         cpu.shares
</code></pre>
<p>Limit CPU utilization by percentage:</p>
<pre><code class="language-shell">$ lscpu | grep ^CPU\(s\):
CPU(s):                96

$ cgset -r cpu.cfs_quota_us=200000 cpulimited
</code></pre>
<p>Check the cgroup settings:</p>
<pre><code class="language-shell">$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000

$ cgget -g cpu:cpulimited
cpulimited:
cpu.cfs_period_us: 100000
cpu.stat: nr_periods 2
	nr_throttled 0
	throttled_time 0
cpu.shares: 1024
cpu.cfs_quota_us: 200000
cpu.rt_runtime_us: 0
cpu.rt_period_us: 1000000
</code></pre>
<p>Delete the cgroup:</p>
<pre><code class="language-shell">$ cgdelete cpu,cpuacct:/cpulimited
</code></pre>
<h2 id="verify-the-cpu-utilization-with-fio-workload">Verify the CPU utilization with fio workload</h2>
<p>Create a fio job file:</p>
<pre><code class="language-shell">$ cat burn_cpu.job
[burn_cpu]
# Don&apos;t transfer any data, just burn CPU cycles
ioengine=cpuio
# Stress the CPU at 100%
cpuload=100
# Make 4 clones of the job
numjobs=4
</code></pre>
<p>Run the fio jobs without CPU limit:</p>
<pre><code class="language-shell">$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: -1

$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code class="language-shell">$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
13775 root      20   0 1079912   4016   2404 R 100.0  0.0   0:11.65 fio
13776 root      20   0 1079916   4004   2392 R 100.0  0.0   0:11.65 fio
13777 root      20   0 1079920   4004   2392 R 100.0  0.0   0:11.65 fio
13778 root      20   0 1079924   4004   2392 R 100.0  0.0   0:11.65 fio
</code></pre>
<p>The CPU utilization is 400% for the 4 fio jobs when there is no CPU limit set. Note that, there is totally 9600% CPU bandwidth available.</p>
<p>Limit the CPU utilization to 200%:</p>
<pre><code class="language-shell">$ cgset -r cpu.cfs_quota_us=200000 cpulimited

$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000
</code></pre>
<p>Run the fio jobs again:</p>
<pre><code class="language-shell">$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code class="language-shell">$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12908 root      20   0 1079916   3948   2336 R  50.3  0.0   0:06.91 fio
12909 root      20   0 1079920   3948   2336 R  50.0  0.0   0:06.88 fio
12910 root      20   0 1079924   3948   2336 R  50.0  0.0   0:06.93 fio
12907 root      20   0 1079912   3948   2336 R  49.3  0.0   0:06.86 fio
</code></pre>
<p>The CPU utilization is 200% for the 4 fio jobs when the CPU utilization is limited to 200%.</p>
<p>Check the processes are running on which CPU cores:</p>
<pre><code class="language-shell">$ mpstat -P ALL 5 | awk &apos;{if ($3==&quot;CPU&quot; || $NF&lt;99)print;}&apos;

12:40:32 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:37 AM  all    2.11    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   97.89
12:40:37 AM    0   20.52    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   79.48
12:40:37 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:37 AM    2   50.10    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.90
12:40:37 AM   24   29.74    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   70.26
12:40:37 AM   87   50.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.80

12:40:37 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:42 AM  all    2.11    0.00    0.01    0.00    0.00    0.00    0.00    0.00    0.00   97.88
12:40:42 AM    0   11.49    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00   88.31
12:40:42 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:42 AM    2   50.30    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.70
12:40:42 AM   24   38.97    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   61.03
12:40:42 AM   87   49.90    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   50.10
</code></pre>
<p>The 4 fio jobs are running on 5 CPU cores with total utilization of 200%. So, it indicates this method limits the total CPU utilization out of all the CPU cores. However, the number of CPU cores is not limited.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html">https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_libcgroup_tools">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_libcgroup_tools</a></li>
<li><a href="https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups">https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Deploy CockroachDB in kubernetes cluster]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="deploy-kubernetes-cluster">Deploy Kubernetes cluster</h2>
<pre><code class="language-shell">$ kubectl get nodes
NAME            STATUS   ROLES    AGE    VERSION
node0   Ready    &lt;none&gt;   115d   v1.19.2
node1   Ready    &lt;none&gt;   115d   v1.19.2
node2   Ready    &lt;none&gt;   115d   v1.19.2
node3   Ready    master   115d   v1.19.2
</code></pre>
<h2 id="start-cockroachdb-cluster">Start CockroachDB cluster</h2>
<p>Start the</p>]]></description><link>https://relentlesstorm.github.io/deploy-cockroachdb-in-kubernetes-cluster/</link><guid isPermaLink="false">63b48e821527a00d97449ca1</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sat, 03 Dec 2022 00:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1605745341112-85968b19335b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGRvY2tlcnxlbnwwfHx8fDE2NzI3NzcyNzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="deploy-kubernetes-cluster">Deploy Kubernetes cluster</h2>
<pre><code class="language-shell">$ kubectl get nodes
NAME            STATUS   ROLES    AGE    VERSION
node0   Ready    &lt;none&gt;   115d   v1.19.2
node1   Ready    &lt;none&gt;   115d   v1.19.2
node2   Ready    &lt;none&gt;   115d   v1.19.2
node3   Ready    master   115d   v1.19.2
</code></pre>
<h2 id="start-cockroachdb-cluster">Start CockroachDB cluster</h2>
<img src="https://images.unsplash.com/photo-1605745341112-85968b19335b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGRvY2tlcnxlbnwwfHx8fDE2NzI3NzcyNzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Deploy CockroachDB in kubernetes cluster"><p>Start the CockroachDB nodes with a configuration file that has been customized for performance:</p>
<pre><code class="language-shell">$ curl -O https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/performance/cockroachdb-statefulset-insecure.yaml

$ kubectl create -f cockroachdb-statefulset-insecure.yaml
service/cockroachdb-public created
service/cockroachdb created
poddisruptionbudget.policy/cockroachdb-budget created
statefulset.apps/cockroachdb created
</code></pre>
<p>Confirm that three pods are Running successfully. Note that they will not be considered Ready until after the cluster has been initialized:</p>
<pre><code class="language-shell">$ kubectl get pods
NAME            READY     STATUS    RESTARTS   AGE
cockroachdb-0   0/1       Running   0          2m
cockroachdb-1   0/1       Running   0          2m
cockroachdb-2   0/1       Running   0          2m
</code></pre>
<p>Confirm that the persistent volumes and corresponding claims were created successfully for all three pods:</p>
<pre><code class="language-shell">$ kubectl get pvc
NAME                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
datadir-cockroachdb-0   Bound    pvc-ac210538-2a20-4d99-9b5d-c5a03d733b4d   1Ti        RWO            px-repl1       35s
datadir-cockroachdb-1   Bound    pvc-54f24a59-a063-46bb-9645-d4292eb483a3   1Ti        RWO            px-repl1       25s
datadir-cockroachdb-2   Bound    pvc-0ebd7fa7-dc36-4c77-ab9d-003d10680f56   1Ti        RWO            px-repl1       15s

$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                           STORAGECLASS   REASON   AGE
pvc-0ebd7fa7-dc36-4c77-ab9d-003d10680f56   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-2   px-repl1                17s
pvc-54f24a59-a063-46bb-9645-d4292eb483a3   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-1   px-repl1                27s
pvc-ac210538-2a20-4d99-9b5d-c5a03d733b4d   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-0   px-repl1                27s
</code></pre>
<p>Before initialize the CockroachDB cluster, check the network(use ping command) to make sure each host can communicate with each other. Otherwise, you may run into connection failure between cluster nodes.</p>
<p>In my case, I want to run the cluster over the private network which doesn&apos;t have DNS setup. So, I just add the private IP addresses and Kubernetes qualified hostnames and CockroachDB node names in /etc/hosts file on each host as below.</p>
<pre><code class="language-shell">10.0.0.1 ip-10-10-0-1 cockroachdb-0.cockroachdb
10.0.0.2 ip-10-10-0-2 cockroachdb-1.cockroachdb
10.0.0.3 ip-10-10-0-3 cockroachdb-2.cockroachdb
</code></pre>
<p>Use the provided cluster-init.yaml file to perform a one-time initialization that joins the CockroachDB nodes into a single cluster:</p>
<pre><code class="language-shell">$ kubectl create \
-f https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/cluster-init.yaml
</code></pre>
<p>Confirm that cluster initialization has completed successfully. The job should be considered successful and the Kubernetes pods should soon be considered Ready:</p>
<pre><code class="language-shell">$ kubectl get job cluster-init
NAME           COMPLETIONS   DURATION   AGE
cluster-init   1/1           7s         27s
kubectl get pods
NAME                 READY   STATUS      RESTARTS   AGE
cluster-init-cqf8l   0/1     Completed   0          56s
cockroachdb-0        1/1     Running     0          7m51s
cockroachdb-1        1/1     Running     0          7m51s
cockroachdb-2        1/1     Running     0          7m51s
</code></pre>
<h2 id="use-the-built-in-sql-client">Use the built-in SQL client</h2>
<pre><code class="language-shell">$ kubectl run cockroachdb -it --image=cockroachdb/cockroach:v22.1.7 --rm --restart=Never -- sql --insecure --host=cockroachdb-public

root@cockroachdb-public:26257/defaultdb&gt; show databases;
  database_name | owner | primary_region | regions | survival_goal
----------------+-------+----------------+---------+----------------
  defaultdb     | root  | NULL           | {}      | NULL
  postgres      | root  | NULL           | {}      | NULL
  system        | node  | NULL           | {}      | NULL
(3 rows)


Time: 5ms total (execution 4ms / network 1ms)
</code></pre>
<h2 id="run-tpc-c-workload-on-the-cockroachdb-cluster">Run TPC-C workload on the CockroachDB cluster</h2>
<pre><code class="language-shell">$ kubectl run cockroachdb -it --image=cockroachdb/cockroach:v22.1.7 --rm --restart=Never -- bash
[root@cockroachdb cockroach]# cockroach node ls --insecure --host=cockroachdb-public
  id
------
   1
   2
   3
(3 rows)

[root@cockroachdb cockroach]# cockroach workload init tpcc &quot;postgresql://root@cockroachdb-public:26257?sslmode=disable&quot; --warehouses 10 --drop
I220922 19:48:15.543311 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 10 rows)
I220922 19:48:15.580634 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 100 rows)
I220922 19:48:20.098986 1 workload/workloadsql/dataload.go:146  [-] 3  imported customer (5s, 300000 rows)
I220922 19:48:21.601978 1 workload/workloadsql/dataload.go:146  [-] 4  imported history (2s, 300000 rows)
I220922 19:48:24.317881 1 workload/workloadsql/dataload.go:146  [-] 5  imported order (3s, 300000 rows)
I220922 19:48:24.540100 1 workload/workloadsql/dataload.go:146  [-] 6  imported new_order (0s, 90000 rows)
I220922 19:48:24.998829 1 workload/workloadsql/dataload.go:146  [-] 7  imported item (0s, 100000 rows)
I220922 19:48:34.737491 1 workload/workloadsql/dataload.go:146  [-] 8  imported stock (10s, 1000000 rows)
I220922 19:48:49.030366 1 workload/workloadsql/dataload.go:146  [-] 9  imported order_line (14s, 3001222 rows)

[root@cockroachdb cockroach]# cockroach sql --insecure --database tpcc --host=cockroachdb-public:26257 -e &apos;show tables&apos;
  schema_name | table_name | type  | owner | estimated_row_count | locality
--------------+------------+-------+-------+---------------------+-----------
  public      | customer   | table | root  |              300000 | NULL
  public      | district   | table | root  |                 100 | NULL
  public      | history    | table | root  |              300000 | NULL
  public      | item       | table | root  |              100000 | NULL
  public      | new_order  | table | root  |               90000 | NULL
  public      | order      | table | root  |              300000 | NULL
  public      | order_line | table | root  |             3001222 | NULL
  public      | stock      | table | root  |             1000000 | NULL
  public      | warehouse  | table | root  |                  10 | NULL
(9 rows)

[root@cockroachdb cockroach]# cockroach workload run tpcc --warehouses=10 --ramp=10s --duration=20s postgres://root@cockroachdb-public:26257?sslmode=disable
I220922 19:52:41.633625 1 workload/cli/run.go:414  [-] 1  creating load generator...
Initializing 20 connections...
Initializing 0 idle connections...
Initializing 100 workers and preparing statements...
I220922 19:52:51.646000 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 10.012371773s)

_elapsed_______tpmC____efc__avg(ms)__p50(ms)__p90(ms)__p95(ms)__p99(ms)_pMax(ms)
   20.0s      153.0 119.0%     38.2     44.0     48.2     48.2     50.3     52.4
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-with-kubernetes-insecure">https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-with-kubernetes-insecure</a></li>
<li><a href="https://www.cockroachlabs.com/docs/stable/kubernetes-performance.html">https://www.cockroachlabs.com/docs/stable/kubernetes-performance.html</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>