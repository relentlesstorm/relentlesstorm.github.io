<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[FlamingBytes]]></title><description><![CDATA[Stay hungry, Stay foolish.]]></description><link>https://www.flamingbytes.com/</link><image><url>https://www.flamingbytes.com/favicon.png</url><title>FlamingBytes</title><link>https://www.flamingbytes.com/</link></image><generator>Ghost 5.26</generator><lastBuildDate>Sun, 05 Mar 2023 18:45:29 GMT</lastBuildDate><atom:link href="https://www.flamingbytes.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Using AWS CLI to manage EBS volumes]]></title><description><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-aws-cli">Intro to AWS CLI</h2>
<p>The AWS Command Line Interface (AWS CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.</p>
<h2 id="aws-cli-install">AWS CLI install</h2>
<p>To install AWS CLI</p>]]></description><link>https://www.flamingbytes.com/blog/using-aws-cli-to-manage-ebs-volumes/</link><guid isPermaLink="false">6402d4fb32be7b0c9cc952fb</guid><category><![CDATA[Cloud Storage]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sat, 04 Mar 2023 06:30:26 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1628296499994-70face79ab36?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fEFXUyUyMHxlbnwwfHx8fDE2Nzc5MTEyMzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-aws-cli">Intro to AWS CLI</h2>
<img src="https://images.unsplash.com/photo-1628296499994-70face79ab36?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fEFXUyUyMHxlbnwwfHx8fDE2Nzc5MTEyMzc&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using AWS CLI to manage EBS volumes"><p>The AWS Command Line Interface (AWS CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.</p>
<h2 id="aws-cli-install">AWS CLI install</h2>
<p>To install AWS CLI on MacOS:</p>
<pre><code>$ curl &quot;https://awscli.amazonaws.com/AWSCLIV2.pkg&quot; -o &quot;AWSCLIV2.pkg&quot;
$ sudo installer -pkg AWSCLIV2.pkg -target /
$ which aws
/usr/local/bin/aws
$ aws --version
aws-cli/2.10.3 Python/3.9.11 Darwin/20.4.0 exe/x86_64 prompt/off
</code></pre>
<h2 id="configure-aws-credentials">Configure AWS credentials</h2>
<p>The AWS CLI stores sensitive credential information that you specify with <em>aws configure</em> in a local file ~/.aws/credentials.</p>
<p>$ aws configure<br>
AWS Access Key ID [<strong><strong><strong><strong><strong><strong><strong><strong>6N7Q]: [Your AWS access Key ID]<br>
AWS Secret Access Key [</strong></strong></strong></strong></strong></strong></strong></strong>+Ic+]: [Your AWS Secret Access Key]<br>
Default region name [None]: [Your Region Name]<br>
Default output format [None]:</p>
<p>$ cat ~/.aws/credentials<br>
[default]<br>
aws_access_key_id =  [Your AWS access Key ID]<br>
aws_secret_access_key = [Your AWS Secret Access Key]<br>
aws_region = [Your Region Name]</p>
<h2 id="aws-cli-commands">AWS CLI Commands</h2>
<p>To get commands help:</p>
<pre><code>$ aws eks help
$ aws ec2 help
</code></pre>
<p>To list and describe clusters:</p>
<pre><code>$ aws eks list-clusters
$ aws eks describe-cluster --name [your-cluster-name]
</code></pre>
<p>To list and describe nodegroups:</p>
<pre><code>$ aws eks list-nodegroups --cluster-name [your-cluster-name]
$ aws eks describe-nodegroup --cluster-name [your-cluster-name] --nodegroup-name [you-nodegroup-name]
</code></pre>
<p>To delete multiple EBS volumes(example):</p>
<pre><code>$ aws ec2 describe-volumes --filters &quot;Name=tag:Name,Values=gp3-volume-*&quot;  | egrep &quot;VolumeId&quot; | awk &apos;{print $NF}&apos; | sed &apos;s/\&quot;//g;s/\,//&apos; &gt; vol_ids

$ for id in `cat vol_ids`
do
    aws ec2 delete-volume --volume-id $id
done

$ aws ec2 describe-volumes --filters &quot;Name=tag:Name,Values=gp3-volume-*&quot;  | egrep &quot;VolumeId&quot; | awk &apos;{print $NF}&apos;  | wc -l
0
</code></pre>
<p>To create multiple EBS volumes(example):</p>
<pre><code>$ for i in `seq 1 8`
do
aws ec2 create-volume --volume-type gp3 --size 256 --iops 16000 --throughput 1000 --availability-zone us-east-1a --tag-specifications &quot;ResourceType=volume,Tags=[{Key=Name,Value=gp3-volume-$i}]&quot;
done
</code></pre>
<p>To attach EBS volume to EC2 instance:</p>
<pre><code>$ aws ec2 describe-volumes --filters &quot;Name=tag:Name,Values=gp3-volume-1&quot; --query &quot;Volumes[*].{ID:VolumeId}&quot;
[
    {
        &quot;ID&quot;: &quot;vol-0101002b66d4fc211&quot;
    }
]

$ aws ec2 attach-volume --volume-id vol-0101002b66d4fc211 --instance-id i-0c2b7553a99a7277b --device /dev/sdf
{
    &quot;AttachTime&quot;: &quot;2023-03-04T01:58:37.155000+00:00&quot;,
    &quot;Device&quot;: &quot;/dev/sdf&quot;,
    &quot;InstanceId&quot;: &quot;i-0c2b7553a99a7277b&quot;,
    &quot;State&quot;: &quot;attaching&quot;,
    &quot;VolumeId&quot;: &quot;vol-0101002b66d4fc211&quot;
}
</code></pre>
<p>To describe instance:</p>
<pre><code>$ aws ec2 describe-instances --instance-ids i-0c2b7553a99a7277b | egrep &quot;DeviceName|Status|VolumeId&quot;
&quot;DeviceName&quot;: &quot;/dev/sdf&quot;,
&quot;Status&quot;: &quot;attached&quot;,
&quot;VolumeId&quot;: &quot;vol-0101002b66d4fc211&quot;
&quot;Status&quot;: &quot;attached&quot;
</code></pre>
<p>To verify the attached volumes in EC2 instance:</p>
<pre><code>[ec2-user@ip-192-168-25-183 ~]$ ls -la /dev/sdf
lrwxrwxrwx 1 root root 7 Mar  4 01:58 /dev/sdf -&gt; nvme1n1

[ec2-user@ip-192-168-25-183 ~]$ lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1       259:0    0  256G  0 disk
&#x251C;&#x2500;nvme0n1p1   259:1    0  256G  0 part /
&#x2514;&#x2500;nvme0n1p128 259:2    0    1M  0 part
nvme1n1       259:3    0  256G  0 disk
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Analyze Cockroach performance from the built-in DB console]]></title><description><![CDATA[In this post, we try to understand how to analyze CockroachDB performance by monitoring its built-in DB console.]]></description><link>https://www.flamingbytes.com/blog/analyze-cockroach-performance-from-db-console/</link><guid isPermaLink="false">63fbd09c5fc5490f28da1daa</guid><category><![CDATA[Performance]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 26 Feb 2023 23:50:02 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1608222351212-18fe0ec7b13b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRhc2hib2FyZHxlbnwwfHx8fDE2Nzc0NTUwMzE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1608222351212-18fe0ec7b13b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGRhc2hib2FyZHxlbnwwfHx8fDE2Nzc0NTUwMzE&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Analyze Cockroach performance from the built-in DB console"><p>In this post, we try to understand how to analyze CockroachDB performance by monitoring its built-in DB console.</p>
<h2 id="problem-statement">Problem statement</h2>
<p>We have a YCSB workload(insert:read=0.5:0.5) run over single node CockroachDB server. As a result, the insert OPS is 10940 and average latency is ~31.3ms per operation. We notice that the backend NVME disk I/O latency is less than 1ms, which is fast enough to handle the I/Os. A question is which component introduces the extra ~30ms latency for each insert operation? We also notice that the CockroachDB consumes ~50% CPU in user space. So, we want to dig more about the CockroachDB behaviors by leveraging its DB console.</p>
<pre><code>INSERT - Takes(s): 4517.6, Count: 49423144, OPS: 10940.0, Avg(us): 31295, Min(us): 1080, Max(us): 61951, 99th(us): 59583, 99.9th(us): 61663, 99.99th(us): 61919
READ   - Takes(s): 4517.7, Count: 49892329, OPS: 11043.9, Avg(us): 2640, Min(us): 351, Max(us): 33887, 99th(us): 16527, 99.9th(us): 29887, 99.99th(us): 33535
</code></pre>
<h2 id="db-console-access">DB console access</h2>
<p>The DB Console provides details about the CockroachDB cluster and database configuration, and helps optimize cluster performance.</p>
<p>You can access the DB Console from every node at http://[host]:[http-port], or http://[host]:8080 by default.</p>
<h2 id="overview">Overview</h2>
<p>In the  Overview page, we know that it is a single node CockroachDB cluster. The node has 96 CPUs, 2% memory, and 2.6% disk capacity in use. Also, there are 477 CockroachDB range replicas.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_overview.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<h2 id="databases">Databases</h2>
<p>The Databases page shows details about the system and user databases in the cluster.</p>
<p>In our case, the database <em>test</em> is used for YCSB benchmark. The database size is 148.7GB and it has one table with 433 ranges. The table is called <em>usertable</em> which has 11 columns. There is one index created on the table.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_database.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_table.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_table_1.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<h2 id="sql-activity">SQL Activity</h2>
<p>The SQL Activity page summarizes SQL activity in the cluster. Transactions(Statements) shows frequently executed and high-latency SQL transactions(statements).</p>
<p>In our case, there are 138M insert and 50M select transactions. There are more inserts than selects because we had initial database load operations which insert 100M records to the table. After that, we ran YCSB workload with the insert to read ratio &quot;0.5:0.5&quot;.</p>
<p>Furthermore, the transaction time for the &quot;Insert into usertable&quot; is 30.4ms. The SQL statement time is 30.2ms for this transaction. This helps explain why the average YCSB operation latency is ~31.3ms. The major overhead is from the insert transaction itself.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_sql_transaction.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_sql_statement.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<h2 id="metrics">Metrics</h2>
<p>The Metrics page provides dashboards for all types of CockroachDB metrics.</p>
<ul>
<li>Overview dashboard has metrics about SQL performance, replication, and storage.</li>
<li>Hardware dashboard has metrics about CPU usage, disk throughput, network traffic, storage capacity, and memory.</li>
<li>Runtime dashboard has metrics about node count, CPU time, and memory usage.</li>
<li>SQL dashboard has metrics about SQL connections, byte traffic, queries, transactions, and service latency.</li>
<li>Storage dashboard has metrics about storage capacity and file descriptors.</li>
<li>Replication dashboard has metrics about how data is replicated across the cluster, e.g., range status, replicas per store, and replica quiescence.</li>
<li>Distributed dashboard has metrics about distribution tasks across the cluster, including RPCs, transactions, and node heartbeats.</li>
<li>Queues dashboard has metrics about the health and performance of various queueing systems in CockroachDB, including the garbage collection and Raft log queues.</li>
<li>Slow requests dashboard has metrics about important cluster tasks that take longer than expected to complete, including Raft proposals and lease acquisitions.</li>
<li>Changefeeds dashboard has metrics about the changefeeds created across your cluster.</li>
<li>Overload dashboard has metrics about the performance of the parts of your cluster relevant to the cluster&apos;s admission control system.<br>
TTL dashboard has metrics about the progress and performance of batch deleting expired data using Row-Level TTL from your cluster.</li>
</ul>
<h3 id="overview-metrics">Overview metrics</h3>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_metric.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<h3 id="sql-metrics">SQL metrics</h3>
<p>This shows the KV execution latency(90th and 99th percentile). In our case, the 90th percentile latency is ~31ms for the KV execution. At this point, we know why the CockroachDB introduces the high latency at the application layer. To improve the overall YCSB benchmark performance, changes on CockroachDB layer is needed.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_metric_sql.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<h3 id="replication-metrics">Replication metrics</h3>
<p>This shows how the number of range replicas increases significantly during database load time.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_metric_replicas.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<h3 id="hardware-metrics">Hardware metrics</h3>
<p>The hardware metrics are extremely useful for performance issue analysis. It shows the CPU, Memory, Disk I/O and network bandwidth utilization, which allows us to know if the system performance is limited by hardware or not.</p>
<p>In our case, there is no hardware bottleneck. But we do notice that CockroachDB consumes up to 50% CPU in user space for 96 CPU cores system. From previous analysis, we know each SQL transaction takes a relative long CPU cycle(~30ms) to complete.</p>
<p>I used <em>vmstat</em> and <em>top</em> to identify how much user/kernel CPU are utilized at system level and how much CPU is consumed by CockroachDB process. When YCSB is running, CockroachDB process consumes up to 50% CPU and most of the CPU is in user space.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_hw_cpu_mem.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_hw_bw.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_hw_iops.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<p><img src="https://www.flamingbytes.com/content/images/posts/crdb_hw_nw.png" alt="Analyze Cockroach performance from the built-in DB console" loading="lazy"></p>
<h2 id="summary">Summary</h2>
<p>This post concludes how to analyze a typical performance issue by making use of the Cockroach DB console. The DB console is production ready to use and shows many useful insight of CockroachDB and system metrics.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.cockroachlabs.com/docs/v22.2/ui-overview.html">https://www.cockroachlabs.com/docs/v22.2/ui-overview.html</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Introduction to BPF performance tools]]></title><description><![CDATA[BPF stands for Berkeley Packet Filter, which was firstly developed to improve the performance of packet capture tools. It was rewritten later and turned into a general-purpose execution engine including the creation of performance analysis tools.]]></description><link>https://www.flamingbytes.com/blog/introduction-to-bpf-performance-tools/</link><guid isPermaLink="false">63f7fe52a9435e0ecab20d72</guid><category><![CDATA[Performance]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 22 Feb 2023 01:37:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1519229875649-6b8da16368fd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwOHx8dHJhY2V8ZW58MHx8fHwxNjc3MTk4NTc1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="what-are-bpf-and-ebpf">What are BPF and eBPF</h2>
<img src="https://images.unsplash.com/photo-1519229875649-6b8da16368fd?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwOHx8dHJhY2V8ZW58MHx8fHwxNjc3MTk4NTc1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Introduction to BPF performance tools"><p>BPF stands for Berkeley Packet Filter, which was firstly developed to improve the performance of packet capture tools. It was rewritten later and turned into a general-purpose execution engine including the creation of performance analysis tools.</p>
<p>BPF allows the kernel to run mini programs on system and application events, such as disk I/O. It makes the kernel fully programmable, empowering users(even non-kernel developers) to customize and control the system in order to solve real-world problems.</p>
<p>Extended BPF is abbreviated as eBPF. Since the classic BPF is no longer developed, the kernel contains only one execution engine, BPF(eBPF), to run both extended BPF and classic BPF programs.</p>
<h2 id="what-are-tracing-snooping-sampling-and-profiling">What are tracing, snooping, sampling and profiling</h2>
<p><strong>Tracing</strong> is event based recording. BPF tools uses this type of instrumentation. You might be familiar with some Linux tracing tools. For example, <em>strace</em> records and prints system calls. <em>tcpdump</em> is another tracing tool for network packets.</p>
<p>There are many ready-to-use BPF tools after you install <a href="https://www.flamingbytes.com/blog/getting-started-with-bcc/">BCC</a>. Some of them are <strong>snooping</strong> tools. It just a differnt naming. It&apos;s actually event based tracing.</p>
<pre><code>$ ls /usr/share/bcc/tools/| grep snoop
bindsnoop biosnoop compactsnoop dcsnoop drsnoop execsnoop exitsnoop killsnoop mountsnoop opensnoop shmsnoop sofdsnoop statsnoop syncsnoop threadsnoop ttysnoop
</code></pre>
<p><strong>Sampling</strong> tools take a subset of measurements which gives a coarse picture of the target system. It is also known as <strong>profiling</strong>. It&apos;s usually timer(frequency) based. For example, 100 samples can be taken every second per CPU. Obviously, the overhead is generally lower than event-based tracing. However, a disadvantage is the sampling would miss events.</p>
<h2 id="what-are-bcc-and-bpftrace">What are BCC and bpftrace</h2>
<p>Both BCC and bpftrace are front ends which have been developed for tracing based on BPF technology.</p>
<p>BCC was the first tracing framework developed for BPF. It provides C programming environment for writing kernel BPF code and other languages for the user-level interface like python, Lua and C++. BCC is better suited for complex scripts and daemons. It can make use of other libraries. To get started with BCC, refer to this <a href="https://www.flamingbytes.com/blog/getting-started-with-bcc/">post</a>.</p>
<p>bpftrace is a newer front end for developing BPF tools.It is ideal for powerful one-liners and custom short script. To get started with bpftrace, refer to this <a href="https://www.flamingbytes.com/blog/getting-started-with-bpftrace/">post</a>.</p>
<p>BCC and bpftrace live in a Linux Foundation project on github called <strong>IO Visor</strong>.</p>
<ul>
<li><a href="https://github.com/iovisor/bcc">https://github.com/iovisor/bcc</a></li>
<li><a href="https://github.com/iovisor/bpftrace">https://github.com/iovisor/bpftrace</a></li>
</ul>
<h2 id="bpf-tracing-visibility">BPF tracing visibility</h2>
<p>BPF tracing gives you the visibility across the full software stack and allows new tools to be created on demand.</p>
<p>Thanks to Brendan Gregg who annotated with BPF-based performance tools to observe different components across the system software stack as below.</p>
<p><img src="https://www.brendangregg.com/Perf/bpf_book_tools.png" alt="Introduction to BPF performance tools" loading="lazy"></p>
<p>In the past, I used traditional tools like perf, strace, ftrace and systemtap for the system tracing and profiling. I&apos;m interested in learning more about these BPF tracing tools in future.</p>
<h2 id="dynamic-instrumentation-kprobes-and-uprobes">Dynamic instrumentation: kprobes and uprobes</h2>
<p>BPF tracing supports multiple sources of events to provide visibility of the entire software stack.</p>
<p>Dynamic instrumentation(also calledd Dynamic tracing) provides the ability to insert instrumentation points into software. It&apos;s often used by BPF tools to instrument the start and end of the kernel and application functions. BPF tracing tools use both kprobes and uprobes for dynamic instrumentation.</p>
<p>For example,</p>
<ul>
<li>kprobe:vfs_read</li>
<li>kretprobe:vfs_read</li>
<li>uprobe:/bin/bash:readline</li>
<li>uretprobe:/bin/bash:readline</li>
</ul>
<h2 id="static-instrumentation-tracepoints-and-usdt">Static instrumentation: tracepoints and USDT</h2>
<p>A downside for dynamic instrumentation is the instruments functions can be renamed or removed from one software version to the next. This is <em>interface stability issue</em>. Static instrumentation comes with more stable events(functions) which are more well maintained by developers. Of course, this will become a maintenance burden for developers. So, the number of static instrumentation should be limited.</p>
<p>BPF supports <em>tracepoints</em> for kernel static instrumentation and <em>USDT</em> for user-level static instrumentation. For example,</p>
<ul>
<li>tracepoint:syscalls:sys_enter_open</li>
<li>usdt:/usr/sbin/mysqld:mysql:query__start</li>
</ul>
<h2 id="summary">Summary</h2>
<p>This post introduces BPF tracing technology and its front ends BCC and bpftrace at high level. Dynamic and static instrumentation(tracing) can be used for performance analysis and troubeshooting.</p>
<p>We will dive into these technologies in future posts.</p>
<h2 id="reference">Reference</h2>
<ul>
<li>BPF Performance Tools by Brendan Gregg</li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Getting started with bpftrace]]></title><description><![CDATA[bpftrace is a high-level tracing language for Linux enhanced Berkeley Packet Filter (eBPF) available in recent Linux kernels (4.x). ]]></description><link>https://www.flamingbytes.com/blog/getting-started-with-bpftrace/</link><guid isPermaLink="false">63f7d46ea9435e0ecab20d18</guid><category><![CDATA[Performance]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 21 Feb 2023 21:08:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1544206269-b7e48d9a93f7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHx0cmFjZXxlbnwwfHx8fDE2NzcxODY5MDI&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-bpftrace">Intro to bpftrace</h2>
<img src="https://images.unsplash.com/photo-1544206269-b7e48d9a93f7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHx0cmFjZXxlbnwwfHx8fDE2NzcxODY5MDI&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Getting started with bpftrace"><p><a href="https://github.com/iovisor/bpftrace">bpftrace</a> is a high-level tracing language for Linux enhanced Berkeley Packet Filter (eBPF) available in recent Linux kernels (4.x). bpftrace uses LLVM as a backend to compile scripts to BPF-bytecode and makes use of BCC for interacting with the Linux BPF system, as well as existing Linux tracing capabilities: kernel dynamic tracing (kprobes), user-level dynamic tracing (uprobes), and tracepoints. The bpftrace language is inspired by awk and C, and predecessor tracers such as DTrace and SystemTap. bpftrace was created by Alastair Robertson.</p>
<h2 id="bpftrace-package-install">bpftrace package install</h2>
<pre><code>$ curl https://repos.baslab.org/rhel/7/bpftools/bpftools.repo --output /etc/yum.repos.d/bpftools.repo
 
$ yum install bpftrace bpftrace-tools bpftrace-doc bcc-static bcc-tools
Installed:
  bcc-static.x86_64 0:0.21.0-1.el7  bpftrace.x86_64 0:0.13.0-2.el7  bpftrace-doc.noarch 0:0.13.0-2.el7   bpftrace-tools.noarch 0:0.13.0-2.el7
 
Updated:
  bcc-tools.x86_64 0:0.21.0-1.el7
 
Dependency Updated:
  bcc.x86_64 0:0.21.0-1.el7  python-bcc.noarch 0:0.21.0-1.el7
</code></pre>
<p>Bpftrace ships with many ready-to-run tools after installation.</p>
<pre><code>$ cd /usr/share/bpftrace/tools
$ ls
bashreadline.bt  biostacks.bt  cpuwalk.bt  execsnoop.bt       killsnoop.bt  naptime.bt    pidpersec.bt  setuids.bt    syncsnoop.bt  tcpconnect.bt  tcpretrans.bt   vfscount.bt   xfsdist.bt biolatency.bt    bitesize.bt   dcsnoop.bt  ext4dist.bt        loads.bt      oomkill.bt    runqlat.bt    statsnoop.bt  syscount.bt   tcpdrop.bt     tcpsynbl.bt     vfsstat.bt
biosnoop.bt      capable.bt    doc         gethostlatency.bt  mdflush.bt    opensnoop.bt  runqlen.bt    swapin.bt     tcpaccept.bt  tcplife.bt     threadsnoop.bt  writeback.bt
</code></pre>
<h2 id="a-first-look-at-bpftrace-tracing-system-call-open">A first look at bpftrace: tracing system call open()</h2>
<p>Run the bpftrace program at the command line(a one-liner):</p>
<pre><code>$ bpftrace -e &apos;tracepoint:syscalls:sys_enter_open { printf(&quot;%s %s\n&quot;, comm, str(args-&gt;filename)); }&apos;
Attaching 1 probe...
</code></pre>
<p>From a different terminal, start a <em>iostat</em> process to be traced:</p>
<pre><code>$ iostat -ktdx 2
</code></pre>
<p>Monitor the tracing output:</p>
<pre><code>$ bpftrace -e &apos;tracepoint:syscalls:sys_enter_open { printf(&quot;%s %s\n&quot;, comm, str(args-&gt;filename)); }&apos;
&lt;omitted..&gt;
iostat /proc/uptime
iostat /proc/stat
iostat /proc/diskstats
iostat /proc/uptime
iostat /proc/stat
iostat /proc/diskstats
iostat /proc/uptime
iostat /proc/stat
iostat /proc/diskstats
^C
</code></pre>
<p>The output shows the process name and the filename passed to the open syscall system-wide. In the above example, the <em>iostat</em> process opens the following files every 2 seconds.</p>
<pre><code>/proc/uptime
/proc/stat
/proc/diskstats
</code></pre>
<p>List all the open tracepoints:</p>
<pre><code>$ bpftrace -l &apos;tracepoint:syscalls:sys_enter_open*&apos;
tracepoint:syscalls:sys_enter_open
tracepoint:syscalls:sys_enter_open_by_handle_at
tracepoint:syscalls:sys_enter_open_tree
tracepoint:syscalls:sys_enter_openat
tracepoint:syscalls:sys_enter_openat2
</code></pre>
<p>Count the open(and variant) syscalls:</p>
<pre><code>$ bpftrace -e &apos;tracepoint:syscalls:sys_enter_open* { @[probe]=count();}&apos;
Attaching 5 probes...
^C
@[tracepoint:syscalls:sys_enter_open]: 66
@[tracepoint:syscalls:sys_enter_openat]: 3963
</code></pre>
<p>Bpftrace ships with opensnoop.bt which traces both the start and end of open/openat syscall.</p>
<pre><code>$ cat opensnoop.bt
#!/usr/bin/bpftrace
/*
 * opensnoop        Trace open() syscalls.
 *                For Linux, uses bpftrace and eBPF.
 *
 * Also a basic example of bpftrace.
 *
 * USAGE: opensnoop.bt
 *
 * This is a bpftrace version of the bcc tool of the same name.
 *
 * Copyright 2018 Netflix, Inc.
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;)
 *
 * 08-Sep-2018        Brendan Gregg        Created this.
 */

BEGIN
{
    printf(&quot;Tracing open syscalls... Hit Ctrl-C to end.\n&quot;);
    printf(&quot;%-6s %-16s %4s %3s %s\n&quot;, &quot;PID&quot;, &quot;COMM&quot;, &quot;FD&quot;, &quot;ERR&quot;, &quot;PATH&quot;);
}

tracepoint:syscalls:sys_enter_open,
tracepoint:syscalls:sys_enter_openat
{
    @filename[tid] = args-&gt;filename;
}

tracepoint:syscalls:sys_exit_open,
tracepoint:syscalls:sys_exit_openat
/@filename[tid]/
{
    $ret = args-&gt;ret;
    $fd = $ret &gt; 0 ? $ret : -1;
    $errno = $ret &gt; 0 ? 0 : - $ret;
    printf(&quot;%-6d %-16s %4d %3d %s\n&quot;, pid, comm, $fd, $errno, str(@filename[tid]));
    delete(@filename[tid]);
}

END
{
    clear(@filename);
}
</code></pre>
<p>It outputs process id, command, fd and the opened file path.</p>
<pre><code>$ ./opensnoop.bt | egrep &quot;PID|iostat&quot;
PID    COMM               FD ERR PATH
28992  iostat              3   0 /etc/ld.so.cache
28992  iostat              3   0 /lib64/libc.so.6
28992  iostat              3   0 /usr/lib/locale/locale-archive
28992  iostat              3   0 /sys/devices/system/cpu
28992  iostat              3   0 /proc/diskstats
28992  iostat              3   0 /etc/localtime
28992  iostat              3   0 /proc/uptime
28992  iostat              3   0 /proc/stat
28992  iostat              3   0 /proc/diskstats
28992  iostat              4   0 /etc/sysconfig/sysstat.ioconf
28992  iostat              3   0 /proc/uptime
28992  iostat              3   0 /proc/stat
28992  iostat              3   0 /proc/diskstats
^C
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/iovisor/bpftrace/blob/master/INSTALL.md#CentOS-package">https://github.com/iovisor/bpftrace/blob/master/INSTALL.md#CentOS-package</a></li>
<li><a href="https://github.com/fbs/el7-bpf-specs/blob/master/README.md#repository">https://github.com/fbs/el7-bpf-specs/blob/master/README.md#repository</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[How to install BCC on Ubuntu 22.04]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p><strong>To build the toolchain from source, one needs:</strong></p>
<p>LLVM 3.7.1 or newer, compiled with BPF support (default=on)<br>
Clang, built from the same tree as LLVM<br>
cmake (&gt;=3.1), gcc (&gt;=4.7), flex, bison<br>
LuaJIT, if you want Lua support</p>
<pre><code>root@ubuntu:~# cat /etc/*release
DISTRIB_</code></pre>]]></description><link>https://www.flamingbytes.com/blog/how-to-install-bcc-on-ubuntu-22-04/</link><guid isPermaLink="false">63ec80774febc7438165a75d</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 14 Feb 2023 06:54:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1518414881329-0f96c8f2a924?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHZhbGVudGluZSUyMGRheXxlbnwwfHx8fDE2NzY0NDQxNDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1518414881329-0f96c8f2a924?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHZhbGVudGluZSUyMGRheXxlbnwwfHx8fDE2NzY0NDQxNDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="How to install BCC on Ubuntu 22.04"><p><strong>To build the toolchain from source, one needs:</strong></p>
<p>LLVM 3.7.1 or newer, compiled with BPF support (default=on)<br>
Clang, built from the same tree as LLVM<br>
cmake (&gt;=3.1), gcc (&gt;=4.7), flex, bison<br>
LuaJIT, if you want Lua support</p>
<pre><code>root@ubuntu:~# cat /etc/*release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION=&quot;Ubuntu 22.04.1 LTS&quot;
PRETTY_NAME=&quot;Ubuntu 22.04.1 LTS&quot;
NAME=&quot;Ubuntu&quot;
VERSION_ID=&quot;22.04&quot;
VERSION=&quot;22.04.1 LTS (Jammy Jellyfish)&quot;
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL=&quot;https://www.ubuntu.com/&quot;
SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;
BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;
PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;
UBUNTU_CODENAME=jammy
root@ubuntu:~# uname -r
5.15.0-60-generic

</code></pre>
<h2 id="install-build-dependencies">Install build dependencies</h2>
<pre><code># For Jammy (22.04)
root@ubuntu:~# apt install -y bison build-essential cmake flex git libedit-dev \
libllvm14 llvm-14-dev libclang-14-dev python3 zlib1g-dev libelf-dev libfl-dev python3-distutils
</code></pre>
<h2 id="install-and-compile-bcc">Install and compile BCC</h2>
<pre><code>root@ubuntu:~# git clone https://github.com/iovisor/bcc.git
root@ubuntu:~# mkdir bcc/build; cd bcc/build
root@ubuntu:~# cmake ..
root@ubuntu:~# make
root@ubuntu:~# make install
root@ubuntu:~# cmake -DPYTHON_CMD=python3 .. # build python3 binding
root@ubuntu:~# pushd src/python/
root@ubuntu:~# make
root@ubuntu:~# make install
root@ubuntu:~# popd
</code></pre>
<h2 id="a-first-look-at-bcc">A first look at BCC</h2>
<pre><code>root@ubuntu:~# biolatency 
Tracing block device I/O... Hit Ctrl-C to end.
^C
     usecs               : count     distribution
     0 -&gt; 1          : 0        |                                        |
     2 -&gt; 3          : 0        |                                        |
     4 -&gt; 7          : 0        |                                        |
     8 -&gt; 15         : 0        |                                        |
     16 -&gt; 31        : 0        |                                        |
     32 -&gt; 63        : 0        |                                        |
     64 -&gt; 127       : 0        |                                        |
     128 -&gt; 255      : 1        |****************************************|

</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md#ubuntu---source">https://github.com/iovisor/bcc/blob/master/INSTALL.md#ubuntu---source</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[YCSB benchmark on PostgreSQL]]></title><description><![CDATA[go-ycsb is a Go port of YCSB. It fully supports all YCSB generators and the Core workload so we can do the basic CRUD benchmarks with Go.]]></description><link>https://www.flamingbytes.com/blog/ycsb-benchmark-on-postgresql/</link><guid isPermaLink="false">63eb1767361c0d0ddf89fbef</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 14 Feb 2023 05:20:50 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1533284133567-0da9844151ce?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fHJhY2V8ZW58MHx8fHwxNjc2MzUxODY1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="create-a-postgresql-database">Create a PostgreSQL database</h2>
<img src="https://images.unsplash.com/photo-1533284133567-0da9844151ce?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDl8fHJhY2V8ZW58MHx8fHwxNjc2MzUxODY1&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="YCSB benchmark on PostgreSQL"><p>You can follow this <a href="https://www.flamingbytes.com/blog/getting-started-with-postgresql/">post</a> to install PostgreSQL and create the database cluster.</p>
<p><strong>To create a database:</strong></p>
<pre><code>$ db_host=10.10.10.243; db_port=5432; db_user=postgres; db_name=testdb

$ psql --host=$db_host --port=$db_port --username=$db_user -w -c &quot;create database $db_name&quot;
$ psql --host=$db_host --port=$db_port --username=$db_user -w -c &quot;\l&quot;
$ psql --host=$db_host --port=$db_port --username=$db_user -w -d $db_name -c &quot;\dt+&quot;
</code></pre>
<h2 id="load-data-to-database">Load data to database</h2>
<p><a href="https://github.com/pingcap/go-ycsb">go-ycsb</a> is a Go port of YCSB. It fully supports all YCSB generators and the Core workload so we can do the basic CRUD benchmarks with Go.</p>
<pre><code>$ ./bin/go-ycsb load postgresql -P workloads/workloadd --threads 768 -p pg.host=$db_host -p pg.port=$db_port -p pg.user=$db_user -p pg.db=$db_name -p pg.sslmode=disable -p dropdata=true
&lt;snippet&gt;
Run finished, takes 29m39.847077518s
INSERT - Takes(s): 1779.8, Count: 97404992, OPS: 54727.5, Avg(us): 11176, Min(us): 179, Max(us): 80767, 99th(us): 72127, 99.9th(us): 79807, 99.99th(us): 80703
</code></pre>
<h2 id="run-ycsb-benchmark">Run YCSB benchmark</h2>
<pre><code>$ ./bin/go-ycsb run postgresql -P workloads/workloadd --threads 768 -p pg.host=$db_host -p pg.port=$db_port -p pg.user=$db_user -p pg.db=$db_name -p pg.sslmode=disable
&lt;snippet&gt;
Run finished, takes 14m8.828228178s
INSERT - Takes(s): 848.8, Count: 4981537, OPS: 5868.8, Avg(us): 1968, Min(us): 226, Max(us): 19327, 99th(us): 11623, 99.9th(us): 17567, 99.99th(us): 19119
READ   - Takes(s): 848.8, Count: 93786269, OPS: 110492.3, Avg(us): 5899, Min(us): 78, Max(us): 41247, 99th(us): 32831, 99.9th(us): 40063, 99.99th(us): 41119
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/pingcap/go-ycsb">https://github.com/pingcap/go-ycsb</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Getting started with PostgreSQL]]></title><description><![CDATA[PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads. ]]></description><link>https://www.flamingbytes.com/blog/getting-started-with-postgresql/</link><guid isPermaLink="false">63eb11f1361c0d0ddf89fb77</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 13 Feb 2023 05:02:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1489875347897-49f64b51c1f8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG15c3FsfGVufDB8fHx8MTY3NjM1MDkxOQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="install-the-postgresql-from-yum-repository">Install the PostgreSQL from YUM repository</h2>
<img src="https://images.unsplash.com/photo-1489875347897-49f64b51c1f8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG15c3FsfGVufDB8fHx8MTY3NjM1MDkxOQ&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Getting started with PostgreSQL"><p><strong>Install the repository RPM:</strong></p>
<pre><code>$ yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
</code></pre>
<p><strong>Install PostgreSQL:</strong></p>
<pre><code>$ yum install -y postgresql15-server
</code></pre>
<h2 id="create-a-database-cluster">Create a Database Cluster</h2>
<p>Before you can do anything, you must initialize a database storage area on disk. We call this a database cluster.</p>
<pre><code>$ su - postgres

-bash-4.2$ /usr/pgsql-15/bin/pg_ctl --help
pg_ctl is a utility to initialize, start, stop, or control a PostgreSQL server.

Usage:
  pg_ctl init[db]   [-D DATADIR] [-s] [-o OPTIONS]
  pg_ctl start      [-D DATADIR] [-l FILENAME] [-W] [-t SECS] [-s]
                    [-o OPTIONS] [-p PATH] [-c]
  pg_ctl stop       [-D DATADIR] [-m SHUTDOWN-MODE] [-W] [-t SECS] [-s]
  pg_ctl restart    [-D DATADIR] [-m SHUTDOWN-MODE] [-W] [-t SECS] [-s]
                    [-o OPTIONS] [-c]
  pg_ctl reload     [-D DATADIR] [-s]
  pg_ctl status     [-D DATADIR]
  pg_ctl promote    [-D DATADIR] [-W] [-t SECS] [-s]
  pg_ctl logrotate  [-D DATADIR] [-s]
  pg_ctl kill       SIGNALNAME PID

Common options:
  -D, --pgdata=DATADIR   location of the database storage area
  -s, --silent           only print errors, no informational messages
  -t, --timeout=SECS     seconds to wait when using -w option
  -V, --version          output version information, then exit
  -w, --wait             wait until operation completes (default)
  -W, --no-wait          do not wait until operation completes
  -?, --help             show this help, then exit
If the -D option is omitted, the environment variable PGDATA is used.

Options for start or restart:
  -c, --core-files       allow postgres to produce core files
  -l, --log=FILENAME     write (or append) server log to FILENAME
  -o, --options=OPTIONS  command line options to pass to postgres
                         (PostgreSQL server executable) or initdb
  -p PATH-TO-POSTGRES    normally not necessary

Options for stop or restart:
  -m, --mode=MODE        MODE can be &quot;smart&quot;, &quot;fast&quot;, or &quot;immediate&quot;

Shutdown modes are:
  smart       quit after all clients have disconnected
  fast        quit directly, with proper shutdown (default)
  immediate   quit without complete shutdown; will lead to recovery on restart

Allowed signal names for kill:
  ABRT HUP INT KILL QUIT TERM USR1 USR2

Report bugs to &lt;pgsql-bugs@lists.postgresql.org&gt;.
PostgreSQL home page: &lt;https://www.postgresql.org/&gt;
</code></pre>
<p><strong>Create the data directory:</strong></p>
<pre><code>$ mkdir -p /mnt/pgsql15/data
$ chown -R postgres /mnt/pgsql15
</code></pre>
<p><strong>Create the database cluster:</strong></p>
<pre><code>$ su - postgres -c &quot;/usr/pgsql-15/bin/initdb -D /mnt/pgsql15/data&quot;

$ ls /mnt/pgsql15/data
base log pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots  pg_stat_tmp pg_tblspc PG_VERSION pg_xact postgresql.conf
global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_wal postgresql.auto.conf
</code></pre>
<p><strong>Start and stop the database server:</strong></p>
<pre><code>-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data -l logfile start
waiting for server to start.... done
server started

-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data stop
waiting for server to shut down.... done
server stopped

-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data -l logfile start
waiting for server to start.... done
server started

-bash-4.2$ ls -ltr
total 4
drwx------ 4 postgres postgres  51 Feb 13 20:48 15
-rw------- 1 postgres postgres 374 Feb 13 22:26 logfile
</code></pre>
<p><strong>Restart the database server:</strong></p>
<pre><code>-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data -l logfile restart
waiting for server to shut down.... done
server stopped
waiting for server to start.... done
server started
</code></pre>
<p><strong>Check the database service:</strong></p>
<pre><code>-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data status
pg_ctl: server is running (PID: 77038)
/usr/pgsql-15/bin/postgres &quot;-D&quot; &quot;/mnt/pgsql15/data&quot;

-bash-4.2$ ps aux | grep postgres | grep -v grep
root     73095  0.0  0.0 191900  4348 pts/0    S    21:49   0:00 su - postgres
postgres 73096  0.0  0.0 115560  3456 pts/0    S    21:49   0:00 -bash
postgres 77434  0.1  0.0 401372 23652 ?        Ss   22:26   0:00 /usr/pgsql-15/bin/postgres -D /mnt/pgsql15/data
postgres 77435  0.0  0.0 253204  5692 ?        Ss   22:26   0:00 postgres: logger
postgres 77436  0.0  0.0 401524  5728 ?        Ss   22:26   0:00 postgres: checkpointer
postgres 77437  0.0  0.0 401508  5784 ?        Ss   22:26   0:00 postgres: background writer
postgres 77439  0.0  0.0 401508 10372 ?        Ss   22:26   0:00 postgres: walwriter
postgres 77440  0.0  0.0 402992  8864 ?        Ss   22:26   0:00 postgres: autovacuum launcher
postgres 77441  0.0  0.0 402976  6904 ?        Ss   22:26   0:00 postgres: logical replication launcher
postgres 77480  0.0  0.0 155468  3784 pts/0    R+   22:27   0:00 ps aux
</code></pre>
<h2 id="using-psql">Using psql</h2>
<p><a href="https://www.postgresql.org/docs/current/app-psql.html">psql</a> is a terminal-based front-end to PostgreSQL. It enables you to type in queries interactively, issue them to PostgreSQL, and see the query results. Alternatively, input can be from a file or from command line arguments. In addition, psql provides a number of meta-commands and various shell-like features to facilitate writing scripts and automating a wide variety of tasks.</p>
<p><strong>Create a database:</strong></p>
<pre><code>$ su - postgres
Last login: Mon Feb 13 20:12:36 UTC 2023 on pts/0

-bash-4.2$ psql
psql (15.2)
Type &quot;help&quot; for help.

postgres=# \l
                                                 List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
(3 rows)


postgres=# create database testdb;
CREATE DATABASE

postgres=# \l
                                                 List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 testdb    | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
(4 rows)
</code></pre>
<p><strong>Drop database:</strong></p>
<pre><code>postgres=# drop database if exists testdb;
DROP DATABASE
postgres=# \l
                                                 List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
(3 rows)
</code></pre>
<p><strong>Manipulate the database remotely:</strong></p>
<pre><code>psql --host=10.13.121.243 --port=5432 --username=postgres -w -c &quot;create database testdb&quot;
psql --host=10.13.121.243 --port=5432 --username=postgres -w -c &quot;\l&quot;
psql --host=10.13.121.243 --port=5432 --username=postgres -w -d testdb -c &quot;\dt+&quot;
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://www.postgresql.org/download/linux/redhat/">https://www.postgresql.org/download/linux/redhat/</a></li>
<li><a href="https://www.postgresql.org/docs/15/creating-cluster.html">https://www.postgresql.org/docs/15/creating-cluster.html</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Getting started with BCC (BPF Compiler Collection)]]></title><description><![CDATA[BCC is a toolkit for creating efficient kernel tracing and manipulation programs, and includes several useful tools and examples. ]]></description><link>https://www.flamingbytes.com/blog/getting-started-with-bcc/</link><guid isPermaLink="false">63e9a9a1f19d570e66aae077</guid><category><![CDATA[Performance]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Sun, 12 Feb 2023 03:40:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1661956602868-6ae368943878?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wxfDF8YWxsfDE2fHx8fHx8Mnx8MTY3NjI1NTU4OA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="intro-to-bcc">Intro to BCC</h2>
<img src="https://images.unsplash.com/photo-1661956602868-6ae368943878?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wxfDF8YWxsfDE2fHx8fHx8Mnx8MTY3NjI1NTU4OA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Getting started with BCC (BPF Compiler Collection)"><p><a href="https://github.com/iovisor/bcc">BCC</a> is a toolkit for creating efficient kernel tracing and manipulation programs, and includes several useful tools and examples. It makes use of extended BPF (Berkeley Packet Filters), formally known as eBPF, a new feature that was first added to Linux 3.15. Much of what BCC uses requires Linux 4.1 and above.</p>
<h2 id="install-bcc-from-packages">Install BCC from packages</h2>
<p>In general, a Linux kernel version 4.1 or newer is required.</p>
<pre><code>$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

$ uname -r
5.7.12-1.el7.elrepo.x86_64
</code></pre>
<p>In addition, the kernel should have been compiled with the following <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md">flags set</a>.</p>
<pre><code>CONFIG_BPF=y
CONFIG_BPF_SYSCALL=y
# [optional, for tc filters]
CONFIG_NET_CLS_BPF=m
# [optional, for tc actions]
CONFIG_NET_ACT_BPF=m
CONFIG_BPF_JIT=y
# [for Linux kernel versions 4.1 through 4.6]
CONFIG_HAVE_BPF_JIT=y
# [for Linux kernel versions 4.7 and later]
CONFIG_HAVE_EBPF_JIT=y
# [optional, for kprobes]
CONFIG_BPF_EVENTS=y
# Need kernel headers through /sys/kernel/kheaders.tar.xz
CONFIG_IKHEADERS=y
</code></pre>
<p>There are a few optional kernel flags needed for running bcc networking examples on vanilla kernel:</p>
<pre><code>CONFIG_NET_SCH_SFQ=m
CONFIG_NET_ACT_POLICE=m
CONFIG_NET_ACT_GACT=m
CONFIG_DUMMY=m
CONFIG_VXLAN=m
</code></pre>
<p>These kernel configuration might be set by default after the OS installation but you should double check as below.</p>
<pre><code>$ cat /boot/config-$(uname -r)
</code></pre>
<p><strong>To install the BCC tools from the official yum repository:</strong></p>
<pre><code>$ yum install bcc-tools
Installed:
  bcc-tools.x86_64 0:0.10.0-1.el7

Dependency Installed:
  bcc.x86_64 0:0.10.0-1.el7   python-bcc.x86_64 0:0.10.0-1.el7
</code></pre>
<p>The following BCC tools are pre-defined and available to use after installation.</p>
<pre><code>$ cd /usr/share/bcc
$ ls
introspection  tools
$ cd /usr/share/bcc/tools
$ ls
argdist bpflist cobjnew dcstat ext4dist funclatency javagc llcstat nfsslower opensnoop phpstat pythonstat rubystat sofdsnoop syncsnoop tcpaccept tcpsubnet vfscount bashreadline btrfsdist cpudist deadlock ext4slower funcslower javaobjnew mdflush nodegc perlcalls  pidpersec reset-trace runqlat softirqs syscount tcpconnect tcptop vfsstat biolatency btrfsslower cpuunclaimed deadlock.c filelife gethostlatency  javastat memleak nodestat perlflow profile rubycalls runqlen solisten tclcalls tcpconnlat tcptracer wakeuptime biosnoop cachestat dbslower doc fileslower hardirqs javathreads mountsnoop offcputime perlstat pythoncalls rubyflow runqslower sslsniff tclflow tcpdrop tplist xfsdist biotop cachetop dbstat drsnoop filetop javacalls killsnoop mysqld_qslower offwaketime phpcalls pythonflow rubygc shmsnoop stackcount tclobjnew tcplife trace xfsslower bitesize capable dcsnoop execsnoop funccount javaflow lib nfsdist oomkill phpflow pythongc rubyobjnew slabratetop statsnoop tclstat tcpretrans ttysnoop
</code></pre>
<p><strong>To add bcc directory to the $PATH:</strong></p>
<pre><code>$ vim .bash_profile
bcctools=/usr/share/bcc/tools
PATH=$PATH:$HOME/bin:$bcctools
export PATH
    
$ source ~/.bash_profile
$ echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin:/usr/share/bcc/tools
</code></pre>
<h2 id="install-bcc-from-source">Install BCC from source</h2>
<p>If you want to install a different version of BCC, you can refer to <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md">here</a>. I tried this but it seems very tricky to install it successfully. I&apos;ll not discuss it in this post.</p>
<h2 id="use-the-bcc-tools">Use the BCC tools</h2>
<p>It&apos;s not suprise if you see the following error for the first time run of BCC tools.</p>
<pre><code>$ biolatency
In file included from /virtual/main.c:2:
In file included from /lib/modules/5.7.12-1.el7.elrepo.x86_64/build/include/uapi/linux/ptrace.h:142:
In file included from /lib/modules/5.7.12-1.el7.elrepo.x86_64/build/arch/x86/include/asm/ptrace.h:5:
/lib/modules/5.7.12-1.el7.elrepo.x86_64/build/arch/x86/include/asm/segment.h:266:2: error: expected &apos;(&apos; after &apos;asm&apos;
        alternative_io (&quot;lsl %[seg],%[p]&quot;,
        ^
/lib/modules/5.7.12-1.el7.elrepo.x86_64/build/arch/x86/include/asm/alternative.h:240:2: note: expanded from macro &apos;alternative_io&apos;
        asm_inline volatile (ALTERNATIVE(oldinstr, newinstr, feature)   \
        ^
/lib/modules/5.7.12-1.el7.elrepo.x86_64/build/include/linux/compiler_types.h:201:24: note: expanded from macro &apos;asm_inline&apos;
#define asm_inline asm __inline
                       ^
In file included from /virtual/main.c:3:
</code></pre>
<p>This is because many BCC tools are broken with kernel 5.4+ and libbcc 0.10.</p>
<p><strong>To fix this problem:</strong></p>
<p>Modify as below for the BPF program definition.</p>
<p>Original code:</p>
<pre><code>$ vim biolatency
&lt;snippet&gt;
# define BPF program
bpf_text = &quot;&quot;&quot;
#include &lt;uapi/linux/ptrace.h&gt;
#include &lt;linux/blkdev.h&gt;
&lt;snippet&gt;
</code></pre>
<p>Modified code:</p>
<pre><code>$ vim biolatency
&lt;snippet&gt;
# define BPF program
bpf_text = &quot;&quot;&quot;
#ifdef asm_inline
#undef asm_inline
#define asm_inline asm
#endif
#include &lt;uapi/linux/ptrace.h&gt;
#include &lt;linux/blkdev.h&gt;
&lt;snippet&gt;
</code></pre>
<p><strong>Run the BCC tool again now:</strong></p>
<pre><code>$ biolatency
Tracing block device I/O... Hit Ctrl-C to end.
^C
    usecs               : count     distribution
    0 -&gt; 1          : 0        |                                        |
    2 -&gt; 3          : 0        |                                        |
    4 -&gt; 7          : 0        |                                        |
    8 -&gt; 15         : 0        |                                        |
    16 -&gt; 31        : 0        |                                        |
    32 -&gt; 63        : 6        |********************                    |
    64 -&gt; 127       : 12       |****************************************|
    128 -&gt; 255      : 3        |**********                              |
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/iovisor/bcc">https://github.com/iovisor/bcc</a></li>
<li><a href="https://blogs.oracle.com/linux/post/intro-to-bcc-1">https://blogs.oracle.com/linux/post/intro-to-bcc-1</a></li>
<li><a href="https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/12052">https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/12052</a></li>
<li><a href="https://blog.csdn.net/thesre/article/details/122508493">https://blog.csdn.net/thesre/article/details/122508493</a></li>
<li><a href="https://www.containiq.com/post/bcc-tools">https://www.containiq.com/post/bcc-tools</a></li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Install GCC from source]]></title><description><![CDATA[In this post, we will go through the steps to install GCC.]]></description><link>https://www.flamingbytes.com/blog/install-gcc-from-source/</link><guid isPermaLink="false">63e9a08bf19d570e66aae054</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Fri, 10 Feb 2023 02:33:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1675959449383-1c7a27684c3c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8YWxsfDJ8fHx8fHwyfHwxNjc2MjU1NTg4&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1675959449383-1c7a27684c3c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8YWxsfDJ8fHx8fHwyfHwxNjc2MjU1NTg4&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Install GCC from source"><p>In this post, we will go through the steps to install <a href="https://gcc.gnu.org/">GCC</a> from source on CentOS 7.</p>
<pre><code>$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

$ uname -r
5.7.12-1.el7.elrepo.x86_64
</code></pre>
<h2 id="download-gcc-source">Download GCC <a href="https://gcc.gnu.org/pub/gcc/releases/">source</a></h2>
<pre><code>$ curl -LO https://gcc.gnu.org/pub/gcc/releases/gcc-12.2.0/gcc-12.2.0.tar.gz
</code></pre>
<p>Untar the source file:</p>
<pre><code>$ tar zxf gcc-12.2.0.tar.gz
</code></pre>
<h2 id="download-the-dependent-libraries">Download the dependent libraries</h2>
<pre><code>$ cd gcc-12.2.0/
$ ./contrib/download_prerequisites
2023-02-01 00:41:01 URL:http://gcc.gnu.org/pub/gcc/infrastructure/gmp-6.2.1.tar.bz2 [2493916/2493916] -&gt; &quot;gmp-6.2.1.tar.bz2&quot; [1]
2023-02-01 00:41:02 URL:http://gcc.gnu.org/pub/gcc/infrastructure/mpfr-4.1.0.tar.bz2 [1747243/1747243] -&gt; &quot;mpfr-4.1.0.tar.bz2&quot; [1]
2023-02-01 00:41:03 URL:http://gcc.gnu.org/pub/gcc/infrastructure/mpc-1.2.1.tar.gz [838731/838731] -&gt; &quot;mpc-1.2.1.tar.gz&quot; [1]
2023-02-01 00:41:04 URL:http://gcc.gnu.org/pub/gcc/infrastructure/isl-0.24.tar.bz2 [2261594/2261594] -&gt; &quot;isl-0.24.tar.bz2&quot; [1]
gmp-6.2.1.tar.bz2: OK
mpfr-4.1.0.tar.bz2: OK
mpc-1.2.1.tar.gz: OK
isl-0.24.tar.bz2: OK
All prerequisites downloaded successfully.
</code></pre>
<h2 id="configure-and-compile-the-source">Configure and compile the source</h2>
<pre><code>$ ./configure --enable-languages=c,c++ --disable-multilib
$ make -j$(nproc)
</code></pre>
<h2 id="install-gcc">Install GCC</h2>
<pre><code>$ make install
</code></pre>
<h2 id="verify-gcc-version">Verify GCC version</h2>
<pre><code>$ gcc --version
gcc (GCC) 12.2.0
</code></pre>
<h2 id="write-a-hello-world-c-program">Write a  &quot;hello world&quot; C program</h2>
<pre><code>$ vi hello.c
#include &lt;stdio.h&gt;
void main()
{
printf(&quot;Hello world!\n&quot;);
}
</code></pre>
<h2 id="compile-and-run-the-program">Compile and run the program</h2>
<pre><code>$ gcc -o hello hello.c
$ ./hello
Hello world!
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Surprised! How can the write IOPS be 10x faster than the cloud native disk performance]]></title><description><![CDATA[<p>In a competitive fio write performance benchmark between 3rd party storage solution and cloud native disk, we noticed that the write performance for the 3rd party storage solution is 10x faster than the cloud native. This usually seems impossible since we would argue that no one can beat the raw</p>]]></description><link>https://www.flamingbytes.com/blog/surprised-how-can-the-write-throughput-be-10x-faster-than-the-cloud-native-disk-performance/</link><guid isPermaLink="false">63d0cefe2eaec20e4c700378</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 25 Jan 2023 07:18:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1457969414820-5fdd86fc0b84?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fHJhY2V8ZW58MHx8fHwxNjc0NjMwNzE5&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1457969414820-5fdd86fc0b84?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fHJhY2V8ZW58MHx8fHwxNjc0NjMwNzE5&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance"><p>In a competitive fio write performance benchmark between 3rd party storage solution and cloud native disk, we noticed that the write performance for the 3rd party storage solution is 10x faster than the cloud native. This usually seems impossible since we would argue that no one can beat the raw disk performance. The following case shows an interesting write performance optimization from the storage solution under test.</p><!--kg-card-begin: markdown--><h2 id="cloud-native-raw-disk-performancefio4kwrite">Cloud native raw disk performance(fio,4k,write)</h2>
<p>From the benchmark result, it shows the IOPS limit of the cloud drive is ~7500. This aligns with the cloud storage SPEC in use.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/10xfast_1.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"><br>
<img src="https://www.flamingbytes.com/content/images/posts/10xfast_2.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"><br>
<img src="https://www.flamingbytes.com/content/images/posts/10xfast_3.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"></p>
<!--kg-card-end: markdown--><p></p><!--kg-card-begin: markdown--><h2 id="10x-faster-iops-is-observed">10x faster IOPS is observed!!!</h2>
<p>In the output of fio, the IOPS(&gt;75k) is 10x faster than the result from cloud native raw disk(~7500). This can be verified from the iostat output. The IOPS at logic volume layer is aligned with the fio output.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/10xfast_4.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"><br>
<img src="https://www.flamingbytes.com/content/images/posts/10xfast_5.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"><br>
<img src="https://www.flamingbytes.com/content/images/posts/10xfast_6.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="iosize-matters">Iosize matters!!!</h2>
<p>When the write request comes to the physical disk from logic volume, the I/O size is changed from 4k to 400k. It indicates the smaller writes got merged to larger writes.</p>
<p><img src="https://www.flamingbytes.com/content/images/posts/10xfast_7.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"><br>
<img src="https://www.flamingbytes.com/content/images/posts/10xfast_8.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"><br>
<img src="https://www.flamingbytes.com/content/images/posts/10xfast_9.jpg" alt="Surprised! How can the write IOPS be 10x faster than the cloud native disk performance" loading="lazy"></p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="conclusion">Conclusion</h2>
<p>With limited IOPS on the cloud drive, larger IO size really help improve the write performance by reducing the write requests to disk. With proper write performance optimization at logic volume layer, the write performance can be boosted 10x faster with no surprise!!!</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[fio direct I/O error with 1k blocksize]]></title><description><![CDATA[<p>When to run fio write with small blocksize(e.g. 1k), the following error is seen.</p><pre><code>$ fio --blocksize=1k --ioengine=libaio --readwrite=randwrite --filesize=2G --group_reporting --direct=1 --iodepth=128 --randrepeat=1 --end_fsync=1 --
name=job1 --numjobs=1 --filename=/mnt/fiomnt/fio.dat
job1: (g=0): rw=</code></pre>]]></description><link>https://www.flamingbytes.com/blog/fio-direct-i-o-error-with-1k-blocksize/</link><guid isPermaLink="false">63d0cadc2eaec20e4c700329</guid><category><![CDATA[Benchmarking]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Mon, 23 Jan 2023 06:39:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1566633806501-349d557a616d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGFsaWdufGVufDB8fHx8MTY3NDYyODcwMw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1566633806501-349d557a616d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGFsaWdufGVufDB8fHx8MTY3NDYyODcwMw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="fio direct I/O error with 1k blocksize"><p>When to run fio write with small blocksize(e.g. 1k), the following error is seen.</p><pre><code>$ fio --blocksize=1k --ioengine=libaio --readwrite=randwrite --filesize=2G --group_reporting --direct=1 --iodepth=128 --randrepeat=1 --end_fsync=1 --
name=job1 --numjobs=1 --filename=/mnt/fiomnt/fio.dat
job1: (g=0): rw=randwrite, bs=(R) 1024B-1024B, (W) 1024B-1024B, (T) 1024B-1024B, ioengine=libaio, iodepth=128
fio-3.7
Starting 1 process
fio: io_u error on file /mnt/fiomnt/fio.dat: Invalid argument: write offset=129521664, buflen=1024
fio: io_u error on file /mnt/fiomnt/fio.dat: Invalid argument: write offset=1589760000, buflen=1024
fio: pid=93922, err=22/file:io_u.c:1747, func=io_u error, error=Invalid argument
job1: (groupid=0, jobs=1): err=22 (file:io_u.c:1747, func=io_u error, error=Invalid argument): pid=93922: Wed Jan 25 00:42:00 2023
cpu : usr=0.00%, sys=0.00%, ctx=1, majf=0, minf=14
IO depths : 1=0.8%, 2=1.6%, 4=3.1%, 8=6.2%, 16=12.5%, 32=25.0%, &amp;gt;=64=50.8%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
issued rwts: total=0,128,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=128</code></pre><p><strong>Cause:</strong></p><p>For direct I/O, &#xA0;the I/O size has to be multiple of filesystem/block device blocksize. In this case, the filesystem blocksize is 4k which can not be well aligned with the requested I/O size(1k). To fix this, the filesystem blocksize should be less than or equal to 1k.</p>]]></content:encoded></item><item><title><![CDATA[Tracking stock financial metrics in tradingview]]></title><description><![CDATA[In this post, we learn how to use tradingview.com to track financial metrics of the interested stock. It helps understand how the financial metrics support the stock price under the hood.]]></description><link>https://www.flamingbytes.com/blog/tracking-stock-financial-metrics-in-tradingview/</link><guid isPermaLink="false">63c5f75c7f98ed0ceabaa24f</guid><category><![CDATA[Investment]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 17 Jan 2023 05:35:07 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1560221328-12fe60f83ab8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHN0b2NrfGVufDB8fHx8MTY3MzkxOTM4NA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1560221328-12fe60f83ab8?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHN0b2NrfGVufDB8fHx8MTY3MzkxOTM4NA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Tracking stock financial metrics in tradingview"><p></p><p>In this post, we learn how to use tradingview.com to track financial metrics of the interested stock. It helps understand how the financial metrics support the stock price under the hood.</p><!--kg-card-begin: markdown--><h2 id="display-the-stock-price-in-year-view">Display the stock price in year view</h2>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/12m_price.png" class="kg-image" alt="Tracking stock financial metrics in tradingview" loading="lazy" width="1547" height="881" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/12m_price.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/12m_price.png 1000w, https://www.flamingbytes.com/content/images/2023/01/12m_price.png 1547w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h2 id="add-the-revenue-metric">Add the revenue metric</h2>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/revenue.png" class="kg-image" alt="Tracking stock financial metrics in tradingview" loading="lazy" width="1543" height="881" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/revenue.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/revenue.png 1000w, https://www.flamingbytes.com/content/images/2023/01/revenue.png 1543w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h2 id="add-the-operating-income-metric">Add the operating income metric</h2>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/operating_income.png" class="kg-image" alt="Tracking stock financial metrics in tradingview" loading="lazy" width="1544" height="880" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/operating_income.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/operating_income.png 1000w, https://www.flamingbytes.com/content/images/2023/01/operating_income.png 1544w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h2 id="add-the-current-ratio-metric">Add the current ratio metric</h2>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/quick_ratio.png" class="kg-image" alt="Tracking stock financial metrics in tradingview" loading="lazy" width="1543" height="877" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/quick_ratio.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/quick_ratio.png 1000w, https://www.flamingbytes.com/content/images/2023/01/quick_ratio.png 1543w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h2 id="add-the-pe-ratio">Add the P/E ratio</h2>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/pe.png" class="kg-image" alt="Tracking stock financial metrics in tradingview" loading="lazy" width="1540" height="875" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/pe.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/pe.png 1000w, https://www.flamingbytes.com/content/images/2023/01/pe.png 1540w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h2 id="check-the-trend-for-the-added-metrics">Check the trend for the added metrics</h2>
<p>In the case of Tesla stock, the prive surged in 2020 and 2021. Notice that the revenue and operating income increased tremendously at the same time.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/indicators.png" class="kg-image" alt="Tracking stock financial metrics in tradingview" loading="lazy" width="1545" height="881" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/indicators.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/indicators.png 1000w, https://www.flamingbytes.com/content/images/2023/01/indicators.png 1545w" sizes="(min-width: 720px) 720px"></figure>]]></content:encoded></item><item><title><![CDATA[Using ANOVA in R to analyze US COVID data to understand age impact to death rate]]></title><description><![CDATA[A great example to use ANOVA in R to analyze complicated data.]]></description><link>https://www.flamingbytes.com/blog/using-anova-in-r-to-analyze-us-covid-data/</link><guid isPermaLink="false">63c4d8dafc850b0e49cce665</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Thu, 12 Jan 2023 04:57:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1583946099379-f9c9cb8bc030?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxjb3ZpZCUyMDE5fGVufDB8fHx8MTY3Mzg0NTEyMA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="goal">Goal</h2>
<img src="https://images.unsplash.com/photo-1583946099379-f9c9cb8bc030?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxjb3ZpZCUyMDE5fGVufDB8fHx8MTY3Mzg0NTEyMA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using ANOVA in R to analyze US COVID data to understand age impact to death rate"><p>We want to analysis the relationship between age and covid death rate in US.</p>
<h2 id="download-the-data">Download the data</h2>
<p>We will use NCHS(National Center for Health Statistics) as our data source.<br>
Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified</a>, and search <code>VSRR Quarterly</code>, we will find the data we are intrest.</p>
<p><a href="https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x">https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x</a>, in this page, we can export data into csv file.</p>
<p>With that, we may can download data source csv, <code>NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv</code></p>
<h2 id="load-the-data">Load the data</h2>
<pre><code>library(&quot;dplyr&quot;)
library(&quot;ggplot2&quot;)
library(&quot;janitor&quot;)
library(&quot;tidyr&quot;)
library(&quot;readr&quot;)
library(&quot;sjPlot&quot;)
df &lt;- readr::read_csv(file.path(getwd(), &quot;NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv&quot;), col_names = TRUE)
df &lt;- clean_names(df)
# To fix the colname format from origin csv file
df &lt;- df %&gt;% rename(&quot;rate_age_65_74&quot; = &quot;rate_65_74&quot;)
</code></pre>
<h2 id="filter-and-select">Filter and Select</h2>
<pre><code>df1 &lt;- df %&gt;%
    filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;COVID-19&quot;)) %&gt;%
    select(year_and_quarter, rate_age_1_4, rate_age_5_14,rate_age_15_24, rate_age_25_34,rate_age_35_44, rate_age_45_54,rate_age_55_64,rate_age_65_74,rate_age_75_84,rate_age_85_plus)
  # take a quick look from column&apos;s point of view
!&gt; glimpse(df1)
 Rows: 14
 Columns: 11
 $ year_and_quarter &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q2&quot;, &quot;2019 Q3&quot;, &quot;2019 Q4&quot;, &quot;2020 Q1&quot;&#x2026;
 $ rate_age_1_4     &lt;dbl&gt; NA, NA, NA, NA, NA, 0.2, NA, 0.2, 0.2, 0.2, 0.6, 0.4,&#x2026;
 $ rate_age_5_14    &lt;dbl&gt; NA, NA, NA, NA, NA, 0.1, 0.2, 0.2, 0.2, 0.1, 0.6, 0.5&#x2026;
 $ rate_age_15_24   &lt;dbl&gt; NA, NA, NA, NA, 0.1, 1.4, 1.5, 1.6, 1.9, 1.0, 5.9, 4.&#x2026;
 $ rate_age_25_34   &lt;dbl&gt; NA, NA, NA, NA, 0.8, 6.6, 5.4, 6.7, 8.9, 4.6, 23.8, 1&#x2026;
 $ rate_age_35_44   &lt;dbl&gt; NA, NA, NA, NA, 2.1, 18.8, 16.1, 20.6, 25.1, 11.9, 65&#x2026;
 $ rate_age_45_54   &lt;dbl&gt; NA, NA, NA, NA, 4.9, 56.1, 42.8, 64.4, 79.9, 33.9, 14&#x2026;
 $ rate_age_55_64   &lt;dbl&gt; NA, NA, NA, NA, 9.3, 129.8, 96.0, 162.0, 201.9, 67.4,&#x2026;
 $ rate_age_65_74   &lt;dbl&gt; NA, NA, NA, NA, 19.0, 293.6, 206.7, 413.6, 464.8, 109&#x2026;
 $ rate_age_75_84   &lt;dbl&gt; NA, NA, NA, NA, 43.9, 725.6, 465.8, 1112.4, 1110.7, 1&#x2026;
 $ rate_age_85_plus &lt;dbl&gt; NA, NA, NA, NA, 97.7, 2210.8, 1127.2, 3101.9, 2783.8,&#x2026;
</code></pre>
<h2 id="pivotlonger-to-reorg-the-data-for-grouping"><code>pivot_longer</code> to reorg the data for grouping</h2>
<pre><code>df2 = df1 %&gt;% pivot_longer(names_to = &quot;rate_type&quot;, values_to = &quot;rate_of_10k&quot;,cols = -c(year_and_quarter))
# convert NA to 0
df2 &lt;- df2 %&gt;%
    mutate_at(c(&quot;rate_of_10k&quot;), ~coalesce(.,0))
!&gt; df2
 # A tibble: 140 &#xD7; 3
    year_and_quarter rate_type        rate_of_10k
    &lt;chr&gt;            &lt;chr&gt;                  &lt;dbl&gt;
  1 2019 Q1          rate_age_1_4               0
  2 2019 Q1          rate_age_5_14              0
  3 2019 Q1          rate_age_15_24             0
  4 2019 Q1          rate_age_25_34             0
  5 2019 Q1          rate_age_35_44             0
  6 2019 Q1          rate_age_45_54             0
  7 2019 Q1          rate_age_55_64             0
  8 2019 Q1          rate_age_65_74             0
  9 2019 Q1          rate_age_75_84             0
 10 2019 Q1          rate_age_85_plus           0	
</code></pre>
<h2 id="draw-diagram-for-each-group">Draw diagram for each group</h2>
<pre><code>plotdata &lt;- df2 %&gt;%
    group_by(rate_type) %&gt;%
    summarize(n = n(),
            mean = mean(rate_of_10k),
            sd = sd(rate_of_10k),
            ci = qt(0.975, df = n - 1) * sd / sqrt(n))
p = ggplot(plotdata,
       aes(x = factor(rate_type, level = c(&apos;rate_age_1_4&apos;,&apos;rate_age_5_14&apos;,&apos;rate_age_15_24&apos;,&apos;rate_age_25_34&apos;,&apos;rate_age_35_44&apos;,&apos;rate_age_45_54&apos;,&apos;rate_age_55_64&apos;,&apos;rate_age_65_74&apos;,&apos;rate_age_75_84&apos;,&apos;rate_age_85_plus&apos;))
         , y = mean, group = 1)) +
    geom_line(linetype=&quot;dashed&quot;, color=&quot;darkgrey&quot;) +
    geom_errorbar(aes(ymin = mean - ci,
                      ymax = mean + ci),
                  width = .2) +
    geom_point(size = 3, color=&quot;red&quot;) +
    scale_y_continuous(breaks=seq(0,1800,100)) +
    theme_bw() +
    labs(x=&quot;rate_type&quot;,
         y=&quot;rate_of_10k&quot;,
         title=&quot;Mean Plot with 95% Confidence Interval&quot;)
save_plot(&quot;covid_plot_age_impact1.svg&quot;, fig = p, width=30, height=20)
</code></pre>
<p><img src="https://www.flamingbytes.com/content/images/posts/covid_plot_age_impact1.svg" alt="Using ANOVA in R to analyze US COVID data to understand age impact to death rate" loading="lazy"></p>
<h2 id="anova-analysis">ANOVA analysis</h2>
<p>Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the &quot;variation&quot; among and between groups) used to analyze the differences among means.<br>
It&apos;s very useful in our case, <code>aov</code> from R make it very easy to use, see <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/aov">https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/aov</a> for more detail.</p>
<pre><code>fit &lt;- aov(rate_of_10k ~ rate_type, data=df2)
summary(fit)
!&gt; summary(fit)
              Df   Sum Sq Mean Sq F value   Pr(&gt;F)
 rate_type     9 12988087 1443121   10.21 8.49e-12 ***
 Residuals   130 18370513  141312
 ---
 Signif. codes:  0 &#x2018;***&#x2019; 0.001 &#x2018;**&#x2019; 0.01 &#x2018;*&#x2019; 0.05 &#x2018;.&#x2019; 0.1 &#x2018; &#x2019; 1
</code></pre>
<p>we saw <code>8.49e-12 ***</code>, which means age has big impact on covid death rate.</p>
<h2 id="compare-diffrent-age-ranges">Compare diffrent age ranges</h2>
<pre><code>pairwise &lt;- TukeyHSD(fit)
pairwise

 + pairwise
 &gt;   Tukey multiple comparisons of means
     95% family-wise confidence level

 Fit: aov(formula = rate_of_10k ~ rate_type, data = df2)

 $rate_type
                                          diff        lwr       upr     p adj
 rate_age_15_24-rate_age_1_4        1.25000000 -456.16974  458.6697 1.0000000
 rate_age_25_34-rate_age_1_4        5.87857143 -451.54117  463.2983 1.0000000
 rate_age_35_44-rate_age_1_4       16.50714286 -440.91259  473.9269 1.0000000
 rate_age_45_54-rate_age_1_4       43.45714286 -413.96259  500.8769 0.9999996
 rate_age_5_14-rate_age_1_4        -0.02857143 -457.44831  457.3912 1.0000000
 ....
</code></pre>
<p>if <code>p adj</code> is close to 1, it means <code>no big difference</code> for this pair<br>
if <code>p adj</code> is close to 0, it means <code>big difference</code> for this pair</p>
<h2 id="draw-diagram-for-pairs-comparisons">Draw diagram for pairs comparisons</h2>
<pre><code>plotdata &lt;- as.data.frame(pairwise[[1]])
plotdata$conditions &lt;- row.names(plotdata)
p = ggplot(data=plotdata, aes(x=conditions, y=diff)) +
  geom_errorbar(aes(ymin=lwr, ymax=upr, width=.2)) +
  geom_hline(yintercept=0, color=&quot;red&quot;, linetype=&quot;dashed&quot;) +
  geom_point(size=3, color=&quot;red&quot;) +
  theme_bw() +
  labs(y=&quot;Difference in mean levels&quot;, x=&quot;&quot;,
       title=&quot;95% family-wise confidence level&quot;) +
   coord_flip()
save_plot(&quot;covid_plot_age_impact2.svg&quot;, fig = p, width=30, height=20)
</code></pre>
<p><img src="https://www.flamingbytes.com/content/images/posts/covid_plot_age_impact2.svg" alt="Using ANOVA in R to analyze US COVID data to understand age impact to death rate" loading="lazy"></p>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>Age does have big impact for covid death rate.</li>
<li>85+ was impacted the most.</li>
<li>For 35-,  the death rate is very low, and no big difference from age 0-35.</li>
</ul>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using R to analyze US COVID pandemic waves and peaks]]></title><description><![CDATA[A great example to use R to analyze data.]]></description><link>https://www.flamingbytes.com/blog/using-r-to-analyze-us-covid-waves-and-peaks/</link><guid isPermaLink="false">63c2f4d2639c6f0d6ee203e3</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Wed, 11 Jan 2023 18:33:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1593007791459-4b05e1158229?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGNvdmlkfGVufDB8fHx8MTY3MzcyMTIyMw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="goal">Goal</h2>
<img src="https://images.unsplash.com/photo-1593007791459-4b05e1158229?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDh8fGNvdmlkfGVufDB8fHx8MTY3MzcyMTIyMw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using R to analyze US COVID pandemic waves and peaks"><p>According to weekly data between 2020 to 2022, we want to get to know the waves and peaks of COVID pandemic in these years.</p>
<h2 id="download-the-data">Download the data</h2>
<p>We will continue to use NCHS(National Center for Health Statistics) as our data source.<br>
Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified</a>, and search <code>Provisional COVID-19 Death Counts by Week</code>, we will find the data we are intrest.</p>
<p><a href="https://data.cdc.gov/NCHS/Provisional-COVID-19-Death-Counts-by-Week-Ending-D/r8kw-7aab">https://data.cdc.gov/NCHS/Provisional-COVID-19-Death-Counts-by-Week-Ending-D/r8kw-7aab</a>, in this page, we can export data into csv file.</p>
<p>With that, we may get the data source csv, <code>Provisional_COVID-19_Death_Counts_by_Week_Ending_Date_and_State.csv</code>.</p>
<h2 id="load-the-data">Load the data</h2>
<pre><code>library(&quot;dplyr&quot;)
library(&quot;janitor&quot;)
library(&quot;tidyr&quot;)
library(&quot;readr&quot;)
df &lt;- readr::read_csv(file.path(getwd(), &quot;Provisional_COVID-19_Death_Counts_by_Week_Ending_Date_and_State.csv&quot;), col_names = TRUE)
df &lt;- clean_names(df)
tmp_start_date &lt;- strptime(df$start_date, &quot;%m/%d/%Y&quot;)
df$start_date &lt;- format(tmp_start_date, &quot;%Y-%m-%d&quot;)

 &gt; glimpse(df)
 Rows: 10,800
 Columns: 17
 $ data_as_of                             &lt;chr&gt; &quot;01/09/2023&quot;, &quot;01/09/2023&quot;, &quot;01&#x2026;
 $ start_date                             &lt;chr&gt; &quot;2019-12-29&quot;, &quot;2020-01-05&quot;, &quot;20&#x2026;
 $ end_date                               &lt;chr&gt; &quot;01/04/2020&quot;, &quot;01/11/2020&quot;, &quot;01&#x2026;
 $ group                                  &lt;chr&gt; &quot;By Week&quot;, &quot;By Week&quot;, &quot;By Week&quot;&#x2026;
 $ year                                   &lt;chr&gt; &quot;2019/2020&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2&#x2026;
 $ month                                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA,&#x2026;
 $ mmwr_week                              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, &#x2026;
 $ week_ending_date                       &lt;chr&gt; &quot;01/04/2020&quot;, &quot;01/11/2020&quot;, &quot;01&#x2026;
 $ state                                  &lt;chr&gt; &quot;United States&quot;, &quot;United States&#x2026;
 $ covid_19_deaths                        &lt;dbl&gt; 0, 1, 2, 3, 0, 4, 6, 6, 9, 38, &#x2026;
 $ total_deaths                           &lt;dbl&gt; 60176, 60734, 59362, 59162, 588&#x2026;
 $ percent_of_expected_deaths             &lt;dbl&gt; 98, 97, 98, 99, 99, 100, 100, 1&#x2026;
 $ pneumonia_deaths                       &lt;dbl&gt; 4111, 4153, 4066, 3915, 3818, 3&#x2026;
 $ pneumonia_and_covid_19_deaths          &lt;dbl&gt; 0, 1, 2, 0, 0, 1, 1, 3, 5, 19, &#x2026;
 $ influenza_deaths                       &lt;dbl&gt; 434, 475, 468, 500, 481, 520, 5&#x2026;
 $ pneumonia_influenza_or_covid_19_deaths &lt;dbl&gt; 4545, 4628, 4534, 4418, 4299, 4&#x2026;
 $ footnote                               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA,&#x2026;

 !&gt; df
 # A tibble: 10,800 &#xD7; 17
    data_as_of start_date end_d&#x2026;&#xB9; group year  month mmwr_&#x2026;&#xB2; week_&#x2026;&#xB3; state covid&#x2026;&#x2074;
    &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;
  1 01/09/2023 2019-12-29 01/04/&#x2026; By W&#x2026; 2019&#x2026;    NA       1 01/04/&#x2026; Unit&#x2026;       0
  2 01/09/2023 2020-01-05 01/11/&#x2026; By W&#x2026; 2020     NA       2 01/11/&#x2026; Unit&#x2026;       1
  3 01/09/2023 2020-01-12 01/18/&#x2026; By W&#x2026; 2020     NA       3 01/18/&#x2026; Unit&#x2026;       2
  4 01/09/2023 2020-01-19 01/25/&#x2026; By W&#x2026; 2020     NA       4 01/25/&#x2026; Unit&#x2026;       3
  5 01/09/2023 2020-01-26 02/01/&#x2026; By W&#x2026; 2020     NA       5 02/01/&#x2026; Unit&#x2026;       0
  6 01/09/2023 2020-02-02 02/08/&#x2026; By W&#x2026; 2020     NA       6 02/08/&#x2026; Unit&#x2026;       4
  7 01/09/2023 2020-02-09 02/15/&#x2026; By W&#x2026; 2020     NA       7 02/15/&#x2026; Unit&#x2026;       6
  8 01/09/2023 2020-02-16 02/22/&#x2026; By W&#x2026; 2020     NA       8 02/22/&#x2026; Unit&#x2026;       6
  9 01/09/2023 2020-02-23 02/29/&#x2026; By W&#x2026; 2020     NA       9 02/29/&#x2026; Unit&#x2026;       9
 10 01/09/2023 2020-03-01 03/07/&#x2026; By W&#x2026; 2020     NA      10 03/07/&#x2026; Unit&#x2026;      38
 # &#x2026; with 10,790 more rows, 7 more variables: total_deaths &lt;dbl&gt;,
 #   percent_of_expected_deaths &lt;dbl&gt;, pneumonia_deaths &lt;dbl&gt;,
 #   pneumonia_and_covid_19_deaths &lt;dbl&gt;, influenza_deaths &lt;dbl&gt;,
 #   pneumonia_influenza_or_covid_19_deaths &lt;dbl&gt;, footnote &lt;chr&gt;, and
 #   abbreviated variable names &#xB9; end_date, &#xB2; mmwr_week, &#xB3; week_ending_date,
 #   &#x2074; covid_19_deaths
 # &#x2139; Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names
</code></pre>
<h2 id="identify-the-data-we-want-to-focus">Identify the data we want to focus</h2>
<p>As we can see, there are 4 diffrent groups, and it has the <code>whole United states</code> and each state&apos;s data</p>
<pre><code>&gt; unique(df$group)
[1] &quot;By Week&quot;  &quot;By Month&quot; &quot;By Year&quot;  &quot;By Total&quot;
&gt;
</code></pre>
<p>We only want to get the weekly data, so we may want to <code>filter</code> with &quot;group=By Week&quot;, and &quot;state=United states&quot;<br>
in the mean time, we may only want to <code>select</code> only 2 columns.</p>
<ul>
<li>start_date</li>
<li>covid_19_deaths</li>
</ul>
<pre><code>&gt; df1 &lt;- df %&gt;%
    filter(state == &quot;United States&quot; &amp; group == &quot;By Week&quot;) %&gt;%
    select(start_date, covid_19_deaths)
&gt; print(df1,n=20)
+ &gt; # A tibble: 158 &#xD7; 2
   start_date covid_19_deaths
   &lt;chr&gt;                &lt;dbl&gt;
 1 2019-12-29               0
 2 2020-01-05               1
 3 2020-01-12               2
 4 2020-01-19               3
 5 2020-01-26               0
 6 2020-02-02               4
 7 2020-02-09               6
 8 2020-02-16               6
 9 2020-02-23               9
10 2020-03-01              38
11 2020-03-08              60
12 2020-03-15             588
13 2020-03-22            3226
14 2020-03-29           10141
15 2020-04-05           16347
16 2020-04-12           17221
17 2020-04-19           15557
18 2020-04-26           13223
19 2020-05-03           11243
20 2020-05-10            9239
...

</code></pre>
<h2 id="draw-the-graph-to-see-the-wave">Draw the graph to see the wave</h2>
<pre><code>library(&quot;ggplot2&quot;)
library(&quot;sjPlot&quot;)
p = ggplot(df1, aes( x=start_date, y=covid_19_deaths, group=1)) +
    geom_line(color=&quot;blue&quot;) +
    theme(axis.text.x=element_text(angle=45,hjust=1,size=5))
save_plot(&quot;covid_plot_weekly_wave.svg&quot;, fig = p, width=60, height=20)
</code></pre>
<p><img src="https://www.flamingbytes.com/content/images/posts/covid_plot_weekly_wave.svg" alt="Using R to analyze US COVID pandemic waves and peaks" loading="lazy"></p>
<h2 id="find-the-peak-by-r-mark-it-in-the-graph">Find the peak by R mark it in the graph</h2>
<p>From above graph, we can easily to figure out the waves and peaks, but we also can let R help us to do it, it&apos;s pretty useful if we have to deal with many data and many graphs.</p>
<p>To achive it, firstly we can call <code>findpeaks</code> from <code>pracma</code> library to find the peaks</p>
<pre><code>library(&quot;pracma&quot;)
+ peaks = findpeaks(df1$covid_19_deaths, npeaks=5,  sortstr=TRUE)
&gt; &gt; peaks
      [,1] [,2] [,3] [,4]
[1,] 26027   54   40   66
[2,] 21364  108   98  121
[3,] 17221   16    8   26
[4,] 15536   88   79   98
[5,]  8308   31   26   38
&gt;
</code></pre>
<p>The 2nd column means the the row index of the peak. in this case, we can tell, the 54th row has the top peak covid death number <code>26027</code>.</p>
<p>It&apos;s not very obvious which week(start_date) is hitting the peak, so we can do something like this.</p>
<pre><code>is_peak &lt;- vector( &quot;logical&quot; , length(df1$covid_19_deaths ))
df1$is_peak = is_peak

for (x in peaks[,2]) {
  df1$is_peak[x] = TRUE
}
</code></pre>
<p>As you can see, we added a new column <code>is_peak</code>, so we can use it to filter out those none peak data, sort the peak data points.</p>
<pre><code>!&gt; df2 = df1 %&gt;% filter(is_peak == TRUE)
 + df2[order(-df2$covid_19_deaths),]
 &gt; # A tibble: 5 &#xD7; 3
   start_date covid_19_deaths is_peak
   &lt;chr&gt;                &lt;dbl&gt; &lt;lgl&gt;
 1 2021-01-03           26027 TRUE
 2 2022-01-16           21364 TRUE
 3 2020-04-12           17221 TRUE
 4 2021-08-29           15536 TRUE
 5 2020-07-26            8308 TRUE
 &gt; &gt;
</code></pre>
<h2 id="hightlight-the-peak-points">Hightlight the peak points</h2>
<pre><code>p = ggplot(df1, aes(x=start_date, y=covid_19_deaths, group=1)) +
    geom_line(color=&quot;blue&quot;) +
    geom_point(data = . %&gt;% filter(is_peak == TRUE), stat=&quot;identity&quot;, size = 4, color = &quot;red&quot;) +
    scale_y_continuous(breaks=seq(0,30000,4000)) +
    theme(axis.text.x=element_text(angle=45,hjust=1,size=5))

save_plot(&quot;covid_plot_weekly_peak.svg&quot;, fig = p, width=60, height=20)
</code></pre>
<p><img src="https://www.flamingbytes.com/content/images/posts/covid_plot_weekly_peak.svg" alt="Using R to analyze US COVID pandemic waves and peaks" loading="lazy"></p>
<h2 id="other-finding">Other finding</h2>
<pre><code>!&gt; &gt; sum(df1$covid_19_deaths)
 [1] 1089714   ===&gt; the total covid_19_deaths death number from 2020 to 2022

!&gt; summary(df1$covid_19_deaths)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
       0    2223    4428    6897    9862   26027
 &gt;
!&gt; df3 &lt;- df %&gt;%
 +     filter(state == &quot;United States&quot; &amp; group == &quot;By Week&quot;) %&gt;%
 +     select(start_date, total_deaths)
 + sum(df3$total_deaths)
 + &gt; [1] 10077273  ===&gt; the total death number from 2020 to 2022 
 + summary(df3$total_deaths)
 &gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    7100   58522   60451   63780   68610   87415
 &gt;
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using R to analyze the quarterly US COVID data]]></title><description><![CDATA[An R example to analyze big amount of data]]></description><link>https://www.flamingbytes.com/blog/analysis-us-quartely-covid-data-from-cdc/</link><guid isPermaLink="false">63bced3f16d5d70e3a76d7f2</guid><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[relentlesstorm]]></dc:creator><pubDate>Tue, 10 Jan 2023 05:02:33 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1454165804606-c3d57bc86b40?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1fHxkYXRhJTIwbWluaW5nfGVufDB8fHx8MTY3MzMyNTgxNw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><h2 id="goal">Goal</h2>
<img src="https://images.unsplash.com/photo-1454165804606-c3d57bc86b40?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1fHxkYXRhJTIwbWluaW5nfGVufDB8fHx8MTY3MzMyNTgxNw&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using R to analyze the quarterly US COVID data"><p>Based on the overall death data and COVID realated death data since 2019, we want to study the impact trend of COVID to the overall pupulation death of US in these years.</p>
<h2 id="download-the-data">Download the data</h2>
<p>We will use NCHS(National Center for Health Statistics) as our data source.<br>
Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified</a>, and search <code>VSRR Quarterly</code>, we will find the data we are intrested in.</p>
<p><a href="https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x">https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x</a></p>
<p>In this page, we can export data into csv file as <code>NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv</code></p>
<h2 id="take-a-quick-look-at-the-data">Take a quick look at the data</h2>
<p>To load the data:</p>
<pre><code># If &quot;readr&quot; not installed, run install.packages(&quot;readr&quot;) to install it
library(&quot;readr&quot;)
df &lt;- readr::read_csv(file.path(getwd(), &quot;NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv&quot;), col_names = TRUE)

</code></pre>
<p>To check the first few lines:</p>
<pre><code>&gt; head(df)
# A tibble: 6 &#xD7; 69
  Year a&#x2026;&#xB9; Time &#x2026;&#xB2; Cause&#x2026;&#xB3; Rate &#x2026;&#x2074; Unit  Overa&#x2026;&#x2075; Rate &#x2026;&#x2076; Rate &#x2026;&#x2077; Rate &#x2026;&#x2078; Rate &#x2026;&#x2079;
  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
1 2019 Q1  12 mon&#x2026; All ca&#x2026; Age-ad&#x2026; Deat&#x2026;   712.    600.    844.       NA      NA
2 2019 Q1  12 mon&#x2026; Alzhei&#x2026; Age-ad&#x2026; Deat&#x2026;    29.6    33.1    23.8      NA      NA
3 2019 Q1  12 mon&#x2026; COVID-&#x2026; Age-ad&#x2026; Deat&#x2026;    NA      NA      NA        NA      NA
4 2019 Q1  12 mon&#x2026; Cancer  Age-ad&#x2026; Deat&#x2026;   148.    128.    175.       NA      NA
5 2019 Q1  12 mon&#x2026; Chroni&#x2026; Age-ad&#x2026; Deat&#x2026;    11       7.7    14.7      NA      NA
6 2019 Q1  12 mon&#x2026; Chroni&#x2026; Age-ad&#x2026; Deat&#x2026;    38.5    35.7    42.4      NA      NA
# &#x2026; with 59 more variables: `Rate Age 15-24` &lt;dbl&gt;, `Rate Age 25-34` &lt;dbl&gt;,
#   `Rate Age 35-44` &lt;dbl&gt;, `Rate Age 45-54` &lt;dbl&gt;, `Rate Age 55-64` &lt;dbl&gt;,
#   `Rate 65-74` &lt;dbl&gt;, `Rate Age 75-84` &lt;dbl&gt;, `Rate Age 85 plus` &lt;dbl&gt;,
#   `Rate Alaska` &lt;dbl&gt;, `Rate Alabama` &lt;dbl&gt;, `Rate Arkansas` &lt;dbl&gt;,
#   `Rate Arizona` &lt;dbl&gt;, `Rate California` &lt;dbl&gt;, `Rate Colorado` &lt;dbl&gt;,
#   `Rate Connecticut` &lt;dbl&gt;, `Rate District of Columbia` &lt;dbl&gt;,
#   `Rate Delaware` &lt;dbl&gt;, `Rate Florida` &lt;dbl&gt;, `Rate Georgia` &lt;dbl&gt;, &#x2026;
# &#x2139; Use `colnames()` to see all variable names

</code></pre>
<p>To get a summary:</p>
<pre><code>summary(df)
 Year and Quarter   Time Period        Cause of Death      Rate Type        
 Length:1232        Length:1232        Length:1232        Length:1232       
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  

     Unit            Overall Rate     Rate Sex Female   Rate Sex Male    
 Length:1232        Min.   :   1.20   Min.   :   0.60   Min.   :   1.90  
 Class :character   1st Qu.:  11.40   1st Qu.:   6.95   1st Qu.:  13.20  
 Mode  :character   Median :  17.00   Median :  14.80   Median :  23.50  
                    Mean   :  80.51   Mean   :  70.56   Mean   :  91.63  
                    3rd Qu.:  50.60   3rd Qu.:  49.92   3rd Qu.:  65.42  
                    Max.   :1142.30   Max.   :1067.00   Max.   :1219.90  
                    NA&apos;s   :44        NA&apos;s   :44        NA&apos;s   :44       
  ...
</code></pre>
<p>To get glimpse from columns point of view:</p>
<pre><code>&gt; library(&quot;dplyr&quot;)
&gt; glimpse(df)
Rows: 1,232
Columns: 69
$ `Year and Quarter`          &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;&#x2026;
$ `Time Period`               &lt;chr&gt; &quot;12 months ending with quarter&quot;, &quot;12 month&#x2026;
$ `Cause of Death`            &lt;chr&gt; &quot;All causes&quot;, &quot;Alzheimer disease&quot;, &quot;COVID-&#x2026;
$ `Rate Type`                 &lt;chr&gt; &quot;Age-adjusted&quot;, &quot;Age-adjusted&quot;, &quot;Age-adjus&#x2026;
$ Unit                        &lt;chr&gt; &quot;Deaths per 100,000&quot;, &quot;Deaths per 100,000&quot;&#x2026;
$ `Overall Rate`              &lt;dbl&gt; 712.2, 29.6, NA, 148.1, 11.0, 38.5, 21.3, &#x2026;
$ `Rate Sex Female`           &lt;dbl&gt; 600.3, 33.1, NA, 127.9, 7.7, 35.7, 16.8, 1&#x2026;
$ `Rate Sex Male`             &lt;dbl&gt; 843.7, 23.8, NA, 175.4, 14.7, 42.4, 26.9, &#x2026;
$ `Rate Age 1-4`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 5-14`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 15-24`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 25-34`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 35-44`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 45-54`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 55-64`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate 65-74`                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
$ `Rate Age 75-84`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA&#x2026;
...
</code></pre>
<h2 id="clear-the-columns-name">Clear the columns name</h2>
<p>As we can see, the column names has space, it may cause some trouble while refering it in R, it&apos;s a common sugestion to convert space to &quot;_&quot; before doing any R o/p.</p>
<p>A good thing is , there is a R package can help us on it.</p>
<pre><code>&gt; library(&quot;janitor&quot;)
&gt; df &lt;- clean_names(df)
!&gt; glimpse(df)
 Rows: 1,232
 Columns: 69
 $ year_and_quarter          &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &#x2026;
 $ time_period               &lt;chr&gt; &quot;12 months ending with quarter&quot;, &quot;12 months &#x2026;
 $ cause_of_death            &lt;chr&gt; &quot;All causes&quot;, &quot;Alzheimer disease&quot;, &quot;COVID-19&#x2026;
 $ rate_type                 &lt;chr&gt; &quot;Age-adjusted&quot;, &quot;Age-adjusted&quot;, &quot;Age-adjuste&#x2026;
 $ unit                      &lt;chr&gt; &quot;Deaths per 100,000&quot;, &quot;Deaths per 100,000&quot;, &#x2026;
 $ overall_rate              &lt;dbl&gt; 712.2, 29.6, NA, 148.1, 11.0, 38.5, 21.3, 20&#x2026;
 $ rate_sex_female           &lt;dbl&gt; 600.3, 33.1, NA, 127.9, 7.7, 35.7, 16.8, 13.&#x2026;
 $ rate_sex_male             &lt;dbl&gt; 843.7, 23.8, NA, 175.4, 14.7, 42.4, 26.9, 27&#x2026;
 $ rate_age_1_4              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &#x2026;
</code></pre>
<p>As you can see, while we run <code>glimpse(df)</code>, the colum name has been changed from &quot;Year and Quarter&quot; to &quot;year_and_quarter&quot;.</p>
<h2 id="filter-and-select">Filter and select</h2>
<p>The raw data has so many columns and rows, but we just want to focus on those cols/rows we really have intrest.</p>
<p>We can use <code>filter</code>(applying on rows) and <code>select</code>(applying on columns) in such condition.</p>
<pre><code>&gt; df1 &lt;- df %&gt;%
      filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;All causes&quot;, &quot;COVID-19&quot;)) %&gt;%
      select(year_and_quarter, cause_of_death, overall_rate)
&gt; print(df1,n=50)
 # A tibble: 28 &#xD7; 3
    year_and_quarter cause_of_death overall_rate
    &lt;chr&gt;            &lt;chr&gt;                 &lt;dbl&gt;
  1 2019 Q1          All causes            910
  2 2019 Q1          COVID-19               NA
  3 2019 Q2          All causes            851.
  4 2019 Q2          COVID-19               NA
  5 2019 Q3          All causes            827.
  6 2019 Q3          COVID-19               NA
  7 2019 Q4          All causes            891.
  8 2019 Q4          COVID-19               NA
  9 2020 Q1          All causes            945.
 10 2020 Q1          COVID-19                8.2
 11 2020 Q2          All causes           1035.
 12 2020 Q2          COVID-19              137.
 13 2020 Q3          All causes            985.
 14 2020 Q3          COVID-19               87.3
 15 2020 Q4          All causes           1142.
 16 2020 Q4          COVID-19              193.
 17 2021 Q1          All causes           1116
 18 2021 Q1          COVID-19              191.
 19 2021 Q2          All causes            915.
 20 2021 Q2          COVID-19               42.4
 21 2021 Q3          All causes           1051.
 22 2021 Q3          COVID-19              138.
 23 2021 Q4          All causes           1092.
 24 2021 Q4          COVID-19              131.
 25 2022 Q1          All causes           1116
 26 2022 Q1          COVID-19              150.
 27 2022 Q2          All causes            899.
 28 2022 Q2          COVID-19               17.6
</code></pre>
<p><code>%&gt;%</code> looks strange, it&apos;s just like <code>| (pipe)</code> in linux shell command.</p>
<pre><code>df1 &lt;- df %&gt;%
    filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;All causes&quot;, &quot;COVID-19&quot;)) %&gt;%
    select(year_and_quarter, cause_of_death, overall_rate)
</code></pre>
<p>It means only keep those rows which meet those condition of <code>filter</code>, and those columns which meet condition of <code>select</code>.</p>
<h2 id="deal-with-na-value">Deal with NA value</h2>
<p>As you can see, in &quot;overall_rate&quot; column, there are few &quot;NA&quot; values, in this context it means 0, so we may want to convert it as 0 for future&apos;s process.</p>
<p>We can do it like as below.</p>
<pre><code> &gt; df1 &lt;- df1 %&gt;%
     mutate_at(c(&quot;overall_rate&quot;), ~coalesce(.,0))
</code></pre>
<p>It means , we want to convert all NA to 0 in &quot;overall_rate&quot; column.<br>
Now, let&apos;s check the df1 again, we can see all NA has been changed to 0.</p>
<pre><code>!+ &gt; df1
 # A tibble: 28 &#xD7; 3
    year_and_quarter cause_of_death overall_rate
    &lt;chr&gt;            &lt;chr&gt;                 &lt;dbl&gt;
  1 2019 Q1          All causes            910
  2 2019 Q1          COVID-19                0
  3 2019 Q2          All causes            851.
  4 2019 Q2          COVID-19                0
  5 2019 Q3          All causes            827.
  6 2019 Q3          COVID-19                0
  7 2019 Q4          All causes            891.
  8 2019 Q4          COVID-19                0
  9 2020 Q1          All causes            945.
 10 2020 Q1          COVID-19                8.2
 # &#x2026; with 18 more rows
</code></pre>
<h2 id="draw-diagram-for-the-whole-us-data">Draw diagram for the whole US data</h2>
<p>To draw a diagram directly:</p>
<pre><code>&gt; ggplot(df1, aes(fill=cause_of_death, x=year_and_quarter, y=overall_rate)) +
    geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) +
    geom_col() +
    geom_smooth(aes(group=cause_of_death)) +
    scale_y_continuous(breaks=seq(0,1500,100))
    theme_bw()
</code></pre>
<p>To save the diagram in a file:</p>
<pre><code>library(sjPlot)
p = ggplot(df1, aes(fill=cause_of_death, x=year_and_quarter, y=overall_rate)) +
    geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) +
    geom_col() +
    geom_smooth(aes(group=cause_of_death)) +
    scale_y_continuous(breaks=seq(0,1500,100)) +
    theme_bw()

save_plot(&quot;covid_plot.svg&quot;, fig = p, width=30, height=20)
</code></pre>
<p>The diagram looks like below.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://www.flamingbytes.com/content/images/2023/01/image-1.png" class="kg-image" alt="Using R to analyze the quarterly US COVID data" loading="lazy" width="1133" height="755" srcset="https://www.flamingbytes.com/content/images/size/w600/2023/01/image-1.png 600w, https://www.flamingbytes.com/content/images/size/w1000/2023/01/image-1.png 1000w, https://www.flamingbytes.com/content/images/2023/01/image-1.png 1133w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: markdown--><h2 id="draw-diagram-for-california-data">Draw diagram for California data</h2>
<p>Do you want to try it by yourself?</p>
<h2 id="createcalculate-a-new-column-for-covid-ratio">Create/Calculate a new column for covid ratio</h2>
<p>Somehow, we want to get to know the trend for covid ratio.</p>
<ul>
<li>covid_ratio = overall_rate_of_covid / overall_rate_of_all_causes</li>
</ul>
<pre><code>covid_death_rate &lt;- df1 %&gt;%
    filter(cause_of_death == &quot;COVID-19&quot;) %&gt;%
    select(&quot;overall_rate&quot;)

all_causes_rate &lt;- df1 %&gt;%
    filter(cause_of_death == &quot;All causes&quot;) %&gt;%
    select(overall_rate)

covid_ratio &lt;- covid_death_rate / all_causes_rate

df_ratio &lt;- df1 %&gt;%
    filter(cause_of_death == &quot;All causes&quot;) %&gt;%
    select(year_and_quarter)
df_ratio[&quot;covid_ratio&quot;] = covid_ratio


&gt; print(df_ratio)
# A tibble: 14 &#xD7; 2
   year_and_quarter covid_ratio
   &lt;chr&gt;                  &lt;dbl&gt;
 1 2019 Q1              0      
 2 2019 Q2              0      
 3 2019 Q3              0      
 4 2019 Q4              0      
 5 2020 Q1              0.00868
 6 2020 Q2              0.132  
 7 2020 Q3              0.0886 
 8 2020 Q4              0.169  
 9 2021 Q1              0.171  
10 2021 Q2              0.0463 
11 2021 Q3              0.131  
12 2021 Q4              0.120  
13 2022 Q1              0.134  
14 2022 Q2              0.0196 
</code></pre>
<h2 id="draw-diagram-for-covid-ratio">Draw diagram for covid ratio</h2>
<p>Do you want to try it by yourself?</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>